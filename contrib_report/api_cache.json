{"https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/d209c222585e5e4ebc0080a9b00c301c3fe90b6f_null": {"sha": "d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "node_id": "C_kwDOOevfU9oAKGQyMDljMjIyNTg1ZTVlNGViYzAwODBhOWIwMGMzMDFjM2ZlOTBiNmY", "commit": {"author": {"name": "Zak Demo", "email": "zveasy327@gmail.com", "date": "2025-05-09T03:15:29Z"}, "committer": {"name": "Zak Demo", "email": "zveasy327@gmail.com", "date": "2025-05-09T03:19:27Z"}, "message": "Import full project after secret purge", "tree": {"sha": "6c1cf4c89fd661bd17b03a17ce4cad688411d50e", "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/git/trees/6c1cf4c89fd661bd17b03a17ce4cad688411d50e"}, "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/git/commits/d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null, "verified_at": null}}, "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "html_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/commit/d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "comments_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/comments", "author": {"login": "zveasy", "id": 73971304, "node_id": "MDQ6VXNlcjczOTcxMzA0", "avatar_url": "https://avatars.githubusercontent.com/u/73971304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zveasy", "html_url": "https://github.com/zveasy", "followers_url": "https://api.github.com/users/zveasy/followers", "following_url": "https://api.github.com/users/zveasy/following{/other_user}", "gists_url": "https://api.github.com/users/zveasy/gists{/gist_id}", "starred_url": "https://api.github.com/users/zveasy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zveasy/subscriptions", "organizations_url": "https://api.github.com/users/zveasy/orgs", "repos_url": "https://api.github.com/users/zveasy/repos", "events_url": "https://api.github.com/users/zveasy/events{/privacy}", "received_events_url": "https://api.github.com/users/zveasy/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "committer": {"login": "zveasy", "id": 73971304, "node_id": "MDQ6VXNlcjczOTcxMzA0", "avatar_url": "https://avatars.githubusercontent.com/u/73971304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zveasy", "html_url": "https://github.com/zveasy", "followers_url": "https://api.github.com/users/zveasy/followers", "following_url": "https://api.github.com/users/zveasy/following{/other_user}", "gists_url": "https://api.github.com/users/zveasy/gists{/gist_id}", "starred_url": "https://api.github.com/users/zveasy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zveasy/subscriptions", "organizations_url": "https://api.github.com/users/zveasy/orgs", "repos_url": "https://api.github.com/users/zveasy/repos", "events_url": "https://api.github.com/users/zveasy/events{/privacy}", "received_events_url": "https://api.github.com/users/zveasy/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "parents": [{"sha": "58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "html_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/commit/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f"}], "stats": {"total": 13449, "additions": 13449, "deletions": 0}, "files": [{"sha": "542b61f077265915ceb429c12caef38e38efe06a", "filename": "Ansible/sample.yml", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/Ansible%2Fsample.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/Ansible%2Fsample.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/Ansible%2Fsample.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,3 @@\n+- hosts: all\n+  vars:\n+  password: {{ vault_secret }}"}, {"sha": "7fd2dba33a73b7543d922b0c8b9b7cca41ac8017", "filename": "BAD.BOYS.md", "status": "added", "additions": 57, "deletions": 0, "changes": 57, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/BAD.BOYS.md", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/BAD.BOYS.md", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/BAD.BOYS.md?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,57 @@\n+# Bad Boys \n+\n+## TODOs \n+\n+\n+## Already Handled \n+\n+#### FP instances for no resource limits \n+\n+`Not a 'no resource limits', as not 'kind:Pod' ` : [reff: https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/]\n+- 10 YAML manifests: all handled, and under `TEST_ARTIFACTS/fp.no.reso*.yaml`  , 10 test cases added \n+\n+\n+\n+#### FP instances for no security context \n+\n+- `Not a 'no security context', as not 'kind:Pod' ` : https://github.com/in28minutes/spring-microservices-v2/blob/main/05.kubernetes/currency-exchange-service/backup/deployment-03-probes-configured.yaml [reff: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/]  \n+\n+\n+#### FP instances for default namespaces \n+\n+- `Not a default namespace`: https://github.com/kubernetes-native-testbed/kubernetes-native-testbed/blob/develop/manifests/cicd/cd-manifests/infra/nginx-ingress-cd.yaml and https://github.com/narenarjun/ultimate-stack/blob/master/kubernetes/staging/gitops-setup/argocd-app-config.yaml and https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/blob/master/09-EKS-Workloads-on-Fargate/09-02-Fargate-Profiles-Advanced-YAML/kube-manifests/02-Applications/01-ns-app1/02-Nginx-App1-Deployment-and-NodePortService.yml \n+\n+\n+#### Hard-coded secrets (all are FP instances and handled properly)\n+\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/glance-pv.yaml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/nfs-server.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/nfs-server-pvc.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-env-common.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-nova-server-setup.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-pike/glance-pvc.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-ocata/ceilometer-central-pvc.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/pull-images-from-private-reporsitory-in-k8s/my-app-deployment.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/pull-images-from-private-reporsitory-in-k8s/docker-secret.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/container-communication-k8s-networking/nginx-sidecar-container.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/kubernetes-configuration-file-explained/nginx-service.yaml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab-ns.yml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/redis-svc.yml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/gitlab-config-storage.yml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/postgresql-configmap.yml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab-runner/gitlab-runner-docker-deployment.yml\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/load-balancer/nginx/tcp-configmap.yaml\n+\n+\n+##### Save for Taintube (ALREADY HANDLED)\n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-horizon-setup.yaml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-neutron-server-setup.yaml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-openstack-openrc.yaml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/container-communication-k8s-networking/postgres.yaml \n+- /Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/gitlab-deployment.yml\n+\n+\n+##### Parsing is still a problem (LEAVE AS LIMITATION )\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-extras/files/templates/calico/calico.yaml (NESTED YAML: HOST NETWORK)\n+- https://github.com/oktadev/jhipster-microservices-example/blob/master/kubernetes/registry/application-configmap.yml (template generated content where secret is provided) \n+"}, {"sha": "2e0937e00ee40c58b0f6fc3cbf75796c6ea8a5ed", "filename": "Dockerfile", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/Dockerfile", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/Dockerfile", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/Dockerfile?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,18 @@\n+FROM continuumio/miniconda3\n+\n+WORKDIR /app\n+\n+RUN conda config --append channels conda-forge\n+\n+# Create the environment:\n+COPY environment.yml .\n+RUN conda env create -v -f environment.yml\n+\n+# Make RUN commands use the new environment:\n+RUN echo \"conda activate KUBESEC\" >> ~/.bashrc\n+SHELL [\"/bin/bash\", \"--login\", \"-c\"]\n+ENV PATH /opt/conda/envs/KUBESEC/bin:$PATH\n+\n+# The code to run when container is started:\n+COPY constants.py graphtaint.py main.py parser.py scanner.py /app/\n+ENTRYPOINT [\"python\", \"/app/main.py\"]\n\\ No newline at end of file"}, {"sha": "2f8ae540f8cdee6f8b4ac09a97eaff4811e78de8", "filename": "NOTES.md", "status": "added", "additions": 480, "deletions": 0, "changes": 480, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/NOTES.md", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/NOTES.md", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/NOTES.md?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,480 @@\n+### Integrating Taint Tracking for Kubernetes \n+\n+\n+#### File types  ( **COMPLETED** ) \n+\n+Shamim is scanning all YAML files. Need to find YAML files that describe K8S deployments. \n+Need to ignore files like: \n+\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/githook/config/crd/bases/tools.pongzt.com_githooks.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes-apps/network_plugin/contiv/tasks/configure.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/container-engine/cri-o/tasks/crio_repo.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/bootstrap-os/tasks/bootstrap-debian.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/vault/roles/vault/tasks/bootstrap/start_vault_temp.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-owners-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-notifications-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/crds/SGBackupConfig.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/crds/seldon-deployment-crd.yaml (K8S `CustomResuourceDefinition` ... needs `KubeCtl` to execute ... so cannot guarantee any file liek this will be executable) \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/crds/opendatahub_v1alpha1_opendatahub_cr.yaml (K8S `OpenDataHub` ... needs `KubeCtl` to execute ... so cannot guarantee any file like this will be executable) \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/repository/index.yaml (has `apiVersion` but no `kind:`)\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/tests/files/packet_debian10-containerd.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/tests/testcases/040_check-network-adv.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/test-infra/image-builder/roles/kubevirt-images/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/container-engine/docker/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/container-engine/containerd/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/obtao@kubernetes/kubernetes/elastic-stack/values.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-owners-service/src/main/resources/application.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-notifications-service/src/main/resources/application.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/e2e/spec/restore.backup.values.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/olm-catalog/opendatahub-operator/0.3.0/opendatahub-operator.v0.3.0.clusterserviceversion.yaml  (`K8S ClusterServiceVersion` needs to be excuted by command line so no guarantee)\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/olm-catalog/opendatahub-operator/0.2.0/opendatahub-operator.v0.2.0.clusterserviceversion.yaml (`K8S ClusterServiceVersion` needs to be excuted by command line so no guarantee)\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/onap_oom_automatic_installation/vars/ddf.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/onap_oom_automatic_installation/roles/oom_prepare/tasks/main.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi//.gitlab-ci.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes-apps/network_plugin/weave/tasks/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes-apps/container_engine_accelerator/nvidia_gpu/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/network_plugin/contiv/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubespray-defaults/defaults/main.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-vets-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-frontend-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-customers-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-visits-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/clusters//clouds.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/dind/group_vars/all/distro.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/kafka/operator-objects/045-Crd-kafkamirrormaker.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/kafka/operator-objects/040-Crd-kafka.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/onap_oom_automatic_installation/roles/oom_postconfigure/tasks/main.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/doc/data/descriptions/stackgres-operator.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/crds/opendatahub_v1alpha1_opendatahub_cr.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/roles/grafana/tasks/main.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/kafka/operator-objects/042-Crd-kafkaconnects2i.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/opendatahub-operator/deploy/kafka/operator-objects/041-Crd-kafkaconnect.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_alertmanagerCustomResourceDefinition.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/dind/roles/dind-cluster/tasks/main.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/etcd/meta/main.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_servicemonitorCustomResourceDefinition.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-vets-service/src/main/resources/application-k8s.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-customers-service/src/main/resources/application-k8s.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-visits-service/src/main/resources/application-k8s.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-vets-service/src/main/resources/application.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-customers-service/src/main/resources/application.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/spring-petclinic-visits-service/src/main/resources/application.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/adduser/tasks/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/bootstrap-os/molecule/default/molecule.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes/preinstall/meta/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes/client/tasks/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/kvm-setup/roles/kvm-setup/tasks/user.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/vault/roles/vault/meta/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/vault/roles/vault/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/crds/SGCluster.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/koris/deploy/manifests/ext-cloud-openstack.yml (`kind: List`)\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/doc/data/descriptions/stackgres-operator.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/download/tasks/prep_kubeadm_images.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/vault/roles/vault/tasks/shared/config_ca.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/.gitlab/deploy/runner/kubernetes/values.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/kubernetes-apps/cluster_roles/defaults/main.yml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/contrib/dind/roles/dind-host/tasks/main.yaml\n+\n+#### Helm charts  ( **COMPLETED** ) \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/helm/cp-helm-charts/charts/cp-control-center/templates \n+\n+Example of how charts are used. No secuirty violations \n+\n+#### Default namespace  ( **COMPLETED** ) \n+\n+##### True Positive Instances \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/kubernetes-configuration-file-explained/nginx-deployment-result.yaml\n+> This is an example of a TP as the default namespace is acutually used by a Deployment \n+```\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    deployment.kubernetes.io/revision: \"1\"\n+    kubectl.kubernetes.io/last-applied-configuration: |\n+      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"repli$\n+  creationTimestamp: \"2020-01-24T10:54:56Z\"\n+  generation: 1\n+  labels:\n+    app: nginx\n+  name: nginx-deployment\n+  namespace: default\n+  resourceVersion: \"96574\"\n+  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment\n+  uid: e1075fa3-6468-43d0-83c0-63fede0dae51\n+```\n+\n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/manifests/services.yml\n+This file uses \n+```\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: airflow-webserver\n+  namespace: default\n+spec:\n+  selector:\n+    app: airflow\n+\n+```\n+`app: airflow` is later used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/manifests/deployment.yaml` as \n+```\n+apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\n+kind: Deployment\n+metadata:\n+  name: airflow-deployment\n+  namespace: default\n+spec:\n+  selector:\n+    matchLabels:\n+      app: airflow\n+```\n+\n+so TP. \n+\n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/spring-petclinic-kubernetes/helm/cp-helm-charts/examples/ksql-demo.yaml \n+> This is an example of true positive as it default namespace is used for a pod \n+\n+```\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: ksql-demo\n+  namespace: default\n+```\n+\n+##### False  Positive Instances \n+\n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/deploy/serviceaccount-varnish.yaml ... another example of a FP \n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/keycloak/ingress.yaml ... another example of a FP, as the default namespace is used in the file as a service, whcih is not used anywhere else \n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/lb/rl50.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/lb/rl100.yaml ...  examples of FPs \n+\n+\n+\n+\n+> nano -c /Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/cl1-k8s/cl1-lb.yaml \n+> This will be a TP if the Service is used by a deployment, using the provided selector `selector:\n+    load-balancer-myapp-GKE_1-lb: \"true\"` \n+> So far not seeing any mathcing deployments, so FP. \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/cl2-k8s/cl2-ingress.yaml\n+> This file uses an Ingress that is consumed by the Service `serviceName: \"myapp-GKE_2-lb\"`. The service is later used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/cl2-k8s/cl2-lb.yaml` in \n+```\n+kind: \"Service\"\n+metadata:\n+  name: \"myapp-GKE_2-lb\"\n+...\n+  selector:\n+    load-balancer-myapp-GKE_2-lb: \"true\"\n+```\n+If the selector is available in a `kind:Deployment` only then we can determine it as a TP. So far not seeing any deployments that\n+match, so FP \n+\n+#### Insecure HTTP  ( **COMPLETED** ) \n+\n+##### True Positive Instances \n+\n+> In `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-init-container-scripts.yaml ` we have  a config map as shown below: \n+```\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: init-container-scripts\n+```\n+This config map is consumed by `update-configMap-init-container-scripts.sh` as `kubectl create -f configMap-init-container-scripts.yaml`\n+This is a TP, as ConfigMaps can be used from the command line. \n+\n+Same argument goes for `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-init-container-scripts.yaml`\n+\n+and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-init-container-scripts.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-nova-compute-setup.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-nova-compute-setup.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-nova-compute-setup.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-nova-compute-setup.yaml` (This one has instances of 0.0.0.0) and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-glance-setup.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-ceilometer-central-setup.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-openstack-openrc.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-heat-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-glance-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-ceilometer-central-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-openstack-openrc.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-heat-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-glance-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-ceilometer-central-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-openstack-openrc.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-heat-setup.yaml`,\n+`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-glance-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-cinder-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-nova-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-horizon-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-neutron-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-cinder-setup.yaml`,`/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-nova-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-neutron-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-aodh-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-cinder-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-nova-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-neutron-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-aodh-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-cinder-setup.yaml`,  `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-nova-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-neutron-server-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-aodh-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/configMap-keystone-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-keystone-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-horizon-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-keystone-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-horizon-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-keystone-setup.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-horizon-setup.yaml`\n+\n+Note to self: Other ways to store config maps are using Pod, with the `valueFrom:  configMapKeyRef: name` tag and through Volumes with the `configMap: name:` tag. Reff: https://kubernetes.io/docs/concepts/configuration/configmap/\n+\n+\n+>  `/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/gitlab/gitlab-deployment.yml` are TPs: `kind: Deployment` \n+\n+##### False  Positive Instances \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_alertmanagerCustomResourceDefinition.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_prometheusCustomResourceDefinition.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_prometheusruleCustomResourceDefinition.yaml :  _CustomResourceDefinition_ does not go into deployments ... so FPs \n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/lb/glb-configmap-var.yaml`, has a configmap called `nginx`, which is never used in the repo `/Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/` so FP. Same for `/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/gitlab-runner/gitlab-runner-docker-configmap.yml` \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/resources/gangway.yaml, used values go nowhere, so FP. Also, as this is not a validly formatted manifest, tool will not detect HTTP instances, which is correct. \n+\n+\n+\n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/configuration/grafana/values.yaml, part of a Helm chart, but used no where , so FP \n+\n+\n+\n+\n+\n+#### Hard-coded Secrets  ( **COMPLETED** ) \n+\n+##### True positive instances \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/values.yaml ... user name and password \n+used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/templates/integrate-grafana-job.yaml`, so TP \n+\n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/tango-base/values.yaml` is TP as used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/tango-base/templates/databaseds.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/tango-base/templates/tangodb.yaml` \n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/archiver/values.yaml` is TP as values used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/archiver/templates/archiverdb.yaml`, `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/archiver/templates/cleaner.yaml`\n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/dsh-lmc-prototype/values.yaml` is TP as it used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/dsh-lmc-prototype/templates/dslhm.yaml` \n+\n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/minecraft/values.yaml` is TP as used in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/minecraft/templates/secrets.yaml`\n+\n+\n+\n+\n+##### False positive instances \n+\n+> */Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/deploy/helm/kubecf/values.yaml* : invalid username \n+```\n+      bbs:\n+        name: diego\n+        password: ~\n+        username: ~\n+```\n+\n+> */Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/nextcloud/values.yaml* , valid user name, but the values are not used\n+by any helm charts \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/django-k8s-starter//rabbit-values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/postgres/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/keycloak/values.yaml,  and : valid user name and password but not used, so FP \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/partial-deployments/tango-db-standalone/values.yaml and /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/matomo/values.yaml are FP as values not used \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/postgres/values-production.yaml and /Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/test/sample-secret-1.yaml are FPs as values are not used \n+\n+> */Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/keycloak/values.yaml* FP for passowrd as not strings are assigned \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/crds/SGBackup.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/prometheus_config.yml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_servicemonitorCustomResourceDefinition.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/dex/00-dex.yml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/demo-kubernetes-components/mongo-express.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/e2e/spec/sql-scripts.values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/e2e/spec/aks/backup-with-aks-storage.values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/e2e/spec/eks/backup-with-s3-storage.values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/githook/config/samples/2-v1alpha1_githooks.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/deploy/varnish.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/load-balancer/lego/deployment.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/deploy/helm/kubecf/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/configuration/cert-manager/letsencrypt.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/configuration/bank-vaults/vault-secrets-webhook/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/gitlab-runner/gitlab-runner-docker-deployment.yml \n+\n+^ FP due to parsing issues \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/varnish-coffee.yaml (`valueFrom:` inferring data from environment, reff: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/). Also the following: \n+\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/varnish-system.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/varnish.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/varnish-coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/varnish-tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-varnish-ns/varnish-coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-varnish-ns/varnish-tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/demo-kubernetes-components/mongo.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes/configurations/deployment.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-sigs-kubespray/roles/network_plugin/calico/tasks/typha_certs.yml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/linode-kubernetes-engine-demo/test-mongo-express.yaml\n+\n+\n+\n+\n+\n+> FP in `/Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/sni/values.yaml` and `/Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/hello/values.yaml` (`key:` and `crt:` ) as these are not used \n+\n+\n+#### No Rolling Updates  ( **COMPLETED** ) \n+\n+##### True Positive Instances \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/deploy/varnish.yaml , TP, not using rolling update and also used by a deployment \n+> Also, the following:\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/varnish-coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/varnish-system.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/varnish.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/varnish-coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/varnish-tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-varnish-ns/varnish-tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/namespace/varnish.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/turkce-kubernetes/kubernetes-playground/imperative-declarative-yontemler/yaml/gcr-deployment.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/test/e2e/deleteTLSsecret/cafe.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/other.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/tea.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/other.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/tea.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/coffee.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-varnish-ns/coffee.yaml \n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-varnish-ns/tea.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/namespace/cafe.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/externalname/cafe.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/sni/beverage.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/hello/cafe.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/hello/cafe.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/kubernetes-configuration-file-explained/nginx-deployment.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/basic-kubectl-commands/demo-test-deployment.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/basic-microservice-for-learning/kubernetes_dev/randomnum_svc.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/basic-microservice-for-learning/kubernetes_prod/randomnum_svc.yaml\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/.concourse/pipeline.yaml \n+- \n+\n+\n+\n+> Random example collected from Internet for testing\n+```\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: nginx-test\n+spec:\n+  replicas: 10\n+  selector:\n+    matchLabels:\n+      service: http-server\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxSurge: 1\n+      maxUnavailable: 1\n+  minReadySeconds: 5\n+  template:\n+    metadata:\n+      labels:\n+        service: http-server\n+    spec:\n+      containers:\n+      - name: nginx\n+        image: nginx:1.10.2\n+        imagePullPolicy: IfNotPresent\n+        ports:\n+        - containerPort: 80\n+```\n+\n+##### False Positive Instances \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/sni/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/hello/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/varnish_pod_template/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/hello/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/keycloak/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/tango-base/values.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/turkce-kubernetes/kubernetes-playground/replication-yontemlerine-genel-bakis/replication/rs.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/turkce-kubernetes/kubernetes-playground/replication-yontemlerine-genel-bakis/replication/rc.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/42_prometheus_Prometheus.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/values-tea.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/values-other.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/cluster-and-ns-wide/values-coffee.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/values-tea.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/values-other.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/clusterwide/values-coffee.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/values-tea.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/values-coffee.yaml, \n+\n+^ all of these are FPs they do not use `type: RollingUpdate`, but they do not map to `kind: Deployment` \n+\n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/kubernetes-configuration-file-explained/nginx-deployment-result.yaml : this is a FP because even with `type: RollingUpdate` and `kind: Deployment` , the tool throws an alert \n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/obtao@kubernetes/kubernetes/app/php-fpm-deployment.yaml : this is a FP because even with `type: RollingUpdate` and `kind: Deployment` , the tool throws an alert \n+> /Users/arahman/K8S_REPOS/GITLAB_REPOS/obtao@kubernetes/kubernetes/app/nginx-deployment.yaml : this is a FP because even with `type: RollingUpdate` and `kind: Deployment` , the tool throws an alert \n+\n+\n+#### Privilege Escalation   ( **COMPLETED** ) \n+\n+##### True Positive Instances \n+\n+> `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/test/sample-pod.yaml` for the following\n+```\n+kind: Pod\n+metadata:\n+ name: sample-pod\n+spec:\n+ #hostNetwork: true\n+ containers:\n+ - name: sample-pod\n+   #image: call518/oaas-init-container\n+   image: call518/oaas-ocata\n+   securityContext:\n+     privileged: true\n+```\n+\n+`securityContext:  privileged: true` this is the part that is probelmatic \n+\n+Same thing happens for \n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/test/sample-nfs-server.yaml`\n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/test/sample-neutron.yaml`\n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/test/sample-pod-1.yaml`\n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-newton/nfs-server.yaml`\n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/nfs-server.yaml`\n+- `/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/nfs-server.yaml`\n+- \n+\n+##### False Positive Instances \n+\n+- /Users/arahman/K8S_REPOS/GITLAB_REPOS/turkce-kubernetes/kubernetes-playground/daemonset-ve-kullanimi/daemonset/fluentd-ds.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/calico-cumulus/demo-multicast/daemonset-pimd.yaml, /Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-extras/files/templates/calico/calico.yaml, this are FPs as `kind: DaemonSet` \n+\n+\n+### Rules to detect secuirty anti-patterns: \n+\n+- `Missing security context`: ( **COMPLETED** ) A kubernetes Pod or Deployment does not have security context specified i.e., the following is not used \n+```\n+spec:\n+  securityContext:\n+```\n+\n+- `Over-allocated privilege`: ( **COMPLETED** ) A Kubernetes Pod or Deployment uses `allowPrivilegeEscalation=true` or `privileged: true` while specifying security context, i.e., the following is used\n+```\n+spec:\n+ #hostNetwork: true\n+ containers:\n+ - name: sample-pod\n+   #image: call518/oaas-init-container\n+   image: call518/oaas-ocata\n+   securityContext:\n+     privileged: true\n+``` \n+- `Default namespace`:  ( **COMPLETED** )  A Kubernetes pod or deployment uses `namespace: default` \n+\n+- `Insecure HTTP`: ( **COMPLETED** )  Use of `http://` as a value, where they relevant key maps back to a pod or deployment\n+\n+- `Hard-coded secret`: ( **COMPLETED** ) Use or hard-coded user name and passwords , must map to a pod, deployment, configmap, or a helm deployment \n+\n+- `Not rolling update`:  ( **COMPLETED** )  not using `type: RollingUpdate` in a deployment as shown in below\n+\n+```\n+> reff: https://tachingchen.com/blog/kubernetes-rolling-update-with-deployment/ \n+strategy:\n+  # indicate which strategy we want for rolling update\n+  type: RollingUpdate\n+  rollingUpdate:\n+    maxSurge: 1\n+    maxUnavailable: 1\n+```\n+\n+- `Unrestricted Network`:  ( **COMPLETED** )   No NetworkPolicy for a pod. Need to remember _NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network \"entities\" (we use the word \"entity\" here to avoid overloading the more common terms such as \"endpoints\" and \"services\", which have specific Kubernetes connotations) over the network._\n+\n+`kind: NetworkPolicy` needs to be in a YAML file, and the same file should also specify a pod selector label *x*, where *x* is a label of a pod. Example below: \n+\n+```\n+- podSelector:\n+        matchLabels:\n+          role: frontend\n+```\n+\n+\n+Another way is to use any one of the following that alllows all ingress and egress connections respectively:\n+```\n+spec:\n+  podSelector: {}\n+  ingress:\n+  - {}\n+```\n+\n+```\n+spec:\n+  podSelector: {}\n+  egress:\n+  - {}\n+```\n+\n+- `Unrestricted Resource Request`:  ( **COMPLETED** )   No resource limit for Deployment or Pod i.e. the following is not specified for a pod: both cpu and memory \n+must be present  \n+```\n+      limits:\n+        memory: \"128Mi\"\n+        cpu: \"500m\"\n+```\n+\n+\n+### Limitations \n+\n+- Tool will report FP isntances for /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/sdp-prototype/Chart.yaml and /Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/skuid/Chart.yaml ... extra check will generate a lot of false negatives \n\\ No newline at end of file"}, {"sha": "983afcf264d8d848d5cf6e9460e96cd48c4893ad", "filename": "README.md", "status": "modified", "additions": 62, "deletions": 0, "changes": 62, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/README.md", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/README.md", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/README.md?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -1,2 +1,64 @@\n+[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) \n+\n+[![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/)\n+\n+[![Actions Status](https://github.com/paser-group/KubeSec/workflows/Build%20KubeTaint/badge.svg)](https://github.com/Build%20TaintPupp/actions)\n+\n+\n+# Taintube: Taint Tracking for Security Analysis of Kubernetes Manifests \n+\n+## Development Environment\n+We use Conda to manage the virtual environment for KubeSec. To see the content of the environment, see [environment.yml](./environment.yml).\n+\n+To use the environment, use the following commands.\n+\n+```bash\n+# Create\n+conda env create -f environment.yml\n+\n+# Activate\n+conda activate KUBESEC\n+\n+# Deactivate\n+conda deactivate\n+\n+# Export (if you modify the environment)\n+conda env export --from-history > environment.yml\n+```\n+\n+## Running the tool\n+\n+The tool is available as a Docker image: https://hub.docker.com/repository/docker/akondrahman/sli-kube \n+\n+### Instruction to run the tool from Docker Hub:\n+\n+- docker rm $(docker ps -a -f status=exited -f status=created -q)\n+- docker rmi -f $(docker images -a -q)\n+- docker pull akondrahman/sli-kube\n+- docker images -a\n+- docker run --rm -it akondrahman/sli-kube bash\n+- cd SLI-KUBE-WORK/KubeSec-master/\n+- python3 main.py\n+\n+### Build and run locally\n+You can also build the docker container locally.\n+\n+After running the tool, you will find a CSV file called 'slikube_results.csv' in the scanned directory.\n+\n+```bash\n+# Build the image\n+docker build -t slikube .\n+\n+# Run the container \n+# Replace '/Users/phu/Desktop/tf-open-source/aws-eks-base' with your local path\n+docker run --rm -v /Users/phu/Desktop/tf-open-source/aws-eks-base:/iac --name slikube slikube /iac\n+```\n+\n+## Collaborators \n+\n+Akond Rahman (Lead), Rahul Pandita, and Shazibul Islam Shamim \n+\n+\n+=======\n # Veasy-SQA2025-AUBURN\n Project"}, {"sha": "d6658313bfaa67ad1a8553b99f6125a57b8ff165", "filename": "REPO.md", "status": "added", "additions": 91, "deletions": 0, "changes": 91, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/REPO.md", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/REPO.md", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/REPO.md?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,91 @@\n+# \ud83d\udce6 Software Quality Assurance Report\n+**Course**: COMP-5710-001 (Spring 2025)  \n+**Team Repository**: `Veasy-SQA2025-AUBURN`  \n+**Author**: Zakariya Veasy  \n+\n+---\n+\n+## Overview\n+\n+This project integrates advanced software quality assurance strategies into the KubeSec Python security toolkit. The deliverables include static analysis, fuzz testing, forensic logging, secret management automation, and a self-healing rollback mechanism.\n+\n+---\n+\n+## Static Analysis\n+\n+- **Tool Used**: Bandit\n+- **Execution**: Triggered automatically via Git pre-commit hook\n+- **Output**: `bandit_report.csv`\n+\n+### Sample Issues Identified:\n+- Hardcoded passwords (`B105`)\n+- Dangerous subprocess calls in `parser.py` (`B603`, `B607`)\n+- Use of blacklisted modules (`B404`)\n+\n+---\n+\n+## Fuzz Testing\n+\n+- **Tool Used**: `hypothesis`\n+- **Test File**: `fuzz.py`\n+- **Key Functions Tested**:\n+  - `parser.loadMultiYAML`\n+  - `parser.getValsFromKey`\n+  - `main.getCountFromAnalysis`\n+  - `graphtaint.mineSecretGraph`\n+- **Result**: No crashes or exceptions; all methods validated as resilient to randomized inputs.\n+\n+---\n+\n+## Forensic Logging\n+\n+Logging was added to track entry, data flow, and execution paths within:\n+- `loadMultiYAML()`\n+- `getValsFromKey()`\n+- `getCountFromAnalysis()`\n+- `mineSecretGraph()`\n+\n+Log output helps verify stability and trace execution during fuzzing.\n+\n+---\n+\n+## Secret Management Automation\n+\n+- **Tool**: `vault4paper.py`\n+- **Capability**:\n+  - Detect and replace secrets in Ansible files\n+  - Store secrets in Vault\n+  - Replace them with secure placeholders (`{{ vault_secret }}`)\n+  - Backup all original files\n+- **Log File**: `.vault4paper-log.json`\n+- **Backups**: `.vault_backups/`\n+\n+### Antidot Rollback\n+- **Tool**: `vault4paper_antidot.py`\n+- Restores all modified files using `.vault_backups/`\n+\n+---\n+\n+## Lessons Learned\n+\n+- Git hooks are powerful for real-time code policy enforcement\n+- Hypothesis is valuable for testing robustness without manual input crafting\n+- Vault + backup automation allows secret rotation without risk of data loss\n+- Full automation transforms a class project into a production-quality module\n+\n+---\n+\n+## Screenshots/Artifacts\n+\n+- [x] `bandit_report.csv`\n+- [x] Terminal logs from `fuzz.py` and `vault4paper.py`\n+- [x] Before/after diff of `Ansible/sample.yml`\n+\n+---\n+\n+## Bonus Automation Vision\n+- Intelligent secret scanning\n+- Graph-based fuzzing prioritization\n+- Rollback-safe automation via `antidot`\n+- Vision for real-world CI/CD integration\n+"}, {"sha": "2a4135407505078949b47472f73510947dceac04", "filename": "TEST_ARTIFACTS/ANOTHER.DOCKERSOCK.yaml", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2FANOTHER.DOCKERSOCK.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2FANOTHER.DOCKERSOCK.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2FANOTHER.DOCKERSOCK.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,43 @@\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: gitlab-runner-docker\n+  namespace: gitlab\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        name: docker-runner\n+        app: gitlab-runner\n+    spec:\n+      containers:\n+      - name: gitlab-runner-docker\n+        image: gitlab/gitlab-runner:alpine-v9.0.0\n+        command: [\"/bin/bash\", \"/scripts/entrypoint\"]\n+        imagePullPolicy: IfNotPresent\n+        env:\n+        - name: REGISTRATION_TOKEN\n+          valueFrom:\n+            secretKeyRef:\n+              name: gitlab-secrets\n+              key: initial_shared_runners_registration_token\n+        resources:\n+          limits:\n+            memory: 500Mi\n+            cpu: 600m\n+          requests:\n+            memory: 500Mi\n+            cpu: 600m\n+        volumeMounts:\n+        - name: scripts\n+          mountPath: /scripts\n+        - name: var-run-docker-sock\n+          mountPath: /var/run/docker.sock\n+      volumes:\n+      - name: var-run-docker-sock\n+        hostPath:\n+          path: /var/run/docker.sock\n+      - name: scripts\n+        configMap:\n+          name: gitlab-runner-scripts"}, {"sha": "0384957157a67cf6d461b11d4244f7bbb52ccbce", "filename": "TEST_ARTIFACTS/absent.default1.yaml", "status": "added", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.default1.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.default1.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fabsent.default1.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,5 @@\n+apiVersion: v1\n+kind: ServiceAccount\n+metadata:\n+  name: varnish-ingress\n+  namespace: default"}, {"sha": "075b96cefd8d0faac5d9e67fef13225894f03074", "filename": "TEST_ARTIFACTS/absent.ingress.yaml", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.ingress.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.ingress.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fabsent.ingress.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,14 @@\n+apps:\n+  coffee:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 2\n+\n+ingress:\n+  name: coffee-ingress\n+  class: varnish-coffee\n+  rules:\n+  - host: coffee.example.com\n+    paths:\n+    - app: coffee\n+\n+vikingAdmSvc: varnish-coffee-admin"}, {"sha": "2a2b66a0901f250ddbc19f33561f2b931672c186", "filename": "TEST_ARTIFACTS/absent.prome.yaml", "status": "added", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.prome.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fabsent.prome.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fabsent.prome.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,26 @@\n+apiVersion: monitoring.coreos.com/v1\n+kind: Prometheus\n+metadata:\n+  labels:\n+    prometheus: nn\n+  name: nn\n+  namespace: nn-mon\n+spec:\n+  externalLabels:\n+    cluster: ${CLUSTER_NAME}\n+    customer: ${CUSTOMER_NAME}\n+  baseImage: quay.io/prometheus/prometheus\n+  nodeSelector:\n+    beta.kubernetes.io/os: linux\n+  replicas: 3\n+  resources:\n+    requests:\n+      memory: 400Mi\n+  ruleSelector:\n+    matchLabels:\n+      prometheus: nn\n+      role: alert-rules\n+  serviceAccountName: nn-prometheus\n+  serviceMonitorNamespaceSelector: {}\n+  serviceMonitorSelector: {}\n+  version: v2.4.3"}, {"sha": "d801b7615812f32270516762745d99f3c95f3993", "filename": "TEST_ARTIFACTS/allow.privilege.yaml", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fallow.privilege.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fallow.privilege.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fallow.privilege.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,12 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: privileged-pod\n+spec:\n+  containers:\n+  - name: some-container\n+    image: g1g1/py-kube:0.2\n+    command: [\"/bin/bash\", \"-c\", \"while true ; do sleep 10 ; done\"]\n+    securityContext:\n+      privileged: true\n+      allowPrivilegeEscalation: true\n\\ No newline at end of file"}, {"sha": "52db33ed6decb57aa88b14f33e4d94550ee76555", "filename": "TEST_ARTIFACTS/artifact.nfs.server.yaml", "status": "added", "additions": 119, "deletions": 0, "changes": 119, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fartifact.nfs.server.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fartifact.nfs.server.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fartifact.nfs.server.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,119 @@\n+#apiVersion: v1\n+#kind: Service\n+#metadata:\n+#  name: nfs-server\n+#  labels:\n+#    app: nfs-server\n+#spec:\n+#  ports:\n+#  - port: 111\n+#    protocol: TCP\n+#    name: nfs-111-tcp\n+#  - port: 111\n+#    protocol: UDP\n+#    name: nfs-111-udp\n+#  - port: 2049\n+#    protocol: TCP\n+#    name: nfs-2049-tcp\n+#  #sessionAffinity: ClientIP\n+#  #clusterIP: None\n+#  #type: NodePort # Or LoadBalancer in production w/ proper security\n+#  #type: LoadBalancer\n+#  selector:\n+#    app: nfs-server\n+#\n+#---\n+\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: nfs-server\n+spec:\n+  nodeSelector:\n+    nfs-server: \"true\"\n+  restartPolicy: Always\n+  initContainers:\n+  - name: wait1\n+    #imagePullPolicy: Always\n+    imagePullPolicy: IfNotPresent\n+    image: call518/oaas-init-container:1.0\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    volumeMounts:\n+    - name: init-container-scripts\n+      mountPath: /init-container-scripts\n+    command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-etcd.sh\"]\n+  containers:\n+  - name: nfs-server\n+    image: call518/oaas-nfs-server:1.0\n+    securityContext:\n+      privileged: true\n+    ports:\n+    - containerPort: 111\n+      protocol: TCP\n+    - containerPort: 111\n+      protocol: UDP\n+    - containerPort: 2049\n+      protocol: TCP\n+    volumeMounts:\n+    - name: pvc-nfs-server\n+      mountPath: /data\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    env:\n+    - name: MY_POD_IP\n+      valueFrom:\n+        fieldRef:\n+          fieldPath: status.podIP\n+    - name: SHARED_DIRECTORY\n+      value: /data\n+    - name: SYNC\n+      value: \"true\"\n+    - name: FSID\n+      value: \"true\"\n+    command:\n+      - \"bash\"\n+      - \"-c\"\n+      - |\n+        until [ \"$CHECK_ETCD_NFS_SERVER_IP\" == \"$MY_POD_IP\" ];\n+        do\n+          echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` Putting nfs-server to etcd....... waiting...\";\n+          curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XPUT -d value=\"$MY_POD_IP\";\n+          CHECK_ETCD_NFS_SERVER_IP=$(curl --connect-timeout 3 -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XGET | jq -r .node.value)\n+          sleep 5;\n+        done;\n+        echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ etcd for nfs-server is ready~~ (etcd's nfs-server IP: $CHECK_ETCD_NFS_SERVER_IP)\";\n+        rm -rf /data/*;\n+        mkdir -p /data/pv/galera-{0,1,2};\n+        mkdir -p /data/pv/mongodb-{0,1,2};\n+        mkdir -p /data/pv/rabbitmq-{0,1,2};\n+        mkdir -p /data/pv/glance-images;\n+        mkdir -p /data/pv/zookeeper-{0,1,2};\n+        mkdir -p /data/pv/cinder-volumes;\n+        mkdir -p /data/pv/cinder-backups;\n+        #mkdir -p /data/pv/cinder-lock_path;\n+        #mkdir -p /data/pv/nova-server-lock_path;\n+        #mkdir -p /data/pv/nova-compute-lock_path;\n+        mkdir -p /data/pv/nova-compute-images;\n+        mkdir -p /data/pv/nova-compute-instances;\n+        mkdir -p /data/pv/ceilometer-gnocchi;\n+        chmod 777 /data/pv/cinder-* /data/pv/zookeeper-*\n+        /usr/bin/nfsd.sh;\n+    lifecycle:\n+      preStop:\n+        exec:\n+          command:\n+          - /bin/sh\n+          - -c\n+          - >\n+            curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XDELETE;\n+  volumes:\n+  - name: init-container-scripts\n+    configMap:\n+      name: init-container-scripts\n+      defaultMode: 0755\n+  - name: pvc-nfs-server\n+    persistentVolumeClaim:\n+      claimName: pvc-nfs-server"}, {"sha": "33f2128298e98dc923af56fca91f21452e34ff80", "filename": "TEST_ARTIFACTS/bakis.rs.yaml", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fbakis.rs.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fbakis.rs.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fbakis.rs.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,21 @@\n+apiVersion: extensions/v1beta1\n+kind: ReplicaSet\n+metadata: \n+  name: hello-rs\n+spec: \n+  replicas: 2\n+  selector: \n+    matchLabels: \n+      app: hello-rs\n+  template: \n+    metadata: \n+      labels: \n+        app: hello-rs\n+        environment: dev\n+    spec: \n+      containers: \n+        - image: \"gcr.io/google_containers/echoserver:1.4\"\n+          name: hello-rs\n+          ports: \n+            - containerPort: 8080\n+"}, {"sha": "fa782b9cc4d9b2799c456bacc03de3c02379a859", "filename": "TEST_ARTIFACTS/bootstrap.debian.yaml", "status": "added", "additions": 95, "deletions": 0, "changes": 95, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fbootstrap.debian.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fbootstrap.debian.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fbootstrap.debian.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,95 @@\n+---\n+# Some Debian based distros ship without Python installed\n+\n+- name: Check if bootstrap is needed\n+  raw: which python3\n+  register: need_bootstrap\n+  failed_when: false\n+  changed_when: false\n+  # This command should always run, even in check mode\n+  check_mode: false\n+  environment: {}\n+  tags:\n+    - facts\n+\n+- name: Check http::proxy in apt configuration files\n+  raw: apt-config dump | grep -qsi 'Acquire::http::proxy'\n+  register: need_http_proxy\n+  failed_when: false\n+  changed_when: false\n+  # This command should always run, even in check mode\n+  check_mode: false\n+  environment: {}\n+  when:\n+    - http_proxy is defined\n+\n+- name: Add http_proxy to /etc/apt/apt.conf if http_proxy is defined\n+  raw: echo 'Acquire::http::proxy \"{{ http_proxy }}\";' >> /etc/apt/apt.conf\n+  become: true\n+  environment: {}\n+  when:\n+    - http_proxy is defined\n+    - need_http_proxy.rc != 0\n+\n+- name: Check https::proxy in apt configuration files\n+  raw: apt-config dump | grep -qsi 'Acquire::https::proxy'\n+  register: need_https_proxy\n+  failed_when: false\n+  changed_when: false\n+  # This command should always run, even in check mode\n+  check_mode: false\n+  environment: {}\n+  when:\n+    - https_proxy is defined\n+\n+- name: Add https_proxy to /etc/apt/apt.conf if https_proxy is defined\n+  raw: echo 'Acquire::https::proxy \"{{ https_proxy }}\";' >> /etc/apt/apt.conf\n+  become: true\n+  environment: {}\n+  when:\n+    - https_proxy is defined\n+    - need_https_proxy.rc != 0\n+\n+- name: Check Network Name Resolution configuration\n+  raw: grep '^DNSSEC=allow-downgrade' /etc/systemd/resolved.conf\n+  register: need_dnssec_allow_downgrade\n+  failed_when: false\n+  changed_when: false\n+  # This command should always run, even in check mode\n+  check_mode: false\n+  environment: {}\n+  when:\n+    - '\"bionic\" in os_release.stdout'\n+\n+- name: Change Network Name Resolution configuration\n+  raw: sed -i 's/^DNSSEC=yes/DNSSEC=allow-downgrade/g' /etc/systemd/resolved.conf\n+  become: true\n+  environment: {}\n+  when:\n+    - '\"bionic\" in os_release.stdout'\n+    - need_dnssec_allow_downgrade.rc\n+\n+- name: Restart systemd-resolved service\n+  raw: systemctl restart systemd-resolved\n+  become: true\n+  environment: {}\n+  when:\n+    - '\"bionic\" in os_release.stdout'\n+    - need_dnssec_allow_downgrade.rc\n+\n+- name: Install python3\n+  raw:\n+    apt-get update && \\\n+    DEBIAN_FRONTEND=noninteractive apt-get install -y python3-minimal\n+  become: true\n+  environment: {}\n+  when:\n+    - need_bootstrap.rc != 0\n+\n+# Workaround for https://github.com/ansible/ansible/issues/25543\n+- name: Install dbus for the hostname module\n+  package:\n+    name: dbus\n+    state: present\n+    use: apt\n+  become: true"}, {"sha": "bf380d7968bf6911f9434831fe48b0660d61d922", "filename": "TEST_ARTIFACTS/calico.yaml", "status": "added", "additions": 565, "deletions": 0, "changes": 565, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcalico.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcalico.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fcalico.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,565 @@\n+---\n+kind: DaemonSet\n+apiVersion: extensions/v1beta1\n+metadata:\n+  name: calico-node\n+  namespace: kube-system\n+  labels:\n+    k8s-app: calico-node\n+spec:\n+  selector:\n+    matchLabels:\n+      k8s-app: calico-node\n+  updateStrategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: calico-node\n+      annotations:\n+        # This, along with the CriticalAddonsOnly toleration below,\n+        # marks the pod as a critical add-on, ensuring it gets\n+        # priority scheduling and that its resources are reserved\n+        # if it ever gets evicted.\n+        scheduler.alpha.kubernetes.io/critical-pod: ''\n+    spec:\n+      hostNetwork: true\n+      serviceAccountName: calico-node\n+      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n+      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n+      terminationGracePeriodSeconds: 0\n+      containers:\n+        # Runs calico/node container on each Kubernetes node.  This\n+        # container programs network policy and routes on each\n+        # host.\n+        - name: calico-node\n+          image: quay.io/calico/node:v3.1.3\n+          env:\n+            # Use Kubernetes API as the backing datastore.\n+            - name: DATASTORE_TYPE\n+              value: \"kubernetes\"\n+            # Use eni not cali for interface prefix\n+            - name: FELIX_INTERFACEPREFIX\n+              value: \"eni\"\n+            # Enable felix info logging.\n+            - name: FELIX_LOGSEVERITYSCREEN\n+              value: \"info\"\n+            # Don't enable BGP.\n+            - name: CALICO_NETWORKING_BACKEND\n+              value: \"none\"\n+            # Cluster type to identify the deployment type\n+            - name: CLUSTER_TYPE\n+              value: \"k8s,ecs\"\n+            # Disable file logging so `kubectl logs` works.\n+            - name: CALICO_DISABLE_FILE_LOGGING\n+              value: \"true\"\n+            - name: FELIX_TYPHAK8SSERVICENAME\n+              value: \"calico-typha\"\n+            # Set Felix endpoint to host default action to ACCEPT.\n+            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n+              value: \"ACCEPT\"\n+            # Disable IPV6 on Kubernetes.\n+            - name: FELIX_IPV6SUPPORT\n+              value: \"false\"\n+            # Wait for the datastore.\n+            - name: WAIT_FOR_DATASTORE\n+              value: \"true\"\n+            - name: FELIX_LOGSEVERITYSYS\n+              value: \"none\"\n+            - name: FELIX_PROMETHEUSMETRICSENABLED\n+              value: \"true\"\n+            - name: NO_DEFAULT_POOLS\n+              value: \"true\"\n+            # Set based on the k8s node name.\n+            - name: NODENAME\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: spec.nodeName\n+            # No IP address needed.\n+            - name: IP\n+              value: \"\"\n+            - name: FELIX_HEALTHENABLED\n+              value: \"true\"\n+          securityContext:\n+            privileged: true\n+          livenessProbe:\n+            httpGet:\n+              path: /liveness\n+              port: 9099\n+              host: localhost\n+            periodSeconds: 10\n+            initialDelaySeconds: 10\n+            failureThreshold: 6\n+          readinessProbe:\n+            httpGet:\n+              path: /readiness\n+              port: 9099\n+            periodSeconds: 10\n+          volumeMounts:\n+            - mountPath: /lib/modules\n+              name: lib-modules\n+              readOnly: true\n+            - mountPath: /var/run/calico\n+              name: var-run-calico\n+              readOnly: false\n+      volumes:\n+        # Used to ensure proper kmods are installed.\n+        - name: lib-modules\n+          hostPath:\n+            path: /lib/modules\n+        - name: var-run-calico\n+          hostPath:\n+            path: /var/run/calico\n+      tolerations:\n+        # Make sure calico/node gets scheduled on all nodes.\n+      - operator: Exists\n+\n+---\n+\n+# Create all the CustomResourceDefinitions needed for\n+# Calico policy-only mode.\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Felix Configuration\n+kind: CustomResourceDefinition\n+metadata:\n+   name: felixconfigurations.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: FelixConfiguration\n+    plural: felixconfigurations\n+    singular: felixconfiguration\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico BGP Configuration\n+kind: CustomResourceDefinition\n+metadata:\n+  name: bgpconfigurations.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: BGPConfiguration\n+    plural: bgpconfigurations\n+    singular: bgpconfiguration\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico IP Pools\n+kind: CustomResourceDefinition\n+metadata:\n+  name: ippools.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: IPPool\n+    plural: ippools\n+    singular: ippool\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Host Endpoints\n+kind: CustomResourceDefinition\n+metadata:\n+  name: hostendpoints.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: HostEndpoint\n+    plural: hostendpoints\n+    singular: hostendpoint\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Cluster Information\n+kind: CustomResourceDefinition\n+metadata:\n+  name: clusterinformations.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: ClusterInformation\n+    plural: clusterinformations\n+    singular: clusterinformation\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Global Network Policies\n+kind: CustomResourceDefinition\n+metadata:\n+  name: globalnetworkpolicies.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: GlobalNetworkPolicy\n+    plural: globalnetworkpolicies\n+    singular: globalnetworkpolicy\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Global Network Sets\n+kind: CustomResourceDefinition\n+metadata:\n+  name: globalnetworksets.crd.projectcalico.org\n+spec:\n+  scope: Cluster\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: GlobalNetworkSet\n+    plural: globalnetworksets\n+    singular: globalnetworkset\n+\n+---\n+\n+apiVersion: apiextensions.k8s.io/v1beta1\n+description: Calico Network Policies\n+kind: CustomResourceDefinition\n+metadata:\n+  name: networkpolicies.crd.projectcalico.org\n+spec:\n+  scope: Namespaced\n+  group: crd.projectcalico.org\n+  version: v1\n+  names:\n+    kind: NetworkPolicy\n+    plural: networkpolicies\n+    singular: networkpolicy\n+\n+---\n+\n+# Create the ServiceAccount and roles necessary for Calico.\n+\n+apiVersion: v1\n+kind: ServiceAccount\n+metadata:\n+  name: calico-node\n+  namespace: kube-system\n+\n+---\n+\n+kind: ClusterRole\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+metadata:\n+  name: calico-node\n+rules:\n+  - apiGroups: [\"\"]\n+    resources:\n+      - namespaces\n+    verbs:\n+      - get\n+      - list\n+      - watch\n+  - apiGroups: [\"\"]\n+    resources:\n+      - pods/status\n+    verbs:\n+      - update\n+  - apiGroups: [\"\"]\n+    resources:\n+      - pods\n+    verbs:\n+      - get\n+      - list\n+      - watch\n+      - patch\n+  - apiGroups: [\"\"]\n+    resources:\n+      - services\n+    verbs:\n+      - get\n+  - apiGroups: [\"\"]\n+    resources:\n+      - endpoints\n+    verbs:\n+      - get\n+  - apiGroups: [\"\"]\n+    resources:\n+      - nodes\n+    verbs:\n+      - get\n+      - list\n+      - update\n+      - watch\n+  - apiGroups: [\"extensions\"]\n+    resources:\n+      - networkpolicies\n+    verbs:\n+      - get\n+      - list\n+      - watch\n+  - apiGroups: [\"networking.k8s.io\"]\n+    resources:\n+      - networkpolicies\n+    verbs:\n+      - watch\n+      - list\n+  - apiGroups: [\"crd.projectcalico.org\"]\n+    resources:\n+      - globalfelixconfigs\n+      - felixconfigurations\n+      - bgppeers\n+      - globalbgpconfigs\n+      - bgpconfigurations\n+      - ippools\n+      - globalnetworkpolicies\n+      - globalnetworksets\n+      - networkpolicies\n+      - clusterinformations\n+      - hostendpoints\n+    verbs:\n+      - create\n+      - get\n+      - list\n+      - update\n+      - watch\n+\n+---\n+\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: ClusterRoleBinding\n+metadata:\n+  name: calico-node\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  kind: ClusterRole\n+  name: calico-node\n+subjects:\n+- kind: ServiceAccount\n+  name: calico-node\n+  namespace: kube-system\n+\n+---\n+\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: calico-typha\n+  namespace: kube-system\n+  labels:\n+    k8s-app: calico-typha\n+spec:\n+  revisionHistoryLimit: 2\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: calico-typha\n+      annotations:\n+        scheduler.alpha.kubernetes.io/critical-pod: ''\n+    spec:\n+      tolerations:\n+      - operator: Exists\n+      hostNetwork: true\n+      serviceAccountName: calico-node\n+      containers:\n+      - image: quay.io/calico/typha:v0.7.4\n+        name: calico-typha\n+        ports:\n+        - containerPort: 5473\n+          name: calico-typha\n+          protocol: TCP\n+        env:\n+          # Use eni not cali for interface prefix\n+          - name: FELIX_INTERFACEPREFIX\n+            value: \"eni\"\n+          - name: TYPHA_LOGFILEPATH\n+            value: \"none\"\n+          - name: TYPHA_LOGSEVERITYSYS\n+            value: \"none\"\n+          - name: TYPHA_LOGSEVERITYSCREEN\n+            value: \"info\"\n+          - name: TYPHA_PROMETHEUSMETRICSENABLED\n+            value: \"true\"\n+          - name: TYPHA_CONNECTIONREBALANCINGMODE\n+            value: \"kubernetes\"\n+          - name: TYPHA_PROMETHEUSMETRICSPORT\n+            value: \"9093\"\n+          - name: TYPHA_DATASTORETYPE\n+            value: \"kubernetes\"\n+          - name: TYPHA_MAXCONNECTIONSLOWERLIMIT\n+            value: \"1\"\n+          - name: TYPHA_HEALTHENABLED\n+            value: \"true\"\n+        volumeMounts:\n+        - mountPath: /etc/calico\n+          name: etc-calico\n+          readOnly: true\n+        livenessProbe:\n+          httpGet:\n+            path: /liveness\n+            port: 9098\n+          periodSeconds: 30\n+          initialDelaySeconds: 30\n+        readinessProbe:\n+          httpGet:\n+            path: /readiness\n+            port: 9098\n+          periodSeconds: 10\n+      volumes:\n+      - name: etc-calico\n+        hostPath:\n+          path: /etc/calico\n+\n+\n+---\n+\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: ClusterRoleBinding\n+metadata:\n+  name: typha-cpha\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  kind: ClusterRole\n+  name: typha-cpha\n+subjects:\n+  - kind: ServiceAccount\n+    name: typha-cpha\n+    namespace: kube-system\n+\n+---\n+\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: ClusterRole\n+metadata:\n+  name: typha-cpha\n+rules:\n+  - apiGroups: [\"\"]\n+    resources: [\"nodes\"]\n+    verbs: [\"list\"]\n+\n+---\n+\n+kind: ConfigMap\n+apiVersion: v1\n+metadata:\n+  name: calico-typha-horizontal-autoscaler\n+  namespace: kube-system\n+data:\n+  ladder: |-\n+    {\n+      \"coresToReplicas\": [],\n+      \"nodesToReplicas\":\n+      [\n+        [1, 1],\n+        [10, 2],\n+        [100, 3],\n+        [250, 4],\n+        [500, 5],\n+        [1000, 6],\n+        [1500, 7],\n+        [2000, 8]\n+      ]\n+    }\n+\n+---\n+\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: calico-typha-horizontal-autoscaler\n+  namespace: kube-system\n+  labels:\n+    k8s-app: calico-typha-autoscaler\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: calico-typha-autoscaler\n+      annotations:\n+        scheduler.alpha.kubernetes.io/critical-pod: ''\n+    spec:\n+      containers:\n+        - image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2\n+          name: autoscaler\n+          command:\n+            - /cluster-proportional-autoscaler\n+            - --namespace=kube-system\n+            - --configmap=calico-typha-horizontal-autoscaler\n+            - --target=deployment/calico-typha\n+            - --logtostderr=true\n+            - --v=2\n+          resources:\n+            requests:\n+              cpu: 10m\n+            limits:\n+              cpu: 10m\n+      serviceAccountName: typha-cpha\n+\n+---\n+\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: Role\n+metadata:\n+  name: typha-cpha\n+  namespace: kube-system\n+rules:\n+  - apiGroups: [\"\"]\n+    resources: [\"configmaps\"]\n+    verbs: [\"get\"]\n+  - apiGroups: [\"extensions\"]\n+    resources: [\"deployments/scale\"]\n+    verbs: [\"get\", \"update\"]\n+\n+---\n+\n+apiVersion: v1\n+kind: ServiceAccount\n+metadata:\n+  name: typha-cpha\n+  namespace: kube-system\n+\n+---\n+\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: RoleBinding\n+metadata:\n+  name: typha-cpha\n+  namespace: kube-system\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  kind: Role\n+  name: typha-cpha\n+subjects:\n+  - kind: ServiceAccount\n+    name: typha-cpha\n+    namespace: kube-system\n+\n+---\n+\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: calico-typha\n+  namespace: kube-system\n+  labels:\n+    k8s-app: calico-typha\n+spec:\n+  ports:\n+    - port: 5473\n+      protocol: TCP\n+      targetPort: calico-typha\n+      name: calico-typha\n+  selector:\n+    k8s-app: calico-typha"}, {"sha": "bd6eff8fe3cea9b45454c1c369e247c0854c6312", "filename": "TEST_ARTIFACTS/cap-module-ostk.yaml", "status": "added", "additions": 202, "deletions": 0, "changes": 202, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcap-module-ostk.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcap-module-ostk.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fcap-module-ostk.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,202 @@\n+# Reference: (N/A)\n+\n+apiVersion: policy/v1beta1\n+kind: PodDisruptionBudget\n+metadata:\n+  name: neutron-server-pdb\n+spec:\n+  selector:\n+    matchLabels:\n+      app: neutron-server\n+  minAvailable: 1\n+  #maxUnavailable: 2\n+\n+---\n+\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: neutron-server\n+  labels:\n+    app: neutron-server\n+spec:\n+  ports:\n+  - name: neutron-server-api\n+    port: 9696\n+    targetPort: 9696\n+  #sessionAffinity: ClientIP\n+  clusterIP: None\n+  #type: NodePort # Or LoadBalancer in production w/ proper security\n+  #type: LoadBalancer\n+  selector:\n+    app: neutron-server\n+\n+---\n+\n+apiVersion: apps/v1beta1\n+kind: StatefulSet\n+metadata:\n+  name: neutron-server\n+  labels:\n+    app: neutron-server\n+spec:\n+  serviceName: \"neutron-server\"\n+  ## now, replicas must be \"1\", because of high load-average issue. (trying to solv).\n+  replicas: 2\n+  podManagementPolicy: OrderedReady\n+  #podManagementPolicy: Parallel\n+  selector:\n+    matchLabels:\n+      app: neutron-server\n+  template:\n+    metadata:\n+      labels:\n+        app: neutron-server\n+    spec:\n+      terminationGracePeriodSeconds: 10\n+      affinity:\n+         podAntiAffinity:\n+           requiredDuringSchedulingIgnoredDuringExecution:\n+           - labelSelector:\n+               matchExpressions:\n+               - key: \"app\"\n+                 operator: In\n+                 values:\n+                 - neutron-server\n+             topologyKey: \"kubernetes.io/hostname\"\n+      nodeSelector:\n+        network: \"true\"\n+      initContainers:\n+      - name: wait1\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container:1.0\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        volumeMounts:\n+        - name: init-container-scripts\n+          mountPath: /init-container-scripts\n+        command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-haproxy.sh\"]\n+      - name: wait2\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container:1.0\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        volumeMounts:\n+        - name: init-container-scripts\n+          mountPath: /init-container-scripts\n+        command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-memcached.sh\"]\n+      - name: wait3\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container:1.0\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        volumeMounts:\n+        - name: init-container-scripts\n+          mountPath: /init-container-scripts\n+        command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-rabbitmq.sh\"]\n+      - name: wait4\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container:1.0\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        volumeMounts:\n+        - name: init-container-scripts\n+          mountPath: /init-container-scripts\n+        command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-keystone.sh\"]\n+      hostAliases:\n+      - ip: \"127.0.0.1\"\n+        hostnames:\n+        - \"neutron-server\"\n+      #- ip: \"192.168.0.150\"\n+      #  hostnames:\n+      #  - \"nfs-server\"\n+      #hostNetwork: true\n+      containers:\n+        - name: neutron-server\n+          image: call518/oaas-queens:latest\n+          #imagePullPolicy: Always\n+          imagePullPolicy: IfNotPresent\n+          securityContext:\n+            privileged: true\n+            capabilities:\n+              add:\n+              - ALL\n+              - CAP_SYS_ADMIN\n+              - CAP_SYS_MODULE\n+              - CAP_NET_ADMIN\n+          env:\n+            #- name: MY_POD_NAME\n+            #  valueFrom:\n+            #    fieldRef:\n+            #      fieldPath: metadata.name\n+            #- name: MY_POD_NAMESPACE\n+            #  valueFrom:\n+            #    fieldRef:\n+            #      fieldPath: metadata.namespace\n+            - name: MY_POD_IP\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: status.podIP\n+          envFrom:\n+            - configMapRef:\n+                name: env-common\n+          command: [\"/scripts/neutron-server-init.sh\"]\n+          ports:\n+            - containerPort: 9696\n+          volumeMounts:\n+          - name: kernel-modules\n+            mountPath: /lib/modules\n+          - name: openstack-openrc\n+            mountPath: /root/openrc\n+          - name: ovs-setup\n+            mountPath: /ovs-setup\n+          - name: neutron-server-setup\n+            mountPath: /scripts\n+          readinessProbe:\n+            exec:\n+              command:\n+              - /check-init.sh\n+            initialDelaySeconds: 10\n+            periodSeconds: 5\n+            #timeoutSeconds: 5\n+            successThreshold: 1\n+            failureThreshold: 1\n+          #livenessProbe:\n+          #  exec:\n+          #    command:\n+          #    - /healthcheck.sh\n+          #    - --liveness\n+          ##livenessProbe:\n+          ##  tcpSocket:\n+          ##    port: 5000\n+          ##  initialDelaySeconds: 5\n+          ##  periodSeconds: 10\n+      volumes:\n+      - name: init-container-scripts\n+        configMap:\n+          name: init-container-scripts\n+          defaultMode: 0755\n+      - name: kernel-modules\n+        hostPath:\n+          path: /lib/modules\n+          type: Directory\n+      - name: openstack-openrc\n+        configMap:\n+          name: openstack-openrc\n+          defaultMode: 0755\n+      - name: ovs-setup\n+        configMap:\n+          name: ovs-setup\n+          defaultMode: 0755\n+      - name: neutron-server-setup\n+        configMap:\n+          name: neutron-server-setup\n+          defaultMode: 0755"}, {"sha": "b26224f5b6ff35f047b71cb35ad9133e25b4cd5f", "filename": "TEST_ARTIFACTS/cap.sys.yaml", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcap.sys.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcap.sys.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fcap.sys.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,198 @@\n+apiVersion: apps/v1beta1\n+kind: StatefulSet\n+metadata:\n+  name: neutron-server\n+  labels:\n+    app: neutron-server\n+spec:\n+  serviceName: \"neutron-server\"\n+  #replicas: 3\n+  replicas: 1\n+  podManagementPolicy: OrderedReady\n+  #podManagementPolicy: Parallel\n+  selector:\n+    matchLabels:\n+      app: neutron-server\n+  template:\n+    metadata:\n+      labels:\n+        app: neutron-server\n+    spec:\n+      terminationGracePeriodSeconds: 10\n+      affinity:\n+         podAntiAffinity:\n+           requiredDuringSchedulingIgnoredDuringExecution:\n+           - labelSelector:\n+               matchExpressions:\n+               - key: \"app\"\n+                 operator: In\n+                 values:\n+                 - neutron-server\n+             topologyKey: \"kubernetes.io/hostname\"\n+      nodeSelector:\n+        network: \"true\"\n+      initContainers:\n+      - name: wait1\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        command:\n+          - /bin/bash\n+          - -c\n+          - >\n+            QUERY_WSREP_READY=\"SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME='WSREP_READY';\";\n+            QUERY_WSREP_CLUSTER_SIZE=\"SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME='WSREP_CLUSTER_SIZE';\";\n+            QUERY_WSREP_CLUSTER_STATUS=\"SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME='WSREP_CLUSTER_STATUS';\";\n+            QUERY_WSREP_LOCAL_STATE_COMMENT=\"SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME='WSREP_LOCAL_STATE_COMMENT';\";\n+            until [ \"$WSREP_READY\" == \"ON\" ] && [ \"$WSREP_CLUSTER_SIZE\" == \"3\" ] && [ \"$WSREP_CLUSTER_STATUS\" == \"Primary\" ] && [ \"$WSREP_LOCAL_STATE_COMMENT\" == \"Synced\" ];\n+            do\n+              echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` haproxy-galera is not ready..... waiting...\";\n+              WSREP_READY=$(mysql --connect-timeout=3 -hhaproxy-galera -uroot -p$MYSQL_ROOT_PASSWORD -N -s -e \"$QUERY_WSREP_READY\" 2>1 2> /dev/null;);\n+              WSREP_CLUSTER_SIZE=$(mysql --connect-timeout=3 -hhaproxy-galera -uroot -p$MYSQL_ROOT_PASSWORD -N -s -e \"$QUERY_WSREP_CLUSTER_SIZE\" 2>1 2> /dev/null;);\n+              WSREP_CLUSTER_STATUS=$(mysql --connect-timeout=3 -hhaproxy-galera -uroot -p$MYSQL_ROOT_PASSWORD -N -s -e \"$QUERY_WSREP_CLUSTER_STATUS\" 2>1 2> /dev/null;);\n+              WSREP_LOCAL_STATE_COMMENT=$(mysql --connect-timeout=3 -hhaproxy-galera -uroot -p$MYSQL_ROOT_PASSWORD -N -s -e \"$QUERY_WSREP_LOCAL_STATE_COMMENT\" 2>1 2> /dev/null;);\n+              sleep 5;\n+            done;\n+            echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ haproxy-galera is ready~~\";\n+      - name: wait2\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        command:\n+          - /bin/bash\n+          - -c\n+          - >\n+            RETURN=1;\n+            CURRENT_CONNECTIONS=-1;\n+            until [ $RETURN -eq 0 ];\n+            do\n+              echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` memcached is not ready..... waiting...\";\n+              CURRENT_CONNECTIONS=$((echo stats ; echo quit) | nc memcached 11211 | awk '/curr_connections/ {print $3}' | tr -d '\\015');\n+              RETURN=$?;\n+              sleep 5;\n+            done;\n+            UPTIME=$((echo stats ; echo quit) | nc memcached 11211 | awk '/uptime/ {print $3}' | tr -d '\\015');\n+            until [ $CURRENT_CONNECTIONS -gt 0 ] && [ $UPTIME -gt 0 ];\n+            do\n+              echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` memcached is not ready..... waiting...\";\n+              sleep 5;\n+            done;\n+            echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ memcached is ready~~\";\n+      - name: wait3\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        command:\n+          - /bin/bash\n+          - -c\n+          - >\n+            until [ \"$R0_ALIVENESS\" == \"{\\\"status\\\":\\\"ok\\\"}\" ] && [ \"$R1_ALIVENESS\" == \"{\\\"status\\\":\\\"ok\\\"}\" ] && [ \"$R2_ALIVENESS\" == \"{\\\"status\\\":\\\"ok\\\"}\" ];\n+            do\n+              R0_ALIVENESS=$(curl --connect-timeout 3 -s -u $K8S_RABBITMQ_ADMIN_USER:$K8S_RABBITMQ_ADMIN_PASS \"http://rabbitmq-0.rabbitmq:15672/api/aliveness-test/%2F\");\n+              R1_ALIVENESS=$(curl --connect-timeout 3 -s -u $K8S_RABBITMQ_ADMIN_USER:$K8S_RABBITMQ_ADMIN_PASS \"http://rabbitmq-1.rabbitmq:15672/api/aliveness-test/%2F\");\n+              R2_ALIVENESS=$(curl --connect-timeout 3 -s -u $K8S_RABBITMQ_ADMIN_USER:$K8S_RABBITMQ_ADMIN_PASS \"http://rabbitmq-2.rabbitmq:15672/api/aliveness-test/%2F\");\n+              echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` rabbitmq is not ready..... waiting...\";\n+              sleep 5;\n+            done;\n+            echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ rabbitmq is ready~~\";\n+      - name: wait4\n+        #imagePullPolicy: Always\n+        imagePullPolicy: IfNotPresent\n+        image: call518/oaas-init-container\n+        envFrom:\n+          - configMapRef:\n+              name: env-common\n+        command:\n+          - /bin/bash\n+          - -c\n+          - >\n+            until [ \"$API_35357\" == \"\\\"stable\\\"\" ] && [ \"$API_5000\" == \"\\\"stable\\\"\" ];\n+            do\n+              ping -c 1 -W 1 keystone 2>&1 >/dev/null\n+              API_35357=$(curl --connect-timeout 3 -s \"http://keystone:35357\" | jq \".versions.values[0].status\");\n+              API_5000=$(curl --connect-timeout 3 -s \"http://keystone:5000\" | jq \".versions.values[0].status\");\n+              echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` keystone is not ready..... waiting...\";\n+              sleep 5;\n+            done;\n+            echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ keystone is ready~~\";\n+      hostAliases:\n+      - ip: \"127.0.0.1\"\n+        hostnames:\n+        - \"neutron-server\"\n+      #- ip: \"192.168.0.150\"\n+      #  hostnames:\n+      #  - \"nfs-server\"\n+      containers:\n+        - name: neutron-server\n+          image: call518/oaas-newton\n+          #imagePullPolicy: Always\n+          imagePullPolicy: IfNotPresent\n+          securityContext:\n+            privileged: true\n+            capabilities:\n+              add:\n+              - ALL\n+              - CAP_SYS_ADMIN\n+              - CAP_SYS_MODULE\n+              - CAP_NET_ADMIN\n+          env:\n+            #- name: MY_POD_NAME\n+            #  valueFrom:\n+            #    fieldRef:\n+            #      fieldPath: metadata.name\n+            #- name: MY_POD_NAMESPACE\n+            #  valueFrom:\n+            #    fieldRef:\n+            #      fieldPath: metadata.namespace\n+            - name: MY_POD_IP\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: status.podIP\n+          envFrom:\n+            - configMapRef:\n+                name: env-common\n+          command: [\"/scripts/neutron-server-init.sh\"]\n+          ports:\n+            - containerPort: 9696\n+          volumeMounts:\n+          - name: neutron-server-setup\n+            mountPath: /scripts\n+          - name: kernel-modules\n+            mountPath: /lib/modules\n+          readinessProbe:\n+            exec:\n+              command:\n+              - /check-init.sh\n+            initialDelaySeconds: 10\n+            periodSeconds: 5\n+            #timeoutSeconds: 5\n+            successThreshold: 1\n+            failureThreshold: 1\n+          #livenessProbe:\n+          #  exec:\n+          #    command:\n+          #    - /healthcheck.sh\n+          #    - --liveness\n+          ##livenessProbe:\n+          ##  tcpSocket:\n+          ##    port: 35357\n+          ##  initialDelaySeconds: 5\n+          ##  periodSeconds: 10\n+      volumes:\n+      - name: neutron-server-setup\n+        configMap:\n+          name: neutron-server-setup\n+          defaultMode: 0755\n+      - name: kernel-modules\n+        hostPath:\n+          path: /lib/modules\n+          type: Directory"}, {"sha": "f427d3f9e4ca53665da99ae9978d664c36fd2127", "filename": "TEST_ARTIFACTS/charts.values.yaml", "status": "added", "additions": 67, "deletions": 0, "changes": 67, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcharts.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcharts.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fcharts.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,67 @@\n+# Default values for HDB++ Archiver.\n+# This is a YAML-formatted file.\n+# Declare variables to be passed into your templates.\n+\n+display: \":0\"\n+xauthority: \"~/.Xauthority\"\n+minikube: true\n+\n+pv:\n+  enabled: true\n+\n+hdbppdb:\n+  enabled: true\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: mariadb_hdbpp\n+    tag: 1.1.0\n+    pullPolicy: IfNotPresent\n+  db:\n+    rootpw: secret\n+    db: hdbpp\n+    user: tango\n+    password: tango\n+  resources:\n+    requests:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+\n+archiver:\n+  enabled: true\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-archiver\n+    tag: 1.0.0\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+\n+dsconfig:\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-dsconfig\n+    tag: 1.2.5.1\n+    pullPolicy: IfNotPresent\n+\n+hdbppviewer:\n+  enabled: false\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: hdbpp_viewer\n+    tag: 1.10\n+    pullPolicy: IfNotPresent\n+\n+attrconfig:\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-itango\n+    tag: 9.3.1\n+    pullPolicy: IfNotPresent\n+\n+nodeSelector: {}\n+\n+affinity: {}\n+\n+tolerations: []"}, {"sha": "db940942823dede93fecf100168e6288df10d452", "filename": "TEST_ARTIFACTS/cluster.svc.v.yaml", "status": "added", "additions": 922, "deletions": 0, "changes": 922, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcluster.svc.v.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fcluster.svc.v.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fcluster.svc.v.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,922 @@\n+apiVersion: operators.coreos.com/v1alpha1\n+kind: ClusterServiceVersion\n+metadata:\n+  annotations:\n+    capabilities: Basic Install\n+    categories: \"AI/Machine Learning, Big Data\"\n+    description: Open Data Hub is a community effort\n+    containerImage: quay.io/opendatahub/opendatahub-operator:v0.2.0\n+    createdAt: 2019-04-02T:01:23:45Z\n+    repository: https://gitlab.com/opendatahub/opendatahub-operator\n+    support: Open Data Hub\n+    certified: \"false\"\n+    alm-examples: |\n+      [\n+        {\n+          \"apiVersion\": \"opendatahub.io/v1alpha1\",\n+          \"kind\": \"OpenDataHub\",\n+          \"metadata\": {\n+            \"name\": \"example-opendatahub\"\n+          },\n+          \"spec\": {\n+            \"aicoe-jupyterhub\": {\n+              \"odh_deploy\": true,\n+              \"notebook_memory\": \"1Gi\",\n+              \"deploy_all_notebooks\": false,\n+              \"registry\": \"\",\n+              \"repository\": \"\",\n+              \"storage_class\": \"\",\n+              \"db_memory\": \"1Gi\",\n+              \"jupyterhub_memory\": \"1Gi\",\n+              \"notebook_image\": \"s2i-spark-minimal-notebook:3.6\",\n+              \"s3_endpoint_url\": \"http://s3.foo.com:8000\",\n+              \"spark_configmap_template\": \"jupyterhub-spark-operator-configmap\",\n+              \"spark_pyspark_submit_args\": \"--conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\",\n+              \"spark_pyspark_driver_python\": \"jupyter\",\n+              \"spark_pyspark_driver_python_opts\": \"notebook\",\n+              \"spark_home\": \"/opt/app-root/lib/python3.6/site-packages/pyspark/\",\n+              \"spark_pythonpath\": \"$PYTHONPATH:/opt/app-root/lib/python3.6/site-packages/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.8.2.1-src.zip\",\n+              \"spark_worker_nodes\": 2,\n+              \"spark_master_nodes\": 1,\n+              \"spark_memory\": \"2Gi\",\n+              \"spark_cpu\": 2,\n+              \"spark_image\": \"quay.io/opendatahub/spark-cluster-image:spark22python36\"\n+            },\n+            \"spark-operator\": {\n+              \"odh_deploy\": true,\n+              \"master_node_count\": 0,\n+              \"master_memory\": \"1Gi\",\n+              \"master_cpu\": 1,\n+              \"worker_node_count\": 0,\n+              \"worker_memory\": \"2Gi\",\n+              \"worker_cpu\": 2\n+            },\n+            \"jupyter-on-openshift\": {\n+              \"odh_deploy\": false,\n+              \"notebook_memory\": \"2Gi\",\n+              \"jupyterhub_config\": \"c.KubeSpawner.env_keep = ['S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY']\\n\",\n+              \"extra_env_vars\": {\n+                \"S3_ENDPOINT_URL\": \"http://s3.foo.com:8000\",\n+                \"S3_ACCESS_KEY\": \"YOURS3ACCESSKEYHERE\",\n+                \"S3_SECRET_KEY\": \"this1is2just3gibberish\"\n+              }\n+            }\n+          }\n+        }\n+      ]\n+  name: opendatahub-operator.v0.2.0\n+  namespace: placeholder\n+spec:\n+  maturity: alpha\n+  version: 0.2.0\n+  apiservicedefinitions: {}\n+  minKubeVersion: 1.11.0\n+  labels:\n+    operated-by: opendatahub-operator\n+  selector:\n+    matchLabels:\n+      operated-by: opendatahub-operator\n+  customresourcedefinitions:\n+    owned:\n+    - kind: OpenDataHub\n+      name: opendatahubs.opendatahub.io\n+      version: v1alpha1\n+      displayName: Open Data Hub\n+      description: Deployment of components from the Open Data Hub community\n+  description: |\n+    The Open Data Hub is a machine-learning-as-a-service platform built on Red Hat's Kubernetes-based OpenShift\u00ae Container Platform, Ceph Object Storage, and Kafka/Strimzi integrating a collection of open source projects.\n+\n+    Open Data Hub is a meta-project that integrates open source projects into a practical solution. It aims to foster collaboration between communities, vendors, user-enterprises, and academics following open source best practices. The open source community can experiment and develop intelligent applications without incurring high costs and having to master the complexity of modern machine learning and artificial intelligence software stacks.\n+\n+    ### Core Components\n+    * JupyterHub - open source multi-user notebook platform\n+    * Apache Spark - unified analytics engine for large-scale data processing\n+    * Ceph - open source object storage\n+    * Prometheus - monitoring and alerting tool\n+    * Grafana - data visualization and monitoring\n+\n+  keywords:\n+  - \"open data hub\"\n+  - \"aicoe\"\n+  - \"open source\"\n+  maintainers:\n+  - name: Open Data Hub\n+    email: contributors@lists.opendatahub.io\n+  provider:\n+    name: Open Data Hub\n+  links:\n+  - name: Open Data Hub\n+    url: https://opendatahub.io\n+  - name: Open Data Hub Community\n+    url: https://gitlab.com/opendatahub\n+  - name: Open Data Hub Getting Started\n+    url: https://gitlab.com/opendatahub/getting-started\n+  displayName: Open Data Hub Operator\n+  icon:\n+  - base64data: \"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\"\n+    mediatype: image/png\n+  install:\n+    strategy: deployment\n+    spec:\n+      deployments:\n+      - name: opendatahub-operator\n+        spec:\n+          replicas: 1\n+          selector:\n+            matchLabels:\n+              name: opendatahub-operator\n+          strategy: {}\n+          template:\n+            metadata:\n+              labels:\n+                name: opendatahub-operator\n+            spec:\n+              containers:\n+              - env:\n+                - name: WATCH_NAMESPACE\n+                  valueFrom:\n+                    fieldRef:\n+                      fieldPath: metadata.annotations['olm.targetNamespaces']\n+                - name: POD_NAME\n+                  valueFrom:\n+                    fieldRef:\n+                      fieldPath: metadata.name\n+                - name: OPERATOR_NAME\n+                  value: opendatahub-operator\n+                image: quay.io/opendatahub/opendatahub-operator:v0.2.0\n+                imagePullPolicy: Always\n+                name: opendatahub-operator\n+                resources: {}\n+              serviceAccountName: opendatahub-operator\n+      permissions:\n+      - rules:\n+        - apiGroups:\n+          - operators.coreos.com\n+          resources:\n+          - clusterserviceversions\n+          - catalogsources\n+          - installplans\n+          - subscriptions\n+          - packagemanifests\n+          verbs:\n+          - create\n+          - update\n+          - patch\n+          - delete\n+        - apiGroups:\n+          - operators.coreos.com\n+          resources:\n+          - clusterserviceversions\n+          - catalogsources\n+          - installplans\n+          - subscriptions\n+          - packagemanifests\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - automationbroker.io\n+          resources:\n+          - ansible-service-broker-openshift-automation-broker-user-auth\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - logging.openshift.io\n+          resources:\n+          - elasticsearches\n+          verbs:\n+          - create\n+          - update\n+          - patch\n+          - delete\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - secrets\n+          - serviceaccounts\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreamimages\n+          - imagestreammappings\n+          - imagestreams\n+          - imagestreams/secrets\n+          - imagestreamtags\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreamimports\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreams/layers\n+          verbs:\n+          - get\n+          - update\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - namespaces\n+          verbs:\n+          - get\n+        - apiGroups:\n+          - \"\"\n+          - project.openshift.io\n+          resources:\n+          - projects\n+          verbs:\n+          - get\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - pods/attach\n+          - pods/exec\n+          - pods/portforward\n+          - pods/proxy\n+          - secrets\n+          - services/proxy\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - pods\n+          - pods/attach\n+          - pods/exec\n+          - pods/portforward\n+          - pods/proxy\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - configmaps\n+          - endpoints\n+          - persistentvolumeclaims\n+          - replicationcontrollers\n+          - replicationcontrollers/scale\n+          - secrets\n+          - serviceaccounts\n+          - services\n+          - services/proxy\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - apps\n+          resources:\n+          - daemonsets\n+          - deployments\n+          - deployments/rollback\n+          - deployments/scale\n+          - replicasets\n+          - replicasets/scale\n+          - statefulsets\n+          - statefulsets/scale\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - autoscaling\n+          resources:\n+          - horizontalpodautoscalers\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - batch\n+          resources:\n+          - cronjobs\n+          - jobs\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - extensions\n+          resources:\n+          - daemonsets\n+          - deployments\n+          - deployments/rollback\n+          - deployments/scale\n+          - ingresses\n+          - networkpolicies\n+          - replicasets\n+          - replicasets/scale\n+          - replicationcontrollers/scale\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - policy\n+          resources:\n+          - poddisruptionbudgets\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - networking.k8s.io\n+          resources:\n+          - networkpolicies\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - patch\n+          - update\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreams\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - builds/details\n+          verbs:\n+          - update\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - builds\n+          verbs:\n+          - get\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - buildconfigs\n+          - buildconfigs/webhooks\n+          - builds\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - builds/log\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - buildconfigs/instantiate\n+          - buildconfigs/instantiatebinary\n+          - builds/clone\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - apps.openshift.io\n+          resources:\n+          - deploymentconfigs\n+          - deploymentconfigs/scale\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - apps.openshift.io\n+          resources:\n+          - deploymentconfigrollbacks\n+          - deploymentconfigs/instantiate\n+          - deploymentconfigs/rollback\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - apps.openshift.io\n+          resources:\n+          - deploymentconfigs/log\n+          - deploymentconfigs/status\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreams/status\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - quota.openshift.io\n+          resources:\n+          - appliedclusterresourcequotas\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - route.openshift.io\n+          resources:\n+          - routes\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - route.openshift.io\n+          resources:\n+          - routes/custom-host\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - route.openshift.io\n+          resources:\n+          - routes/status\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - template.openshift.io\n+          resources:\n+          - processedtemplates\n+          - templateconfigs\n+          - templateinstances\n+          - templates\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - extensions\n+          - networking.k8s.io\n+          resources:\n+          - networkpolicies\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - buildlogs\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - resourcequotausages\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - servicecatalog.k8s.io\n+          resources:\n+          - servicebrokers\n+          - serviceclasses\n+          - serviceplans\n+          - serviceinstances\n+          - servicebindings\n+          verbs:\n+          - create\n+          - update\n+          - delete\n+          - get\n+          - list\n+          - watch\n+          - patch\n+        - apiGroups:\n+          - settings.k8s.io\n+          resources:\n+          - podpresets\n+          verbs:\n+          - create\n+          - update\n+          - delete\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - apiextensions.k8s.io\n+          resourceNames:\n+          - elasticsearches.logging.openshift.io\n+          resources:\n+          - customresourcedefinitions\n+          verbs:\n+          - get\n+        - apiGroups:\n+          - logging.openshift.io\n+          resources:\n+          - elasticsearches\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreamimages\n+          - imagestreammappings\n+          - imagestreams\n+          - imagestreamtags\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - image.openshift.io\n+          resources:\n+          - imagestreams/layers\n+          verbs:\n+          - get\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - configmaps\n+          - endpoints\n+          - persistentvolumeclaims\n+          - pods\n+          - replicationcontrollers\n+          - replicationcontrollers/scale\n+          - serviceaccounts\n+          - services\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - bindings\n+          - events\n+          - limitranges\n+          - namespaces/status\n+          - pods/log\n+          - pods/status\n+          - replicationcontrollers/status\n+          - resourcequotas\n+          - resourcequotas/status\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          resources:\n+          - namespaces\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - apps\n+          resources:\n+          - controllerrevisions\n+          - daemonsets\n+          - deployments\n+          - deployments/scale\n+          - replicasets\n+          - replicasets/scale\n+          - statefulsets\n+          - statefulsets/scale\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - autoscaling\n+          resources:\n+          - horizontalpodautoscalers\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - batch\n+          resources:\n+          - cronjobs\n+          - jobs\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - extensions\n+          resources:\n+          - daemonsets\n+          - deployments\n+          - deployments/scale\n+          - ingresses\n+          - networkpolicies\n+          - replicasets\n+          - replicasets/scale\n+          - replicationcontrollers/scale\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - policy\n+          resources:\n+          - poddisruptionbudgets\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - networking.k8s.io\n+          resources:\n+          - networkpolicies\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - buildconfigs\n+          - buildconfigs/webhooks\n+          - builds\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - apps.openshift.io\n+          resources:\n+          - deploymentconfigs\n+          - deploymentconfigs/scale\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - route.openshift.io\n+          resources:\n+          - routes\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - template.openshift.io\n+          resources:\n+          - processedtemplates\n+          - templateconfigs\n+          - templateinstances\n+          - templates\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - build.openshift.io\n+          resources:\n+          - buildlogs\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - servicecatalog.k8s.io\n+          resources:\n+          - servicebrokers\n+          - serviceclasses\n+          - serviceplans\n+          - serviceinstances\n+          - servicebindings\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - logging.openshift.io\n+          resources:\n+          - elasticsearches\n+          verbs:\n+          - '*'\n+        - apiGroups:\n+          - \"\"\n+          - authorization.openshift.io\n+          resources:\n+          - rolebindings\n+          - roles\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - rbac.authorization.k8s.io\n+          resources:\n+          - rolebindings\n+          - roles\n+          verbs:\n+          - create\n+          - delete\n+          - deletecollection\n+          - get\n+          - list\n+          - patch\n+          - update\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - authorization.openshift.io\n+          resources:\n+          - localresourceaccessreviews\n+          - localsubjectaccessreviews\n+          - subjectrulesreviews\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - authorization.k8s.io\n+          resources:\n+          - localsubjectaccessreviews\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - project.openshift.io\n+          resources:\n+          - projects\n+          verbs:\n+          - delete\n+          - get\n+        - apiGroups:\n+          - \"\"\n+          - authorization.openshift.io\n+          resources:\n+          - resourceaccessreviews\n+          - subjectaccessreviews\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - security.openshift.io\n+          resources:\n+          - podsecuritypolicyreviews\n+          - podsecuritypolicyselfsubjectreviews\n+          - podsecuritypolicysubjectreviews\n+          verbs:\n+          - create\n+        - apiGroups:\n+          - \"\"\n+          - authorization.openshift.io\n+          resources:\n+          - rolebindingrestrictions\n+          verbs:\n+          - get\n+          - list\n+          - watch\n+        - apiGroups:\n+          - \"\"\n+          - project.openshift.io\n+          resources:\n+          - projects\n+          verbs:\n+          - delete\n+          - get\n+          - patch\n+          - update\n+        - apiGroups:\n+          - \"\"\n+          - route.openshift.io\n+          resources:\n+          - routes/status\n+          verbs:\n+          - update\n+        - apiGroups:\n+          - monitoring.coreos.com\n+          resources:\n+          - servicemonitors\n+          verbs:\n+          - get\n+          - create\n+        - apiGroups:\n+          - apps\n+          resourceNames:\n+          - opendatahub-operator\n+          resources:\n+          - deployments/finalizers\n+          verbs:\n+          - update\n+        - apiGroups:\n+          - opendatahub.io\n+          resources:\n+          - '*'\n+          verbs:\n+          - '*'\n+        serviceAccountName: opendatahub-operator\n+  installModes:\n+  - supported: true\n+    type: OwnNamespace\n+  - supported: true\n+    type: SingleNamespace\n+  - supported: false\n+    type: MultiNamespace\n+  - supported: false\n+    type: AllNamespaces"}, {"sha": "26caaf90b6e7242de0788e042e9b2d86a7f84c59", "filename": "TEST_ARTIFACTS/dataimage.airflowimage.manifests.deployment.yaml", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdataimage.airflowimage.manifests.deployment.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdataimage.airflowimage.manifests.deployment.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fdataimage.airflowimage.manifests.deployment.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,227 @@\n+apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\n+kind: Deployment\n+metadata:\n+  name: airflow-deployment\n+  namespace: default\n+spec:\n+  selector:\n+    matchLabels:\n+      app: airflow\n+      run: airflow\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        app: airflow\n+        run: airflow\n+    spec:\n+\n+      volumes:\n+        - name: airflow-secrets\n+          secret:\n+            secretName: airflow\n+        - name: analytics-repo\n+          emptyDir: {}\n+        - name: airflow-logs\n+          persistentVolumeClaim:\n+            claimName: persistent-airflow-logs\n+        - name: kube-config\n+          emptyDir: {}\n+\n+      containers:\n+      # airflow scheduler\n+      - name: scheduler\n+        image: registry.gitlab.com/gitlab-data/data-image/airflow-image:latest\n+        resources:\n+          limits:\n+            memory: \"4000Mi\"\n+          requests:\n+            memory: \"1000Mi\"\n+            cpu: \"1000m\"\n+        env:\n+          # General\n+          - name: GIT_BRANCH\n+            value: \"master\"\n+          - name: IN_CLUSTER\n+            value: \"False\"\n+          - name: GOOGLE_APPLICATION_CREDENTIALS\n+            value: \"/secrets/cloudsql/cloudsql-credentials\"\n+          - name: NAMESPACE\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: NAMESPACE\n+          - name: SLACK_API_TOKEN\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: SLACK_API_TOKEN\n+          # Secret Env Vars\n+          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+          - name: AIRFLOW__CORE__FERNET_KEY\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__FERNET_KEY\n+        volumeMounts:\n+          - name: airflow-secrets\n+            mountPath: /secrets/cloudsql/\n+            readOnly: true\n+          - name: analytics-repo\n+            mountPath: /usr/local/airflow/analytics\n+          - name: airflow-logs\n+            mountPath: /usr/local/airflow/logs\n+          - name: kube-config\n+            mountPath: /root/.kube/\n+        lifecycle:\n+          postStart:\n+            exec:\n+              command: [\"/bin/sh\", \"-c\", \"gcloud auth activate-service-account --key-file /secrets/cloudsql/cloudsql-credentials\"]\n+        command: [\"airflow\"]\n+        args: [\"scheduler\"]\n+\n+      # airflow webserver\n+      - name: webserver\n+        image: registry.gitlab.com/gitlab-data/data-image/airflow-image:latest\n+        resources:\n+          limits:\n+            memory: \"1000Mi\"\n+          requests:\n+            memory: \"200Mi\"\n+            cpu: \"500m\"\n+        env:\n+          # General\n+          - name: GIT_BRANCH\n+            value: \"master\"\n+          - name: IN_CLUSTER\n+            value: \"False\"\n+          - name: GOOGLE_APPLICATION_CREDENTIALS\n+            value: \"/secrets/cloudsql/cloudsql-credentials\"\n+          - name: NAMESPACE\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: NAMESPACE\n+          - name: SLACK_API_TOKEN\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: SLACK_API_TOKEN\n+          # Secret Env Vars\n+          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+          - name: AIRFLOW__CORE__FERNET_KEY\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__FERNET_KEY\n+        volumeMounts:\n+          - name: airflow-secrets\n+            mountPath: /secrets/cloudsql/\n+            readOnly: true\n+          - name: analytics-repo\n+            mountPath: /usr/local/airflow/analytics\n+          - name: airflow-logs\n+            mountPath: /usr/local/airflow/logs\n+          - name: kube-config\n+            mountPath: /root/.kube/\n+        command: [\"airflow\"]\n+        args: [\"webserver\"]\n+        ports:\n+        - containerPort: 8080\n+          protocol: TCP\n+        livenessProbe:\n+          failureThreshold: 3\n+          httpGet:\n+            path: \"/health\"\n+            port: 8080\n+            scheme: HTTP\n+          initialDelaySeconds: 60\n+          periodSeconds: 60\n+          successThreshold: 1\n+          timeoutSeconds: 10\n+        readinessProbe:\n+          failureThreshold: 3\n+          httpGet:\n+            path: \"/health\"\n+            port: 8080\n+            scheme: HTTP\n+          initialDelaySeconds: 60\n+          periodSeconds: 60\n+          successThreshold: 1\n+          timeoutSeconds: 10\n+\n+      # periodically clone the repo to get updated code/dags\n+      - name: watcher\n+        image: registry.gitlab.com/gitlab-data/data-image/airflow-image:latest\n+        resources:\n+          limits:\n+            memory: \"100Mi\"\n+          requests:\n+            memory: \"50Mi\"\n+        env:\n+          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n+          - name: AIRFLOW__CORE__FERNET_KEY\n+            valueFrom:\n+              secretKeyRef:\n+                name: airflow\n+                key: AIRFLOW__CORE__FERNET_KEY\n+        volumeMounts:\n+          - name: analytics-repo\n+            mountPath: /usr/local/airflow/analytics\n+          - name: airflow-secrets\n+            mountPath: /secrets/cloudsql/\n+            readOnly: true\n+          - name: kube-config\n+            mountPath: /root/.kube/\n+        command: [\"repo_watcher.py\"]\n+\n+        # cloudsql proxy for airflow to talk to the database\n+      - name: cloudsql-proxy\n+        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n+        command: [\"/cloud_sql_proxy\",\n+                  \"-instances=gitlab-analysis:us-west1:airflow-pg=tcp:5432\",\n+                  \"-credential_file=/secrets/cloudsql/cloudsql-credentials\"]\n+        securityContext:\n+          runAsUser: 2  # non-root user\n+          allowPrivilegeEscalation: false\n+        volumeMounts:\n+          - name: airflow-secrets\n+            mountPath: /secrets/cloudsql/\n+            readOnly: true\n+\n+      initContainers:\n+      # Get the credentials for k8s before the other repos start up\n+      - name: init-creds\n+        image: registry.gitlab.com/gitlab-data/data-image/airflow-image:latest\n+        command: [\"/bin/sh\", \"-c\"]\n+        args: [\"gcloud auth activate-service-account --key-file /secrets/cloudsql/cloudsql-credentials && \\\n+                gcloud container clusters get-credentials data-ops --region us-west1-a --project gitlab-analysis\"]\n+        volumeMounts:\n+          - name: airflow-secrets\n+            mountPath: /secrets/cloudsql/\n+            readOnly: true\n+          - name: kube-config\n+            mountPath: /root/.kube/\n+      # Copy the repo before the other containers start up\n+      - name: init-repo\n+        image: registry.gitlab.com/gitlab-data/data-image/airflow-image:latest\n+        command: [\"/bin/sh\", \"-c\"]\n+        args: [\"git clone -b master --single-branch $REPO --depth 1\"]\n+        env:\n+          - name: REPO\n+            value: \"https://gitlab.com/gitlab-data/analytics.git\"\n+        volumeMounts:\n+          - name: analytics-repo\n+            mountPath: /usr/local/airflow/analytics"}, {"sha": "41d12c4e85d0dd0cd95b77dfae2b239ffa7281d8", "filename": "TEST_ARTIFACTS/deamonset1.yaml", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdeamonset1.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdeamonset1.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fdeamonset1.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,41 @@\n+apiVersion: extensions/v1beta1\n+kind: DaemonSet\n+metadata: \n+  name: data-collector-ds\n+spec: \n+  template: \n+    metadata: \n+      labels: \n+        app: data-collector-agent\n+    spec: \n+      containers: \n+        - \n+          image: fluent/fluentd\n+          name: fluentd-elasticsearch\n+          resources: \n+            limits: \n+              memory: 200Mi\n+            requests: \n+              cpu: 100m\n+              memory: 200Mi\n+          securityContext: \n+            privileged: true\n+          volumeMounts: \n+            - \n+              mountPath: /var\n+              name: varlog\n+            - \n+              mountPath: /var/lib/docker/containers\n+              name: varlibdockercontainers\n+              readOnly: true\n+      nodeSelector: \n+        app: collector-node\n+      volumes: \n+        - \n+          hostPath: \n+            path: /var\n+          name: varlog\n+        - \n+          hostPath: \n+            path: /var/lib/docker/containers\n+          name: varlibdockercontainers"}, {"sha": "fd4990cd3597e906953bbf35d0cbaf9df7ebe28c", "filename": "TEST_ARTIFACTS/docker.sock.yaml", "status": "added", "additions": 89, "deletions": 0, "changes": 89, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdocker.sock.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fdocker.sock.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fdocker.sock.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,89 @@\n+apiVersion: extensions/v1beta1\n+kind: DaemonSet\n+metadata:\n+  name: metricbeat\n+  namespace: kube-system\n+  labels:\n+    k8s-app: metricbeat\n+spec:\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: metricbeat\n+    spec:\n+      serviceAccountName: metricbeat\n+      terminationGracePeriodSeconds: 30\n+      hostNetwork: true\n+      dnsPolicy: ClusterFirstWithHostNet\n+      containers:\n+      - name: metricbeat\n+        image: docker.elastic.co/beats/metricbeat:6.3.0\n+        args: [\n+          \"-c\", \"/etc/metricbeat.yml\",\n+          \"-e\",\n+          \"-system.hostfs=/hostfs\",\n+        ]\n+        env:\n+        - name: ELASTICSEARCH_HOST\n+          value: elasticsearch-logging\n+        - name: ELASTICSEARCH_PORT\n+          value: \"9200\"\n+        - name: ELASTICSEARCH_USERNAME\n+          value: elastic\n+        - name: ELASTICSEARCH_PASSWORD\n+          value: changeme\n+        - name: ELASTIC_CLOUD_ID\n+          value:\n+        - name: ELASTIC_CLOUD_AUTH\n+          value:\n+        - name: POD_NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        securityContext:\n+          runAsUser: 0\n+        resources:\n+          limits:\n+            memory: 200Mi\n+          requests:\n+            cpu: 100m\n+            memory: 100Mi\n+        volumeMounts:\n+        - name: config\n+          mountPath: /etc/metricbeat.yml\n+          readOnly: true\n+          subPath: metricbeat.yml\n+        - name: modules\n+          mountPath: /usr/share/metricbeat/modules.d\n+          readOnly: true\n+        - name: dockersock\n+          mountPath: /var/run/docker.sock\n+        - name: proc\n+          mountPath: /hostfs/proc\n+          readOnly: true\n+        - name: cgroup\n+          mountPath: /hostfs/sys/fs/cgroup\n+          readOnly: true\n+      volumes:\n+      - name: proc\n+        hostPath:\n+          path: /proc\n+      - name: cgroup\n+        hostPath:\n+          path: /sys/fs/cgroup\n+      - name: dockersock\n+        hostPath:\n+          path: /var/run/docker.sock\n+      - name: config\n+        configMap:\n+          defaultMode: 0600\n+          name: metricbeat-config\n+      - name: modules\n+        configMap:\n+          defaultMode: 0600\n+          name: metricbeat-daemonset-modules\n+      # We set an `emptyDir` here to ensure the manifest will deploy correctly.\n+      # It's recommended to change this to a `hostPath` folder, to ensure internal data\n+      # files survive pod changes (ie: version upgrade)\n+      - name: data\n+        emptyDir: {}"}, {"sha": "4d9c1e374d2a3dabe9091a41b2775ade7e86ba76", "filename": "TEST_ARTIFACTS/empty.yml", "status": "added", "additions": 156, "deletions": 0, "changes": 156, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fempty.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fempty.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fempty.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,156 @@\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: dex\n+    spec:\n+      serviceAccountName: dex # This is created below\n+      containers:\n+      - image: quay.io/dexidp/dex:v2.10.0\n+        name: dex\n+        command: [\"/usr/local/bin/dex\", \"serve\", \"/etc/dex/cfg/config.yaml\"]\n+\n+        ports:\n+        - name: https\n+          containerPort: 5556\n+\n+        volumeMounts:\n+        - name: config\n+          mountPath: /etc/dex/cfg\n+        - name: tls\n+          mountPath: /etc/dex/tls\n+\n+        env:\n+        - name: GITLAB_CLIENT_ID\n+          valueFrom:\n+            secretKeyRef:\n+              name: gitlab-client\n+              key: client-id\n+        - name: GITLAB_CLIENT_SECRET\n+          valueFrom:\n+            secretKeyRef:\n+              name: gitlab-client\n+              key: client-secret\n+\n+      volumes:\n+      - name: config\n+        configMap:\n+          name: dex\n+          items:\n+          - key: config.yaml\n+            path: config.yaml\n+      - name: tls\n+        secret:\n+          secretName: dex.tls\n+---\n+kind: ConfigMap\n+apiVersion: v1\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+data:\n+  config.yaml: |\n+    # 1.1 Substitute this with your Floating IP\n+    issuer: https://%%FLOATING_IP%%:32000\n+    storage:\n+      type: kubernetes\n+      config:\n+        inCluster: true\n+    web:\n+      https: 0.0.0.0:5556\n+      tlsCert: /etc/dex/tls/tls.crt\n+      tlsKey: /etc/dex/tls/tls.key\n+    connectors:\n+      - type: gitlab\n+        id: gitlab\n+        name: Gitlab\n+        config:\n+          # 1.2 (Optional): Enter the URL of your Gitlab instance\n+          baseURL: https://gitlab.com\n+          # Those environment variables are automatically substituted by\n+          # mounting the secret 'gitlab-client'\n+          clientID: $GITLAB_CLIENT_ID\n+          clientSecret: $GITLAB_CLIENT_SECRET\n+          # 1.3 The URL Gitlab redirects to. Substitute with with your\n+          # Floating IP\n+          redirectURI: https://%%FLOATING_IP%%:32000/callback\n+    oauth2:\n+      skipApprovalScreen: true\n+\n+    staticClients:\n+    - id: example-app\n+      redirectURIs:\n+      # 1.4 The URL Dex redirects to. Substitute with with your Floating IP\n+      - 'http://%%FLOATING_IP%%:5555/callback'\n+      name: 'Example App'\n+      # base64 for 'example-app-secret'\n+      secret: ZXhhbXBsZS1hcHAtc2VjcmV0\n+\n+    enablePasswordDB: true\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+spec:\n+  type: NodePort\n+  ports:\n+  - name: dex\n+    port: 5556\n+    protocol: TCP\n+    targetPort: 5556\n+    nodePort: 32000\n+  selector:\n+    k8s-app: dex\n+---\n+apiVersion: v1\n+kind: ServiceAccount\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+---\n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: ClusterRole\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+rules:\n+- apiGroups: [\"dex.coreos.com\"] # API group created by dex\n+  resources: [\"*\"]\n+  verbs: [\"*\"]\n+- apiGroups: [\"apiextensions.k8s.io\"]\n+  resources: [\"customresourcedefinitions\"]\n+  verbs: [\"create\"] # To manage its own resources, dex must be able to create customresourcedefinitions\n+--- \n+apiVersion: rbac.authorization.k8s.io/v1beta1\n+kind: ClusterRoleBinding\n+metadata:\n+  name: dex\n+  namespace: kube-system\n+  labels:\n+    k8s-app: dex\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  kind: ClusterRole\n+  name: dex\n+subjects:\n+- kind: ServiceAccount\n+  name: dex           # Service account assigned to the dex pod, created above\n+  namespace: kube-system  # The namespace dex is running in"}, {"sha": "6528b3782975dbc3b462903582ceb9a4ec2e1b9c", "filename": "TEST_ARTIFACTS/fp.concourse.yaml", "status": "added", "additions": 935, "deletions": 0, "changes": 935, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.concourse.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.concourse.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.concourse.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,935 @@\n+resource_types:\n+- name: pull-request\n+  type: docker-image\n+  source:\n+    repository: teliaoss/github-pr-resource\n+\n+- name: github-status\n+  type: docker-image\n+  source:\n+    repository: resource/github-status\n+    tag: release\n+\n+- name: concourse-git-queue\n+  type: docker-image\n+  source:\n+    repository: splatform/concourse-git-queue\n+\n+resources:\n+- name: commit-to-test\n+  type: concourse-git-queue\n+  source:\n+    bucket: kubecf-ci\n+    bucket_subfolder: build-queue\n+    aws_access_key_id: ((aws-access-key))\n+    aws_secret_access_key: ((aws-secret-key))\n+    access_token: ((github-access-token))\n+\n+- name: kubecf-master\n+  type: git\n+  source:\n+    branch: master\n+    uri: https://github.com/SUSE/kubecf\n+\n+- name: kubecf-pr\n+  type: pull-request\n+  check_every: 10m\n+  source:\n+    repository: SUSE/kubecf\n+    access_token: ((github-access-token))\n+    labels: [\"Trigger: CI\"]\n+    disable_forks: false\n+\n+- name: catapult\n+  type: git\n+  source:\n+    uri: https://github.com/SUSE/catapult\n+  version:\n+    ref: eb4b8fe1453c7a5f1c6a2082e5f490dd6e953664\n+\n+- name: s3.kubecf\n+  type: s3\n+  source:\n+    bucket: kubecf-ci\n+    access_key_id: ((aws-access-key))\n+    secret_access_key: ((aws-secret-key))\n+    region_name: eu-central-1\n+    regexp: kubecf-v(.*).tgz\n+\n+- name: s3.kubecf-bundle\n+  type: s3\n+  source:\n+    bucket: kubecf-ci\n+    access_key_id: ((aws-access-key))\n+    secret_access_key: ((aws-secret-key))\n+    region_name: eu-central-1\n+    regexp: kubecf-bundle-v(.*).tgz\n+\n+deploy_args: &deploy_args\n+- -xce\n+- |\n+  export SCF_LOCAL=\"${PWD}/commit-to-test\"\n+  export SCF_CHART=\"$(readlink -f s3.kubecf/*.tgz)\"\n+  export SCF_OPERATOR=true\n+  export FORCE_DELETE=true\n+  export SCF_TESTGROUP=true\n+  export BACKEND=ekcp\n+  export DOCKER_ORG=cap-staging\n+  export QUIET_OUTPUT=true\n+  export CLUSTER_NAME=\"${CLUSTER_NAME_PREFIX}-$(ls commit-to-test/*.json | \\\n+                       xargs basename | sed 's/.json$//g' | cut -c1-18)\"\n+  pushd catapult\n+  # Bring up a k8s cluster and builds+deploy kubecf\n+  # https://github.com/SUSE/catapult/wiki/Build-and-run-SCF#build-and-run-kubecf\n+  make k8s scf\n+\n+test_args: &test_args\n+- -xce\n+- |\n+  export BACKEND=ekcp\n+  export KUBECF_TEST_SUITE=\"${TEST_SUITE:-smokes}\"\n+  export SCF_LOCAL=\"${PWD}/commit-to-test\"\n+  export KUBECF_NAMESPACE=\"scf\"\n+  export QUIET_OUTPUT=true\n+  export CLUSTER_NAME=\"${CLUSTER_NAME_PREFIX}-$(ls commit-to-test/*.json | \\\n+                       xargs basename | sed 's/.json$//g' | cut -c1-18)\"\n+  pushd catapult\n+  # Grabs back a deployed cluster and runs test suites on it\n+  # See: https://github.com/SUSE/catapult/wiki/Running-SCF-tests#kubecf\n+  make recover tests-kubecf\n+\n+rotate_args: &rotate_args\n+- -xce\n+- |\n+  export BACKEND=ekcp\n+  export KUBECF_NAMESPACE=\"scf\"\n+  export CLUSTER_NAME=\"${CLUSTER_NAME_PREFIX}-$(ls commit-to-test/*.json | \\\n+                       xargs basename | sed 's/.json$//g' | cut -c1-18)\"\n+  pushd catapult\n+  make recover\n+  source build*/.envrc\n+\n+  \"${KUBECF_CHECKOUT}/testing/ccdb_key_rotation/rotate-ccdb-keys-test.sh\"\n+\n+jobs:\n+- name: queue-pr\n+  public: true\n+  plan:\n+  - get: kubecf-pr\n+    params:\n+      integration_tool: checkout\n+    trigger: true\n+  # Use GitHub API to find the remote repository of the PR (it may be a fork)\n+  # The pr resource doesn't provide this information.\n+  - task: find-pr-remote\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: kubecf-pr\n+      outputs:\n+      - name: output\n+      params:\n+        GITHUB_ACCESS_TOKEN: ((github-access-token))\n+        REPO: \"SUSE/kubecf\"\n+      run:\n+        path: \"/bin/bash\"\n+        args:\n+        - -xce\n+        - |\n+          curl -s -X GET \"https://api.github.com/repos/${REPO}/pulls/$(cat kubecf-pr/.git/resource/pr)\" | \\\n+            jq -r .head.repo.full_name > output/remote\n+  - put: commit-to-test\n+    params: &commit-status\n+      commit_path: \"kubecf-pr/.git/resource/head_sha\"\n+      remote_path: \"output/remote\"\n+      description: \"Queued\"\n+      state: \"pending\"\n+      contexts: >\n+        lint,build,deploy-diego,smoke-diego,rotate-diego,smoke-rotated-diego,\n+        acceptance-diego,deploy-eirini,smoke-eirini,rotate-eirini,\n+        smoke-rotated-eirini,acceptance-eirini\n+      trigger: \"PR\"\n+- name: queue-master\n+  public: true\n+  plan:\n+  - get: kubecf-master\n+    params:\n+      integration_tool: checkout\n+    trigger: true\n+  - put: commit-to-test\n+    params:\n+      <<: *commit-status\n+      commit_path: \"kubecf-master/.git/ref\"\n+      remote: \"SUSE/kubecf\"\n+      remote_path: \"\"\n+      description: \"Queued\"\n+      state: \"pending\"\n+      trigger: \"master\"\n+- name: lint\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    trigger: true\n+  - task: lint\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: thulioassis/bazel-docker-image\n+          tag: 2.0.0\n+      inputs:\n+      - name: commit-to-test\n+      run:\n+        path: \"/bin/bash\"\n+        args:\n+        - -xce\n+        - |\n+          cd commit-to-test\n+          ./dev/linters/shellcheck.sh\n+          ./dev/linters/yamllint.sh\n+          ./dev/linters/helmlint.sh\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Lint was successful\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        state: \"success\"\n+        contexts: \"lint\"\n+    on_failure:\n+      put: commit-to-test\n+      params:\n+        description: \"Lint failed\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        state: \"failure\"\n+        contexts: \"lint\"\n+- name: build\n+  public: false # TODO: public or not?\n+  plan:\n+  - get: commit-to-test\n+    trigger: true\n+    passed:\n+    - lint\n+  - task: build\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: thulioassis/bazel-docker-image\n+          tag: 2.0.0\n+      inputs:\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      run:\n+        path: \"/bin/bash\"\n+        args:\n+        - -xce\n+        - |\n+          cd commit-to-test\n+          ./dev/build.sh ../output\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Build was successful\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        state: \"success\"\n+        contexts: \"build\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Build failed\"\n+          state: \"failure\"\n+          contexts: \"build\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+  - put: s3.kubecf\n+    params:\n+      file: output/kubecf-v*.tgz\n+      acl: public-read\n+  - put: s3.kubecf-bundle\n+    params:\n+      file: output/kubecf-bundle-v*.tgz\n+      acl: public-read\n+\n+- name: deploy-diego\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    trigger: true\n+    passed:\n+    - build\n+  - get: s3.kubecf\n+    passed:\n+    - build\n+  - get: catapult\n+  - task: deploy\n+    privileged: true\n+    timeout: 2h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: commit-to-test\n+      - name: catapult\n+      - name: s3.kubecf\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        QUIET_OUTPUT: true\n+        ENABLE_EIRINI: false\n+        CLUSTER_NAME_PREFIX: kubecf-diego\n+      run:\n+        path: \"/bin/bash\"\n+        args: *deploy_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Deploying with Diego was successful\"\n+        state: \"success\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        contexts: \"deploy-diego\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Deploying with Diego failed\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          state: \"failure\"\n+          contexts: \"deploy-diego\"\n+      - task: cleanup-cluster\n+        config: &cleanup-cluster\n+          platform: linux\n+          image_resource:\n+            type: registry-image\n+            source:\n+              repository: splatform/catapult\n+          inputs:\n+          - name: commit-to-test\n+          params:\n+            EKCP_HOST: ((ekcp-host))\n+            CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+          run:\n+            path: \"/bin/bash\"\n+            args:\n+            - -ce\n+            - |\n+              export CLUSTER_NAME=\"${CLUSTER_NAME_PREFIX}-$(ls commit-to-test/*.json | \\\n+                                   xargs basename | sed 's/.json$//g' | cut -c1-18)\"\n+              curl -X DELETE -s \"http://${EKCP_HOST}/${CLUSTER_NAME}\" | jq -r .Output\n+\n+- name: smoke-tests-diego\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - deploy-diego\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test-diego\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: smokes\n+        CLUSTER_NAME_PREFIX: kubecf-diego\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Smoke tests on Diego were successful\"\n+        state: \"success\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        contexts: \"smoke-diego\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Smoke tests on Diego failed\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          state: \"failure\"\n+          contexts: \"smoke-diego\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: ccdb-rotate-diego\n+  public: true\n+  max_in_flight: 2\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - smoke-tests-diego\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: rotate-diego\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        CLUSTER_NAME_PREFIX: kubecf-diego\n+      run:\n+        path: \"/bin/bash\"\n+        args: *rotate_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Rotating secrets on Diego was successful\"\n+        state: \"success\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        contexts: \"rotate-diego\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Rotating secrets on Diego failed\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          state: \"failure\"\n+          contexts: \"rotate-diego\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: smoke-tests-post-rotate-diego\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - ccdb-rotate-diego\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test-diego\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: smokes\n+        CLUSTER_NAME_PREFIX: kubecf-diego\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Smoke tests on Diego after rotating secrets was successful\"\n+        state: \"success\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+        contexts: \"smoke-rotated-diego\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Smoke tests on Diego after rotating secrets failed\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          state: \"failure\"\n+          contexts: \"smoke-rotated-diego\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: cf-acceptance-tests-diego\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - smoke-tests-post-rotate-diego\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test-diego\n+    privileged: true\n+    timeout: 5h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: cats\n+        CLUSTER_NAME_PREFIX: kubecf-diego\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Acceptance tests on Diego succeeded\"\n+        state: \"success\"\n+        contexts: \"acceptance-diego\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Acceptance tests on Diego failed\"\n+          state: \"failure\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          contexts: \"acceptance-diego\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: cleanup-diego-cluster\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - cf-acceptance-tests-diego\n+    trigger: true\n+  - task: cleanup-cluster\n+    config:\n+      <<: *cleanup-cluster\n+      params:\n+        CLUSTER_NAME_PREFIX: \"kubecf-diego\"\n+        EKCP_HOST: ((ekcp-host))\n+\n+# Eirini\n+- name: deploy-eirini\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    trigger: true\n+    passed:\n+    - build\n+  - get: s3.kubecf\n+    passed:\n+    - build\n+  - get: catapult\n+  - task: deploy\n+    timeout: 2h30m\n+    privileged: true\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: commit-to-test\n+      - name: catapult\n+      - name: s3.kubecf\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        ENABLE_EIRINI: true\n+        CLUSTER_NAME_PREFIX: kubecf-eirini\n+      run:\n+        path: \"/bin/bash\"\n+        args: *deploy_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Deploying Eirini succeeded\"\n+        state: \"success\"\n+        contexts: \"deploy-eirini\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Deploying Eirini failed\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          state: \"failure\"\n+          contexts: \"deploy-eirini\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: smoke-tests-eirini\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - deploy-eirini\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: mail-output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: smokes\n+        CLUSTER_NAME_PREFIX: kubecf-eirini\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Smoke tests on Eirini succeeded\"\n+        state: \"success\"\n+        contexts: \"smoke-eirini\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Smoke tests on Eirini failed\"\n+          state: \"failure\"\n+          contexts: \"smoke-eirini\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: ccdb-rotate-eirini\n+  public: true\n+  max_in_flight: 2\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - smoke-tests-eirini\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: rotate-eirini\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        CLUSTER_NAME_PREFIX: kubecf-eirini\n+      run:\n+        path: \"/bin/bash\"\n+        args: *rotate_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Rotating secrets on Eirini succeeded\"\n+        state: \"success\"\n+        contexts: \"rotate-eirini\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Rotating secrets on Eirini failed\"\n+          state: \"failure\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          contexts: \"rotate-eirini\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: smoke-tests-post-rotate-eirini\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - ccdb-rotate-eirini\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test\n+    privileged: true\n+    timeout: 1h30m\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: mail-output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: smokes\n+        CLUSTER_NAME_PREFIX: kubecf-eirini\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Smoke tests after rotating secrets on Eirini succeeded\"\n+        state: \"success\"\n+        contexts: \"smoke-rotated-eirini\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Smoke tests after rotating secrets on Eirini failed\"\n+          state: \"failure\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          contexts: \"smoke-rotated-eirini\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: cf-acceptance-tests-eirini\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - smoke-tests-post-rotate-eirini\n+    trigger: true\n+  - get: s3.kubecf\n+  - get: catapult\n+  - task: test\n+    timeout: 5h30m\n+    privileged: true\n+    config:\n+      platform: linux\n+      image_resource:\n+        type: registry-image\n+        source:\n+          repository: splatform/catapult\n+      inputs:\n+      - name: catapult\n+      - name: commit-to-test\n+      outputs:\n+      - name: mail-output\n+      params:\n+        DEFAULT_STACK: cflinuxfs3\n+        EKCP_HOST: ((ekcp-host))\n+        TEST_SUITE: cats\n+        CLUSTER_NAME_PREFIX: kubecf-eirini\n+      run:\n+        path: \"/bin/bash\"\n+        args: *test_args\n+    on_success:\n+      put: commit-to-test\n+      params:\n+        description: \"Acceptance tests on Eirini succeeded\"\n+        state: \"success\"\n+        contexts: \"acceptance-eirini\"\n+        commit_path: \"commit-to-test/.git/resource/ref\"\n+        version_path: \"commit-to-test/.git/resource/version\"\n+    on_failure:\n+      do:\n+      - put: commit-to-test\n+        params:\n+          description: \"Acceptance tests on Eirini failed\"\n+          state: \"failure\"\n+          commit_path: \"commit-to-test/.git/resource/ref\"\n+          version_path: \"commit-to-test/.git/resource/version\"\n+          contexts: \"acceptance-eirini\"\n+      - task: cleanup-cluster\n+        config:\n+          <<: *cleanup-cluster\n+          params:\n+            CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+            EKCP_HOST: ((ekcp-host))\n+    on_abort:\n+      task: cleanup-cluster\n+      config:\n+        <<: *cleanup-cluster\n+        params:\n+          CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+          EKCP_HOST: ((ekcp-host))\n+\n+- name: cleanup-eirini-cluster\n+  max_in_flight: 2\n+  public: true\n+  plan:\n+  - get: commit-to-test\n+    passed:\n+    - cf-acceptance-tests-eirini\n+    trigger: true\n+  - task: cleanup-cluster\n+    config:\n+      <<: *cleanup-cluster\n+      params:\n+        CLUSTER_NAME_PREFIX: \"kubecf-eirini\"\n+        EKCP_HOST: ((ekcp-host))"}, {"sha": "4a44e1c7425c9597f3699289addf561f2e3926bb", "filename": "TEST_ARTIFACTS/fp.glance.pv.yaml", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.glance.pv.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.glance.pv.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.glance.pv.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,23 @@\n+apiVersion: v1\n+kind: PersistentVolume\n+metadata:\n+  name: pv-glance-images\n+  labels:\n+    app: pv-glance-images\n+spec:\n+  accessModes:\n+  #- ReadWriteOnce\n+  - ReadWriteMany\n+  capacity:\n+    storage: 10Gi\n+  #hostPath:\n+  #  path: /pv/glance-images\n+  volumeMode: Filesystem\n+  #persistentVolumeReclaimPolicy: Recycle\n+  persistentVolumeReclaimPolicy: Retain\n+  mountOptions:\n+    - hard\n+    - nfsvers=4\n+  nfs:\n+    path: /pv/glance-images\n+    server: ___NFS_SERVER_IP___"}, {"sha": "972aa21fc69b007b9d1eeda2f776b4047f22111a", "filename": "TEST_ARTIFACTS/fp.http.yaml", "status": "added", "additions": 156, "deletions": 0, "changes": 156, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.http.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.http.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.http.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,156 @@\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: trackingapi\n+  labels:\n+        app: trackingapi\n+        component: trackingapi\n+spec:\n+  replicas: 2\n+  minReadySeconds: 10\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+      maxSurge: 1 \n+  template:\n+    metadata:\n+      labels:\n+        app: trackingapi\n+    spec:\n+      containers:\n+      - name: trackingapi\n+        image: micrcouriers.azurecr.io/trackingapi:latest     \n+        ports:\n+        - containerPort: 80\n+        imagePullPolicy: Always   \n+        resources:\n+          requests:        \n+            cpu: \"300m\"\n+          limits:          \n+            cpu: \"600m\"\n+        \n+\n+\n+  \n+---\n+\n+\n+\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: paymentapi\n+  labels:\n+        app: paymentapi\n+        component: paymentapi\n+spec:\n+  replicas: 1  \n+  minReadySeconds: 10\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+      maxSurge: 1 \n+  template:\n+    metadata:\n+      labels:\n+        app: paymentapi\n+    spec:\n+      containers:\n+      - name: paymentapi\n+        image: micrcouriers.azurecr.io/paymentapi:latest     \n+        ports:\n+        - containerPort: 80\n+        imagePullPolicy: Always   \n+       \n+\n+  \n+\n+\n+---\n+\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: bookingapi\n+  labels:\n+        app: bookingapi\n+        component: bookingapi\n+spec:\n+  replicas: 1\n+  minReadySeconds: 10\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+      maxSurge: 1 \n+  template:\n+    metadata:\n+      labels:\n+        app: bookingapi\n+    spec:\n+      containers:\n+      - name: bookingapi\n+        image: micrcouriers.azurecr.io/bookingapi:latest     \n+        ports:\n+        - containerPort: 80\n+        imagePullPolicy: Always   \n+        resources:\n+          requests:        \n+            cpu: \"200m\"\n+          limits:          \n+            cpu: \"600m\"\n+        \n+\n+  \n+  \n+  \n+\n+---\n+\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: mcweb\n+  labels:\n+        app: mcweb\n+        component: mcweb\n+spec:\n+  replicas: 2  \n+  minReadySeconds: 10\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+      maxSurge: 1 \n+  template:\n+    metadata:\n+      labels:\n+        app: mcweb\n+    spec:\n+      containers:\n+      - name: mcweb\n+        image: micrcouriers.azurecr.io/mcweb:latest     \n+        ports:\n+        - containerPort: 80\n+        imagePullPolicy: Always   \n+       \n+\n+    \n+---\n+apiVersion: extensions/v1beta1\n+kind: Ingress\n+metadata:\n+  name: webgateway\n+  annotations:\n+    kubernetes.io/ingress.class: addon-http-application-routing\n+spec:\n+  rules:\n+  - host: e1519b70bda84a609fd5.australiaeast.aksapp.io\n+    http:\n+      paths:     \n+      - backend:\n+          serviceName: mcweb\n+          servicePort: 5004\n+        path: /\n\\ No newline at end of file"}, {"sha": "a8607efaef817433124a95f0601e7512759d919d", "filename": "TEST_ARTIFACTS/fp.no.reso1.yaml", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso1.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso1.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso1.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,36 @@\n+#reff: https://github.com/Ignition-Group-Open-Source-Contrib/Dapr-Microservice-Template/blob/master/DaprActorTemplate/deploy/deployDev.yaml\n+# THIS FILE IS AUTOGENERATED BY A TOOL\n+# IF YOU EDIT IT ANY CHANGES YOU MAKE WILL BE LOST\n+\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: $daprAppName$app\n+  labels:\n+    app: $daprAppName$\n+spec:\n+  replicas: 1\n+  revisionHistoryLimit: 2\n+  selector:\n+    matchLabels:\n+      app: $daprAppName$\n+  template:\n+    metadata:\n+      labels:\n+        app: $daprAppName$\n+      annotations:\n+        dapr.io/enabled: \"true\"\n+        dapr.io/app-id: \"$daprAppName$\"\n+        dapr.io/app-port: \"3000\" \n+        prometheus.io/scrape: 'true'\n+        prometheus.io/port:   '9090'\n+    spec:\n+      containers:\n+      - name: $daprAppName$app\n+        image: adlacrdev.azurecr.io/$daprAppName$:$(tag)\n+        ports:\n+        - containerPort: 3000\n+        imagePullPolicy: Always\n+        env:\n+        - name: \"ASPNETCORE_ENVIRONMENT\"\n+          value: \"Development\""}, {"sha": "5032f573e4618b73f37873982c93390828b9f09f", "filename": "TEST_ARTIFACTS/fp.no.reso10.yaml", "status": "added", "additions": 74, "deletions": 0, "changes": 74, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso10.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso10.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso10.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,74 @@\n+#reff: https://github.com/piomin/course-kubernetes-microservices/blob/master/simple-microservices/employee-service/k8s/deployment.yaml\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: employee-deployment-v1\n+spec:\n+  selector:\n+    matchLabels:\n+      app: employee\n+      version: v1\n+  template:\n+    metadata:\n+      labels:\n+        app: employee\n+        version: v1\n+    spec:\n+      containers:\n+      - name: employee\n+        image: piomin/employee-service\n+        ports:\n+        - containerPort: 8080\n+        volumeMounts:\n+          - mountPath: /etc/podinfo\n+            name: podinfo\n+      volumes:\n+        - name: podinfo\n+          downwardAPI:\n+            items:\n+              - path: \"labels\"\n+                fieldRef:\n+                  fieldPath: metadata.labels\n+---\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: employee-deployment-v2\n+spec:\n+  selector:\n+    matchLabels:\n+      app: employee\n+      version: v2\n+  template:\n+    metadata:\n+      labels:\n+        app: employee\n+        version: v2\n+    spec:\n+      containers:\n+        - name: employee\n+          image: piomin/employee-service\n+          ports:\n+            - containerPort: 8080\n+          volumeMounts:\n+            - mountPath: /etc/podinfo\n+              name: podinfo\n+      volumes:\n+        - name: podinfo\n+          downwardAPI:\n+            items:\n+              - path: \"labels\"\n+                fieldRef:\n+                  fieldPath: metadata.labels\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: employee-service\n+spec:\n+  type: ClusterIP\n+  selector:\n+    app: employee\n+  ports:\n+  - port: 8080\n+    targetPort: 8080\n\\ No newline at end of file"}, {"sha": "aac5e821e366398d6e0d1f983db093efd4d955b2", "filename": "TEST_ARTIFACTS/fp.no.reso2.yaml", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso2.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso2.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso2.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,36 @@\n+# reff: https://github.com/Ignition-Group-Open-Source-Contrib/Dapr-Microservice-Template/blob/master/DaprActorTemplate/deploy/deployDev.yaml\n+# THIS FILE IS AUTOGENERATED BY A TOOL\n+# IF YOU EDIT IT ANY CHANGES YOU MAKE WILL BE LOST\n+\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: $daprAppName$app\n+  labels:\n+    app: $daprAppName$\n+spec:\n+  replicas: 1\n+  revisionHistoryLimit: 2\n+  selector:\n+    matchLabels:\n+      app: $daprAppName$\n+  template:\n+    metadata:\n+      labels:\n+        app: $daprAppName$\n+      annotations:\n+        dapr.io/enabled: \"true\"\n+        dapr.io/app-id: \"$daprAppName$\"\n+        dapr.io/app-port: \"3000\" \n+        prometheus.io/scrape: 'true'\n+        prometheus.io/port:   '9090'\n+    spec:\n+      containers:\n+      - name: $daprAppName$app\n+        image: adlacrdev.azurecr.io/$daprAppName$:$(tag)\n+        ports:\n+        - containerPort: 3000\n+        imagePullPolicy: Always\n+        env:\n+        - name: \"ASPNETCORE_ENVIRONMENT\"\n+          value: \"Development\""}, {"sha": "520cc43095959b0bf71cc4fdd23b29dcc3541a82", "filename": "TEST_ARTIFACTS/fp.no.reso3.yaml", "status": "added", "additions": 66, "deletions": 0, "changes": 66, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso3.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso3.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso3.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,66 @@\n+#reff: https://github.com/ganrad/k8s-springboot-data-rest/blob/master/k8s-scripts/app-deploy.yaml\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: mysql-db-name\n+data:\n+  mysqldb.properties: |\n+    mysql.dbname=sampledb\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: mysql-secret\n+type: Opaque\n+data:\n+  username.properties: bXlzcWwudXNlcj1teXNxbAo=\n+  password.properties: bXlzcWwucGFzc3dvcmQ9cGFzc3dvcmQK\n+---\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: po-service\n+  labels:\n+    app: po-service\n+spec:\n+  replicas: 1\n+  selector:\n+    matchLabels:\n+      app: po-service\n+  template:\n+    metadata:\n+      labels:\n+        app: po-service\n+    spec:\n+      volumes:\n+      - name: mysqlcm\n+        configMap:\n+          name: mysql-db-name\n+      - name: mysqlse\n+        secret:\n+          secretName: mysql-secret\n+      containers:\n+      - name: po-service\n+        image: sep21taacr.azurecr.io/po-service:latest\n+        ports:\n+        - containerPort: 8080\n+          protocol: TCP\n+        volumeMounts:\n+        - name: mysqlcm\n+          mountPath: /etc/config\n+        - name: mysqlse\n+          mountPath: /etc/vol-secrets\n+---\n+kind: Service\n+apiVersion: v1\n+metadata:\n+  name: po-service\n+spec:\n+  type: LoadBalancer\n+  selector:\n+    app: po-service\n+  ports:\n+  - name: 80-tcp\n+    protocol: TCP\n+    port: 80\n+    targetPort: 8080\n\\ No newline at end of file"}, {"sha": "d963b270df8f87e542fe7e89615f16d2e5e6542d", "filename": "TEST_ARTIFACTS/fp.no.reso4.yaml", "status": "added", "additions": 70, "deletions": 0, "changes": 70, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso4.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso4.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso4.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,70 @@\n+#reff: https://github.com/camba1/gotemp/blob/master/cicd/K8s/vault/testYamlFile/promotionsrv-deployment.yaml\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    kompose.cmd: kompose -f ../../docker-compose.yml convert\n+    kompose.version: 1.21.0 ()\n+  labels:\n+    io.kompose.service: promotionsrv\n+  name: promotionsrv\n+spec:\n+  replicas: 1\n+  selector:\n+    matchLabels:\n+      io.kompose.service: promotionsrv\n+  strategy:\n+    type: Recreate\n+  template:\n+    metadata:\n+      annotations:\n+        kompose.cmd: kompose -f ../../docker-compose.yml convert\n+        kompose.version: 1.21.0 ()\n+      labels:\n+        io.kompose.service: promotionsrv\n+    spec:\n+      containers:\n+      - env:\n+        - name: DISABLE_AUDIT_RECORDS\n+          valueFrom:\n+            configMapKeyRef:\n+              key: DISABLE_AUDIT_RECORDS\n+              name: promotion-docker-compose-env\n+        - name: MICRO_BROKER\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_BROKER\n+              name: promotion-docker-compose-env\n+#        - name: MICRO_BROKER_ADDRESS\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: MICRO_BROKER_ADDRESS\n+#              name: promotionsrv-secret\n+        - name: MICRO_SERVER_ADDRESS\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_SERVER_ADDRESS\n+              name: promotion-docker-compose-env\n+        - name: MICRO_STORE\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_STORE\n+              name: promotion-docker-compose-env\n+#        - name: MICRO_STORE_ADDRESS\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: MICRO_STORE_ADDRESS\n+#              name: promotionsrv-secret\n+#        - name: POSTGRES_CONNECT\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: POSTGRES_CONNECT\n+#              name: promotionsrv-secret\n+        image: bolbeck/gotemp_promotionsrv\n+        imagePullPolicy: \"\"\n+        name: promotionsrvcont\n+        ports:\n+        - containerPort: 50051\n+        resources: {}\n+      restartPolicy: Always\n+      serviceAccountName: \"\""}, {"sha": "d963b270df8f87e542fe7e89615f16d2e5e6542d", "filename": "TEST_ARTIFACTS/fp.no.reso5.yaml", "status": "added", "additions": 70, "deletions": 0, "changes": 70, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso5.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso5.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso5.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,70 @@\n+#reff: https://github.com/camba1/gotemp/blob/master/cicd/K8s/vault/testYamlFile/promotionsrv-deployment.yaml\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    kompose.cmd: kompose -f ../../docker-compose.yml convert\n+    kompose.version: 1.21.0 ()\n+  labels:\n+    io.kompose.service: promotionsrv\n+  name: promotionsrv\n+spec:\n+  replicas: 1\n+  selector:\n+    matchLabels:\n+      io.kompose.service: promotionsrv\n+  strategy:\n+    type: Recreate\n+  template:\n+    metadata:\n+      annotations:\n+        kompose.cmd: kompose -f ../../docker-compose.yml convert\n+        kompose.version: 1.21.0 ()\n+      labels:\n+        io.kompose.service: promotionsrv\n+    spec:\n+      containers:\n+      - env:\n+        - name: DISABLE_AUDIT_RECORDS\n+          valueFrom:\n+            configMapKeyRef:\n+              key: DISABLE_AUDIT_RECORDS\n+              name: promotion-docker-compose-env\n+        - name: MICRO_BROKER\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_BROKER\n+              name: promotion-docker-compose-env\n+#        - name: MICRO_BROKER_ADDRESS\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: MICRO_BROKER_ADDRESS\n+#              name: promotionsrv-secret\n+        - name: MICRO_SERVER_ADDRESS\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_SERVER_ADDRESS\n+              name: promotion-docker-compose-env\n+        - name: MICRO_STORE\n+          valueFrom:\n+            configMapKeyRef:\n+              key: MICRO_STORE\n+              name: promotion-docker-compose-env\n+#        - name: MICRO_STORE_ADDRESS\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: MICRO_STORE_ADDRESS\n+#              name: promotionsrv-secret\n+#        - name: POSTGRES_CONNECT\n+#          valueFrom:\n+#            secretKeyRef:\n+#              key: POSTGRES_CONNECT\n+#              name: promotionsrv-secret\n+        image: bolbeck/gotemp_promotionsrv\n+        imagePullPolicy: \"\"\n+        name: promotionsrvcont\n+        ports:\n+        - containerPort: 50051\n+        resources: {}\n+      restartPolicy: Always\n+      serviceAccountName: \"\""}, {"sha": "61d3e414bd8109b332cf9a4ad90cd540f9303554", "filename": "TEST_ARTIFACTS/fp.no.reso6.yaml", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso6.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso6.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso6.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,48 @@\n+#reff: https://github.com/IBM/Java-MicroProfile-on-Kubernetes/blob/master/manifests/deploy-vote.yaml\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: microservice-vote-sample\n+  labels:\n+    app: microprofile-app\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        name: vote-deployment\n+    spec:\n+      containers:\n+      - name: microservice-vote\n+        #change the image name\n+        image: journeycode/microservice-ol-vote\n+        ports:\n+          - containerPort: 9080\n+        imagePullPolicy: IfNotPresent\n+        env:\n+          - name: dbUsername\n+            valueFrom:\n+              secretKeyRef:\n+                name: cloudant-secret\n+                key: dbUsername\n+          - name: dbPassword\n+            valueFrom:\n+              secretKeyRef:\n+                name: cloudant-secret\n+                key: dbPassword\n+          - name: dbUrl\n+            value: http://cloudant-service:80\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: vote-service\n+  labels:\n+    app: microprofile-app\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 9080\n+      targetPort: 9080\n+  selector:\n+    name: vote-deployment\n\\ No newline at end of file"}, {"sha": "073b18647cbbb9737eed13548e79b962248026a7", "filename": "TEST_ARTIFACTS/fp.no.reso7.yaml", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso7.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso7.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso7.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,32 @@\n+# reff: https://github.com/oktadev/jhipster-microservices-example/blob/master/kubernetes/store/store-deployment.yml\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: store\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        app: store\n+    spec:\n+      containers:\n+      - name: store-app\n+        image: mraible/store\n+        imagePullPolicy: IfNotPresent\n+        env:\n+        - name: SPRING_PROFILES_ACTIVE\n+          value: prod\n+        - name: SPRING_CLOUD_CONFIG_URI\n+          value: http://admin:${jhipster.registry.password}@jhipster-registry.default.svc.cluster.local:8761/config\n+        - name: JHIPSTER_REGISTRY_PASSWORD\n+          valueFrom:\n+            secretKeyRef:\n+              name: registry-secret\n+              key: registry-admin-password\n+        - name: SPRING_DATA_MONGODB_URI\n+          value: mongodb://store-mongodb.default.svc.cluster.local:27017\n+        - name: SPRING_DATA_MONGODB_DATABASE\n+          value: store\n+        ports:\n+        - containerPort: 8081\n\\ No newline at end of file"}, {"sha": "81088de03a4503d88520076e0a42ea5fe7d311bb", "filename": "TEST_ARTIFACTS/fp.no.reso8.yaml", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso8.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso8.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso8.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,46 @@\n+# reff: https://github.com/patrikduch/netcore-microservices/blob/master/deployment/aks/services/customer/customer-api/customer-api.yaml\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: customer-api-deployment\n+  labels:\n+    app: customer-api\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: customer-api\n+  template:\n+    metadata:\n+      labels:\n+        app: customer-api\n+    spec:\n+      containers:\n+        - name: customerapi\n+          image:  netcoremicroservicesacr.azurecr.io/customerapi:v1\n+          imagePullPolicy: IfNotPresent\n+          ports:\n+            - containerPort: 80\n+          env:\n+            - name: ASPNETCORE_ENVIRONMENT\n+              value: Release\n+            - name: DatabaseSettings__ConnectionString\n+              valueFrom:\n+                  secretKeyRef:\n+                    name: customer-api-secret\n+                    key: ConnectionString\n+\n+      imagePullSecrets:\n+        - name: acr-secret\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: customer-api-service\n+spec:\n+  type: ClusterIP\n+  selector:\n+    app: customer-api\n+  ports:\n+    - protocol: TCP\n+      port: 80\n\\ No newline at end of file"}, {"sha": "22d4b37c0cb1616296ab5d39d5e32e04f2c5c7df", "filename": "TEST_ARTIFACTS/fp.no.reso9.yaml", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso9.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.reso9.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.reso9.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,35 @@\n+# reff: https://github.com/narenarjun/ultimate-stack/blob/master/kubernetes/dev/mcs/expiration-depl.yaml\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: expiration-depl\n+  labels:\n+    type: dev-depl\n+    svcname: expiration-svc\n+    version: v1\n+spec:\n+  replicas: 1\n+  selector:\n+    matchLabels:\n+      app: expiration\n+      version: v1\n+  template:\n+    metadata:\n+      labels:\n+        app: expiration\n+        version: v1\n+    spec:\n+      containers:\n+        - name: expiration\n+          image: quay.io/ultimatestack/expiration-svc:v1-beta\n+          env:\n+            - name: NATS_CLIENT_ID\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: metadata.name\n+            - name: NATS_URL\n+              value: \"http://nats-srv:4222\"\n+            - name: NATS_CLUSTER_ID\n+              value: ticketing\n+            - name: REDIS_HOST\n+              value: expiration-redis-srv\n\\ No newline at end of file"}, {"sha": "92e00bcc27fcccac83b95fc8886b29181350f93f", "filename": "TEST_ARTIFACTS/fp.no.secu.cont.yaml", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.secu.cont.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.no.secu.cont.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.no.secu.cont.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,54 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    deployment.kubernetes.io/revision: \"1\"\n+  labels:\n+    app: currency-exchange\n+  name: currency-exchange\n+  namespace: default\n+spec:\n+  replicas: 1\n+  selector:\n+    matchLabels:\n+      app: currency-exchange\n+  strategy:\n+    rollingUpdate:\n+      maxSurge: 25%\n+      maxUnavailable: 25%\n+    type: RollingUpdate\n+  template:\n+    metadata:\n+      labels:\n+        app: currency-exchange\n+    spec:\n+      containers:\n+      - image: in28min/mmv2-currency-exchange-service:0.0.12-SNAPSHOT\n+        imagePullPolicy: IfNotPresent\n+        name: mmv2-currency-exchange-service\n+        readinessProbe:\n+          httpGet:\n+            port: 8000\n+            path: /actuator/health/readiness\n+        livenessProbe:\n+          httpGet:\n+            port: 8000\n+            path: /actuator/health/liveness\n+      restartPolicy: Always\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    app: currency-exchange\n+  name: currency-exchange\n+  namespace: default\n+spec:\n+  ports:\n+  - port: 8000\n+    protocol: TCP\n+    targetPort: 8000\n+  selector:\n+    app: currency-exchange\n+  sessionAffinity: None\n+  type: LoadBalancer\n\\ No newline at end of file"}, {"sha": "bfc05e444c60ff362ec58bead2c42591aaf6aa4a", "filename": "TEST_ARTIFACTS/fp.nspace1.yaml", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace1.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace1.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.nspace1.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,52 @@\n+#'''\n+#https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/blob/master/09-EKS-Workloads-on-Fargate/09-02-Fargate-Profiles-Advanced-YAML/kube-manifests/02-Applications/01-ns-app1/02-Nginx-App1-Deployment-and-NodePortService.yml\n+#'''\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: app1-nginx-deployment\n+  labels:\n+    app: app1-nginx\n+  namespace: ns-app1\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: app1-nginx\n+  template:\n+    metadata:\n+      labels:\n+        app: app1-nginx\n+    spec:\n+      containers:\n+        - name: app1-nginx\n+          image: stacksimplify/kube-nginxapp1:1.0.0\n+          ports:\n+            - containerPort: 80\n+          resources:\n+            requests:\n+              memory: \"128Mi\"\n+              cpu: \"500m\"\n+            limits:\n+              memory: \"500Mi\"\n+              cpu: \"1000m\"                         \n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: app1-nginx-nodeport-service\n+  labels:\n+    app: app1-nginx\n+  namespace: ns-app1\n+  annotations:\n+#Important Note:  Need to add health check path annotations in service level if we are planning to use multiple targets in a load balancer    \n+    alb.ingress.kubernetes.io/healthcheck-path: /app1/index.html\n+spec:\n+  type: NodePort\n+  selector:\n+    app: app1-nginx\n+  ports:\n+    - port: 80\n+      targetPort: 80\n+\n+   \n\\ No newline at end of file"}, {"sha": "f8686cf86d862ab2f33a3c0c488780af249ca520", "filename": "TEST_ARTIFACTS/fp.nspace2.yaml", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace2.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace2.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.nspace2.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,21 @@\n+## reff: https://github.com/narenarjun/ultimate-stack/blob/master/kubernetes/staging/gitops-setup/argocd-app-config.yaml\n+apiVersion: argoproj.io/v1alpha1\n+kind: Application\n+metadata:\n+  name: glotixz-app-backends-deploy\n+  namespace: argocd\n+spec:\n+  project: default\n+  source:\n+    repoURL: https://github.com/narenarjun/ultimate-stack.git\n+    targetRevision: HEAD\n+    path: gitops\n+    directory:\n+      recurse: true\n+  destination:\n+    server: https://kubernetes.default.svc\n+    namespace: glotixz-backend\n+  syncPolicy:\n+    automated:\n+      prune: false\n+      selfHeal: true\n\\ No newline at end of file"}, {"sha": "b40325a6f56dede02e32953163cff84e187e4ae1", "filename": "TEST_ARTIFACTS/fp.nspace3.yaml", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace3.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.nspace3.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.nspace3.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,21 @@\n+# reff: https://github.com/kubernetes-native-testbed/kubernetes-native-testbed/blob/develop/manifests/cicd/cd-manifests/infra/nginx-ingress-cd.yaml\n+apiVersion: argoproj.io/v1alpha1\n+kind: Application\n+metadata:\n+  name: nginx-ingress-cd\n+  namespace: argocd\n+spec:\n+  project: default\n+  source:\n+    repoURL: https://github.com/__TB_GITHUB_ORG_NAME__/kubernetes-native-testbed.git\n+    targetRevision: develop\n+    path: manifests/infra/nginx-ingress\n+    directory:\n+      recurse: true\n+  destination:\n+    server: https://kubernetes.default.svc\n+    namespace: infra\n+  syncPolicy:\n+    automated:\n+      prune: true\n+      selfHeal: true"}, {"sha": "409d4b923c45a0e71e7a4809c3a64ba8362c6374", "filename": "TEST_ARTIFACTS/fp.seccomp.unconfined.yaml", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.seccomp.unconfined.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp.seccomp.unconfined.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp.seccomp.unconfined.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,18 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: audit-pod\n+  labels:\n+    app: audit-pod\n+spec:\n+  securityContext:\n+    seccompProfile:\n+      type: Localhost\n+      localhostProfile: profiles/audit.json\n+  containers:\n+  - name: test-container\n+    image: hashicorp/http-echo:0.2.3\n+    args:\n+    - \"-text=just made some syscalls!\"\n+    securityContext:\n+      allowPrivilegeEscalation: false\n\\ No newline at end of file"}, {"sha": "af7f684ee3eaac310024aee21f6e68d761794a53", "filename": "TEST_ARTIFACTS/fp_secu_context_miss.yaml", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp_secu_context_miss.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ffp_secu_context_miss.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ffp_secu_context_miss.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,18 @@\n+---\n+apiVersion: rbac.authorization.k8s.io/v1\n+kind: ClusterRoleBinding\n+metadata:\n+  annotations:\n+    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n+  labels:\n+    kubernetes.io/bootstrapping: rbac-defaults\n+    addonmanager.kubernetes.io/mode: EnsureExists\n+  name: system:coredns\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  kind: ClusterRole\n+  name: system:coredns\n+subjects:\n+  - kind: ServiceAccount\n+    name: coredns\n+    namespace: kube-system"}, {"sha": "61ff957a60af3d97910f5f820581fc93f6fbb8e9", "filename": "TEST_ARTIFACTS/gcr.deployment.yaml", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fgcr.deployment.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fgcr.deployment.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fgcr.deployment.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,22 @@\n+  \n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: gcr0-deployment\n+  labels:\n+    app: gcr0-deployment\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: gcr0\n+  template:\n+    metadata:\n+      labels:\n+        app: gcr0\n+    spec:\n+      containers:\n+      - name: gcr\n+        image: gcr.io/google_containers/echoserver:1.4\n+        ports:\n+        - containerPort: 8080\n\\ No newline at end of file"}, {"sha": "8c639a0a87e16f909148d40d752ae2179b93bd96", "filename": "TEST_ARTIFACTS/githooks.yaml", "status": "added", "additions": 1160, "deletions": 0, "changes": 1160, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fgithooks.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fgithooks.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fgithooks.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,1160 @@\n+\n+---\n+apiVersion: apiextensions.k8s.io/v1beta1\n+kind: CustomResourceDefinition\n+metadata:\n+  creationTimestamp: null\n+  name: githooks.tools.pongzt.com\n+spec:\n+  group: tools.pongzt.com\n+  names:\n+    kind: GitHook\n+    plural: githooks\n+  scope: \"\"\n+  validation:\n+    openAPIV3Schema:\n+      description: GitHook is the Schema for the GitHooks API\n+      properties:\n+        apiVersion:\n+          description: 'APIVersion defines the versioned schema of this representation\n+            of an object. Servers should convert recognized schemas to the latest\n+            internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources'\n+          type: string\n+        kind:\n+          description: 'Kind is a string value representing the REST resource this\n+            object represents. Servers may infer this from the endpoint the client\n+            submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds'\n+          type: string\n+        metadata:\n+          properties:\n+            annotations:\n+              additionalProperties:\n+                type: string\n+              description: 'Annotations is an unstructured key value map stored with\n+                a resource that may be set by external tools to store and retrieve\n+                arbitrary metadata. They are not queryable and should be preserved\n+                when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations'\n+              type: object\n+            clusterName:\n+              description: The name of the cluster which the object belongs to. This\n+                is used to distinguish resources with same name and namespace in different\n+                clusters. This field is not set anywhere right now and apiserver is\n+                going to ignore it if set in create or update request.\n+              type: string\n+            creationTimestamp:\n+              description: \"CreationTimestamp is a timestamp representing the server\n+                time when this object was created. It is not guaranteed to be set\n+                in happens-before order across separate operations. Clients may not\n+                set this value. It is represented in RFC3339 form and is in UTC. \\n\n+                Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata\"\n+              format: date-time\n+              type: string\n+            deletionGracePeriodSeconds:\n+              description: Number of seconds allowed for this object to gracefully\n+                terminate before it will be removed from the system. Only set when\n+                deletionTimestamp is also set. May only be shortened. Read-only.\n+              format: int64\n+              type: integer\n+            deletionTimestamp:\n+              description: \"DeletionTimestamp is RFC 3339 date and time at which this\n+                resource will be deleted. This field is set by the server when a graceful\n+                deletion is requested by the user, and is not directly settable by\n+                a client. The resource is expected to be deleted (no longer visible\n+                from resource lists, and not reachable by name) after the time in\n+                this field, once the finalizers list is empty. As long as the finalizers\n+                list contains items, deletion is blocked. Once the deletionTimestamp\n+                is set, this value may not be unset or be set further into the future,\n+                although it may be shortened or the resource may be deleted prior\n+                to this time. For example, a user may request that a pod is deleted\n+                in 30 seconds. The Kubelet will react by sending a graceful termination\n+                signal to the containers in the pod. After that 30 seconds, the Kubelet\n+                will send a hard termination signal (SIGKILL) to the container and\n+                after cleanup, remove the pod from the API. In the presence of network\n+                partitions, this object may still exist after this timestamp, until\n+                an administrator or automated process can determine the resource is\n+                fully terminated. If not set, graceful deletion of the object has\n+                not been requested. \\n Populated by the system when a graceful deletion\n+                is requested. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata\"\n+              format: date-time\n+              type: string\n+            finalizers:\n+              description: Must be empty before the object is deleted from the registry.\n+                Each entry is an identifier for the responsible component that will\n+                remove the entry from the list. If the deletionTimestamp of the object\n+                is non-nil, entries in this list can only be removed.\n+              items:\n+                type: string\n+              type: array\n+            generateName:\n+              description: \"GenerateName is an optional prefix, used by the server,\n+                to generate a unique name ONLY IF the Name field has not been provided.\n+                If this field is used, the name returned to the client will be different\n+                than the name passed. This value will also be combined with a unique\n+                suffix. The provided value has the same validation rules as the Name\n+                field, and may be truncated by the length of the suffix required to\n+                make the value unique on the server. \\n If this field is specified\n+                and the generated name exists, the server will NOT return a 409 -\n+                instead, it will either return 201 Created or 500 with Reason ServerTimeout\n+                indicating a unique name could not be found in the time allotted,\n+                and the client should retry (optionally after the time indicated in\n+                the Retry-After header). \\n Applied only if Name is not specified.\n+                More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#idempotency\"\n+              type: string\n+            generation:\n+              description: A sequence number representing a specific generation of\n+                the desired state. Populated by the system. Read-only.\n+              format: int64\n+              type: integer\n+            initializers:\n+              description: \"An initializer is a controller which enforces some system\n+                invariant at object creation time. This field is a list of initializers\n+                that have not yet acted on this object. If nil or empty, this object\n+                has been completely initialized. Otherwise, the object is considered\n+                uninitialized and is hidden (in list/watch and get calls) from clients\n+                that haven't explicitly asked to observe uninitialized objects. \\n\n+                When an object is created, the system will populate this list with\n+                the current set of initializers. Only privileged users may set or\n+                modify this list. Once it is empty, it may not be modified further\n+                by any user. \\n DEPRECATED - initializers are an alpha field and will\n+                be removed in v1.15.\"\n+              properties:\n+                pending:\n+                  description: Pending is a list of initializers that must execute\n+                    in order before this object is visible. When the last pending\n+                    initializer is removed, and no failing result is set, the initializers\n+                    struct will be set to nil and the object is considered as initialized\n+                    and visible to all clients.\n+                  items:\n+                    properties:\n+                      name:\n+                        description: name of the process that is responsible for initializing\n+                          this object.\n+                        type: string\n+                    required:\n+                    - name\n+                    type: object\n+                  type: array\n+                result:\n+                  description: If result is set with the Failure field, the object\n+                    will be persisted to storage and then deleted, ensuring that other\n+                    clients can observe the deletion.\n+                  properties:\n+                    apiVersion:\n+                      description: 'APIVersion defines the versioned schema of this\n+                        representation of an object. Servers should convert recognized\n+                        schemas to the latest internal value, and may reject unrecognized\n+                        values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources'\n+                      type: string\n+                    code:\n+                      description: Suggested HTTP return code for this status, 0 if\n+                        not set.\n+                      format: int32\n+                      type: integer\n+                    details:\n+                      description: Extended data associated with the reason.  Each\n+                        reason may define its own extended details. This field is\n+                        optional and the data returned is not guaranteed to conform\n+                        to any schema except that defined by the reason type.\n+                      properties:\n+                        causes:\n+                          description: The Causes array includes more details associated\n+                            with the StatusReason failure. Not all StatusReasons may\n+                            provide detailed causes.\n+                          items:\n+                            properties:\n+                              field:\n+                                description: \"The field of the resource that has caused\n+                                  this error, as named by its JSON serialization.\n+                                  May include dot and postfix notation for nested\n+                                  attributes. Arrays are zero-indexed.  Fields may\n+                                  appear more than once in an array of causes due\n+                                  to fields having multiple errors. Optional. \\n Examples:\n+                                  \\  \\\"name\\\" - the field \\\"name\\\" on the current\n+                                  resource   \\\"items[0].name\\\" - the field \\\"name\\\"\n+                                  on the first array entry in \\\"items\\\"\"\n+                                type: string\n+                              message:\n+                                description: A human-readable description of the cause\n+                                  of the error.  This field may be presented as-is\n+                                  to a reader.\n+                                type: string\n+                              reason:\n+                                description: A machine-readable description of the\n+                                  cause of the error. If this value is empty there\n+                                  is no information available.\n+                                type: string\n+                            type: object\n+                          type: array\n+                        group:\n+                          description: The group attribute of the resource associated\n+                            with the status StatusReason.\n+                          type: string\n+                        kind:\n+                          description: 'The kind attribute of the resource associated\n+                            with the status StatusReason. On some operations may differ\n+                            from the requested resource Kind. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds'\n+                          type: string\n+                        name:\n+                          description: The name attribute of the resource associated\n+                            with the status StatusReason (when there is a single name\n+                            which can be described).\n+                          type: string\n+                        retryAfterSeconds:\n+                          description: If specified, the time in seconds before the\n+                            operation should be retried. Some errors may indicate\n+                            the client must take an alternate action - for those errors\n+                            this field may indicate how long to wait before taking\n+                            the alternate action.\n+                          format: int32\n+                          type: integer\n+                        uid:\n+                          description: 'UID of the resource. (when there is a single\n+                            resource which can be described). More info: http://kubernetes.io/docs/user-guide/identifiers#uids'\n+                          type: string\n+                      type: object\n+                    kind:\n+                      description: 'Kind is a string value representing the REST resource\n+                        this object represents. Servers may infer this from the endpoint\n+                        the client submits requests to. Cannot be updated. In CamelCase.\n+                        More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds'\n+                      type: string\n+                    message:\n+                      description: A human-readable description of the status of this\n+                        operation.\n+                      type: string\n+                    metadata:\n+                      description: 'Standard list metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds'\n+                      properties:\n+                        continue:\n+                          description: continue may be set if the user set a limit\n+                            on the number of items returned, and indicates that the\n+                            server has more data available. The value is opaque and\n+                            may be used to issue another request to the endpoint that\n+                            served this list to retrieve the next set of available\n+                            objects. Continuing a consistent list may not be possible\n+                            if the server configuration has changed or more than a\n+                            few minutes have passed. The resourceVersion field returned\n+                            when using this continue value will be identical to the\n+                            value in the first response, unless you have received\n+                            this token from an error message.\n+                          type: string\n+                        resourceVersion:\n+                          description: 'String that identifies the server''s internal\n+                            version of this object that can be used by clients to\n+                            determine when objects have changed. Value must be treated\n+                            as opaque by clients and passed unmodified back to the\n+                            server. Populated by the system. Read-only. More info:\n+                            https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency'\n+                          type: string\n+                        selfLink:\n+                          description: selfLink is a URL representing this object.\n+                            Populated by the system. Read-only.\n+                          type: string\n+                      type: object\n+                    reason:\n+                      description: A machine-readable description of why this operation\n+                        is in the \"Failure\" status. If this value is empty there is\n+                        no information available. A Reason clarifies an HTTP status\n+                        code but does not override it.\n+                      type: string\n+                    status:\n+                      description: 'Status of the operation. One of: \"Success\" or\n+                        \"Failure\". More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status'\n+                      type: string\n+                  type: object\n+              required:\n+              - pending\n+              type: object\n+            labels:\n+              additionalProperties:\n+                type: string\n+              description: 'Map of string keys and values that can be used to organize\n+                and categorize (scope and select) objects. May match selectors of\n+                replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels'\n+              type: object\n+            managedFields:\n+              description: \"ManagedFields maps workflow-id and version to the set\n+                of fields that are managed by that workflow. This is mostly for internal\n+                housekeeping, and users typically shouldn't need to set or understand\n+                this field. A workflow can be the user's name, a controller's name,\n+                or the name of a specific apply path like \\\"ci-cd\\\". The set of fields\n+                is always in the version that the workflow used when modifying the\n+                object. \\n This field is alpha and can be changed or removed without\n+                notice.\"\n+              items:\n+                properties:\n+                  apiVersion:\n+                    description: APIVersion defines the version of this resource that\n+                      this field set applies to. The format is \"group/version\" just\n+                      like the top-level APIVersion field. It is necessary to track\n+                      the version of a field set because it cannot be automatically\n+                      converted.\n+                    type: string\n+                  fields:\n+                    additionalProperties: true\n+                    description: Fields identifies a set of fields.\n+                    type: object\n+                  manager:\n+                    description: Manager is an identifier of the workflow managing\n+                      these fields.\n+                    type: string\n+                  operation:\n+                    description: Operation is the type of operation which lead to\n+                      this ManagedFieldsEntry being created. The only valid values\n+                      for this field are 'Apply' and 'Update'.\n+                    type: string\n+                  time:\n+                    description: Time is timestamp of when these fields were set.\n+                      It should always be empty if Operation is 'Apply'\n+                    format: date-time\n+                    type: string\n+                type: object\n+              type: array\n+            name:\n+              description: 'Name must be unique within a namespace. Is required when\n+                creating resources, although some resources may allow a client to\n+                request the generation of an appropriate name automatically. Name\n+                is primarily intended for creation idempotence and configuration definition.\n+                Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names'\n+              type: string\n+            namespace:\n+              description: \"Namespace defines the space within each name must be unique.\n+                An empty namespace is equivalent to the \\\"default\\\" namespace, but\n+                \\\"default\\\" is the canonical representation. Not all objects are required\n+                to be scoped to a namespace - the value of this field for those objects\n+                will be empty. \\n Must be a DNS_LABEL. Cannot be updated. More info:\n+                http://kubernetes.io/docs/user-guide/namespaces\"\n+              type: string\n+            ownerReferences:\n+              description: List of objects depended by this object. If ALL objects\n+                in the list have been deleted, this object will be garbage collected.\n+                If this object is managed by a controller, then an entry in this list\n+                will point to this controller, with the controller field set to true.\n+                There cannot be more than one managing controller.\n+              items:\n+                properties:\n+                  apiVersion:\n+                    description: API version of the referent.\n+                    type: string\n+                  blockOwnerDeletion:\n+                    description: If true, AND if the owner has the \"foregroundDeletion\"\n+                      finalizer, then the owner cannot be deleted from the key-value\n+                      store until this reference is removed. Defaults to false. To\n+                      set this field, a user needs \"delete\" permission of the owner,\n+                      otherwise 422 (Unprocessable Entity) will be returned.\n+                    type: boolean\n+                  controller:\n+                    description: If true, this reference points to the managing controller.\n+                    type: boolean\n+                  kind:\n+                    description: 'Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds'\n+                    type: string\n+                  name:\n+                    description: 'Name of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#names'\n+                    type: string\n+                  uid:\n+                    description: 'UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids'\n+                    type: string\n+                required:\n+                - apiVersion\n+                - kind\n+                - name\n+                - uid\n+                type: object\n+              type: array\n+            resourceVersion:\n+              description: \"An opaque value that represents the internal version of\n+                this object that can be used by clients to determine when objects\n+                have changed. May be used for optimistic concurrency, change detection,\n+                and the watch operation on a resource or set of resources. Clients\n+                must treat these values as opaque and passed unmodified back to the\n+                server. They may only be valid for a particular resource or set of\n+                resources. \\n Populated by the system. Read-only. Value must be treated\n+                as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency\"\n+              type: string\n+            selfLink:\n+              description: SelfLink is a URL representing this object. Populated by\n+                the system. Read-only.\n+              type: string\n+            uid:\n+              description: \"UID is the unique in time and space value for this object.\n+                It is typically generated by the server on successful creation of\n+                a resource and is not allowed to change on PUT operations. \\n Populated\n+                by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids\"\n+              type: string\n+          type: object\n+        spec:\n+          properties:\n+            accessToken:\n+              description: AccessToken is the Kubernetes secret containing the Gogs\n+                access token\n+              properties:\n+                secretKeyRef:\n+                  description: The Secret key to select from.\n+                  properties:\n+                    key:\n+                      description: The key of the secret to select from.  Must be\n+                        a valid secret key.\n+                      type: string\n+                    name:\n+                      description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names\n+                        TODO: Add other useful fields. apiVersion, kind, uid?'\n+                      type: string\n+                    optional:\n+                      description: Specify whether the Secret or it's key must be\n+                        defined\n+                      type: boolean\n+                  required:\n+                  - key\n+                  type: object\n+              type: object\n+            eventTypes:\n+              description: EventType is the type of event to receive from Gogs. These\n+                correspond to supported events to the add project hook\n+              items:\n+                enum:\n+                - create\n+                - delete\n+                - fork\n+                - push\n+                - issues\n+                - issue_comment\n+                - pull_request\n+                - release\n+                type: string\n+              minItems: 1\n+              type: array\n+            gitProvider:\n+              description: GitProvder is the name of the git source in which we would\n+                like register webhook\n+              enum:\n+              - gitlab\n+              - github\n+              - gogs\n+              type: string\n+            projectUrl:\n+              description: 'ProjectUrl is the url of the git project for which we\n+                are interested to receive events from. Examples:   https://gitlab.com/pongsatt/githook'\n+              minLength: 1\n+              type: string\n+            runspec:\n+              description: RunSpec is a tekton pipelinerun spec to be run when events\n+                triggered\n+              properties:\n+                affinity:\n+                  description: If specified, the pod's scheduling constraints\n+                  properties:\n+                    nodeAffinity:\n+                      description: Describes node affinity scheduling rules for the\n+                        pod.\n+                      properties:\n+                        preferredDuringSchedulingIgnoredDuringExecution:\n+                          description: The scheduler will prefer to schedule pods\n+                            to nodes that satisfy the affinity expressions specified\n+                            by this field, but it may choose a node that violates\n+                            one or more of the expressions. The node that is most\n+                            preferred is the one with the greatest sum of weights,\n+                            i.e. for each node that meets all of the scheduling requirements\n+                            (resource request, requiredDuringScheduling affinity expressions,\n+                            etc.), compute a sum by iterating through the elements\n+                            of this field and adding \"weight\" to the sum if the node\n+                            matches the corresponding matchExpressions; the node(s)\n+                            with the highest sum are the most preferred.\n+                          items:\n+                            properties:\n+                              preference:\n+                                description: A node selector term, associated with\n+                                  the corresponding weight.\n+                                properties:\n+                                  matchExpressions:\n+                                    description: A list of node selector requirements\n+                                      by node's labels.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: The label key that the selector\n+                                            applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: Represents a key's relationship\n+                                            to a set of values. Valid operators are\n+                                            In, NotIn, Exists, DoesNotExist. Gt, and\n+                                            Lt.\n+                                          type: string\n+                                        values:\n+                                          description: An array of string values.\n+                                            If the operator is In or NotIn, the values\n+                                            array must be non-empty. If the operator\n+                                            is Exists or DoesNotExist, the values\n+                                            array must be empty. If the operator is\n+                                            Gt or Lt, the values array must have a\n+                                            single element, which will be interpreted\n+                                            as an integer. This array is replaced\n+                                            during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                  matchFields:\n+                                    description: A list of node selector requirements\n+                                      by node's fields.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: The label key that the selector\n+                                            applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: Represents a key's relationship\n+                                            to a set of values. Valid operators are\n+                                            In, NotIn, Exists, DoesNotExist. Gt, and\n+                                            Lt.\n+                                          type: string\n+                                        values:\n+                                          description: An array of string values.\n+                                            If the operator is In or NotIn, the values\n+                                            array must be non-empty. If the operator\n+                                            is Exists or DoesNotExist, the values\n+                                            array must be empty. If the operator is\n+                                            Gt or Lt, the values array must have a\n+                                            single element, which will be interpreted\n+                                            as an integer. This array is replaced\n+                                            during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                type: object\n+                              weight:\n+                                description: Weight associated with matching the corresponding\n+                                  nodeSelectorTerm, in the range 1-100.\n+                                format: int32\n+                                type: integer\n+                            required:\n+                            - weight\n+                            - preference\n+                            type: object\n+                          type: array\n+                        requiredDuringSchedulingIgnoredDuringExecution:\n+                          description: If the affinity requirements specified by this\n+                            field are not met at scheduling time, the pod will not\n+                            be scheduled onto the node. If the affinity requirements\n+                            specified by this field cease to be met at some point\n+                            during pod execution (e.g. due to an update), the system\n+                            may or may not try to eventually evict the pod from its\n+                            node.\n+                          properties:\n+                            nodeSelectorTerms:\n+                              description: Required. A list of node selector terms.\n+                                The terms are ORed.\n+                              items:\n+                                properties:\n+                                  matchExpressions:\n+                                    description: A list of node selector requirements\n+                                      by node's labels.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: The label key that the selector\n+                                            applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: Represents a key's relationship\n+                                            to a set of values. Valid operators are\n+                                            In, NotIn, Exists, DoesNotExist. Gt, and\n+                                            Lt.\n+                                          type: string\n+                                        values:\n+                                          description: An array of string values.\n+                                            If the operator is In or NotIn, the values\n+                                            array must be non-empty. If the operator\n+                                            is Exists or DoesNotExist, the values\n+                                            array must be empty. If the operator is\n+                                            Gt or Lt, the values array must have a\n+                                            single element, which will be interpreted\n+                                            as an integer. This array is replaced\n+                                            during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                  matchFields:\n+                                    description: A list of node selector requirements\n+                                      by node's fields.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: The label key that the selector\n+                                            applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: Represents a key's relationship\n+                                            to a set of values. Valid operators are\n+                                            In, NotIn, Exists, DoesNotExist. Gt, and\n+                                            Lt.\n+                                          type: string\n+                                        values:\n+                                          description: An array of string values.\n+                                            If the operator is In or NotIn, the values\n+                                            array must be non-empty. If the operator\n+                                            is Exists or DoesNotExist, the values\n+                                            array must be empty. If the operator is\n+                                            Gt or Lt, the values array must have a\n+                                            single element, which will be interpreted\n+                                            as an integer. This array is replaced\n+                                            during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                type: object\n+                              type: array\n+                          required:\n+                          - nodeSelectorTerms\n+                          type: object\n+                      type: object\n+                    podAffinity:\n+                      description: Describes pod affinity scheduling rules (e.g. co-locate\n+                        this pod in the same node, zone, etc. as some other pod(s)).\n+                      properties:\n+                        preferredDuringSchedulingIgnoredDuringExecution:\n+                          description: The scheduler will prefer to schedule pods\n+                            to nodes that satisfy the affinity expressions specified\n+                            by this field, but it may choose a node that violates\n+                            one or more of the expressions. The node that is most\n+                            preferred is the one with the greatest sum of weights,\n+                            i.e. for each node that meets all of the scheduling requirements\n+                            (resource request, requiredDuringScheduling affinity expressions,\n+                            etc.), compute a sum by iterating through the elements\n+                            of this field and adding \"weight\" to the sum if the node\n+                            has pods which matches the corresponding podAffinityTerm;\n+                            the node(s) with the highest sum are the most preferred.\n+                          items:\n+                            properties:\n+                              podAffinityTerm:\n+                                description: Required. A pod affinity term, associated\n+                                  with the corresponding weight.\n+                                properties:\n+                                  labelSelector:\n+                                    description: A label query over a set of resources,\n+                                      in this case pods.\n+                                    properties:\n+                                      matchExpressions:\n+                                        description: matchExpressions is a list of\n+                                          label selector requirements. The requirements\n+                                          are ANDed.\n+                                        items:\n+                                          properties:\n+                                            key:\n+                                              description: key is the label key that\n+                                                the selector applies to.\n+                                              type: string\n+                                            operator:\n+                                              description: operator represents a key's\n+                                                relationship to a set of values. Valid\n+                                                operators are In, NotIn, Exists and\n+                                                DoesNotExist.\n+                                              type: string\n+                                            values:\n+                                              description: values is an array of string\n+                                                values. If the operator is In or NotIn,\n+                                                the values array must be non-empty.\n+                                                If the operator is Exists or DoesNotExist,\n+                                                the values array must be empty. This\n+                                                array is replaced during a strategic\n+                                                merge patch.\n+                                              items:\n+                                                type: string\n+                                              type: array\n+                                          required:\n+                                          - key\n+                                          - operator\n+                                          type: object\n+                                        type: array\n+                                      matchLabels:\n+                                        additionalProperties:\n+                                          type: string\n+                                        description: matchLabels is a map of {key,value}\n+                                          pairs. A single {key,value} in the matchLabels\n+                                          map is equivalent to an element of matchExpressions,\n+                                          whose key field is \"key\", the operator is\n+                                          \"In\", and the values array contains only\n+                                          \"value\". The requirements are ANDed.\n+                                        type: object\n+                                    type: object\n+                                  namespaces:\n+                                    description: namespaces specifies which namespaces\n+                                      the labelSelector applies to (matches against);\n+                                      null or empty list means \"this pod's namespace\"\n+                                    items:\n+                                      type: string\n+                                    type: array\n+                                  topologyKey:\n+                                    description: This pod should be co-located (affinity)\n+                                      or not co-located (anti-affinity) with the pods\n+                                      matching the labelSelector in the specified\n+                                      namespaces, where co-located is defined as running\n+                                      on a node whose value of the label with key\n+                                      topologyKey matches that of any node on which\n+                                      any of the selected pods is running. Empty topologyKey\n+                                      is not allowed.\n+                                    type: string\n+                                required:\n+                                - topologyKey\n+                                type: object\n+                              weight:\n+                                description: weight associated with matching the corresponding\n+                                  podAffinityTerm, in the range 1-100.\n+                                format: int32\n+                                type: integer\n+                            required:\n+                            - weight\n+                            - podAffinityTerm\n+                            type: object\n+                          type: array\n+                        requiredDuringSchedulingIgnoredDuringExecution:\n+                          description: If the affinity requirements specified by this\n+                            field are not met at scheduling time, the pod will not\n+                            be scheduled onto the node. If the affinity requirements\n+                            specified by this field cease to be met at some point\n+                            during pod execution (e.g. due to a pod label update),\n+                            the system may or may not try to eventually evict the\n+                            pod from its node. When there are multiple elements, the\n+                            lists of nodes corresponding to each podAffinityTerm are\n+                            intersected, i.e. all terms must be satisfied.\n+                          items:\n+                            properties:\n+                              labelSelector:\n+                                description: A label query over a set of resources,\n+                                  in this case pods.\n+                                properties:\n+                                  matchExpressions:\n+                                    description: matchExpressions is a list of label\n+                                      selector requirements. The requirements are\n+                                      ANDed.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: key is the label key that the\n+                                            selector applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: operator represents a key's\n+                                            relationship to a set of values. Valid\n+                                            operators are In, NotIn, Exists and DoesNotExist.\n+                                          type: string\n+                                        values:\n+                                          description: values is an array of string\n+                                            values. If the operator is In or NotIn,\n+                                            the values array must be non-empty. If\n+                                            the operator is Exists or DoesNotExist,\n+                                            the values array must be empty. This array\n+                                            is replaced during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                  matchLabels:\n+                                    additionalProperties:\n+                                      type: string\n+                                    description: matchLabels is a map of {key,value}\n+                                      pairs. A single {key,value} in the matchLabels\n+                                      map is equivalent to an element of matchExpressions,\n+                                      whose key field is \"key\", the operator is \"In\",\n+                                      and the values array contains only \"value\".\n+                                      The requirements are ANDed.\n+                                    type: object\n+                                type: object\n+                              namespaces:\n+                                description: namespaces specifies which namespaces\n+                                  the labelSelector applies to (matches against);\n+                                  null or empty list means \"this pod's namespace\"\n+                                items:\n+                                  type: string\n+                                type: array\n+                              topologyKey:\n+                                description: This pod should be co-located (affinity)\n+                                  or not co-located (anti-affinity) with the pods\n+                                  matching the labelSelector in the specified namespaces,\n+                                  where co-located is defined as running on a node\n+                                  whose value of the label with key topologyKey matches\n+                                  that of any node on which any of the selected pods\n+                                  is running. Empty topologyKey is not allowed.\n+                                type: string\n+                            required:\n+                            - topologyKey\n+                            type: object\n+                          type: array\n+                      type: object\n+                    podAntiAffinity:\n+                      description: Describes pod anti-affinity scheduling rules (e.g.\n+                        avoid putting this pod in the same node, zone, etc. as some\n+                        other pod(s)).\n+                      properties:\n+                        preferredDuringSchedulingIgnoredDuringExecution:\n+                          description: The scheduler will prefer to schedule pods\n+                            to nodes that satisfy the anti-affinity expressions specified\n+                            by this field, but it may choose a node that violates\n+                            one or more of the expressions. The node that is most\n+                            preferred is the one with the greatest sum of weights,\n+                            i.e. for each node that meets all of the scheduling requirements\n+                            (resource request, requiredDuringScheduling anti-affinity\n+                            expressions, etc.), compute a sum by iterating through\n+                            the elements of this field and adding \"weight\" to the\n+                            sum if the node has pods which matches the corresponding\n+                            podAffinityTerm; the node(s) with the highest sum are\n+                            the most preferred.\n+                          items:\n+                            properties:\n+                              podAffinityTerm:\n+                                description: Required. A pod affinity term, associated\n+                                  with the corresponding weight.\n+                                properties:\n+                                  labelSelector:\n+                                    description: A label query over a set of resources,\n+                                      in this case pods.\n+                                    properties:\n+                                      matchExpressions:\n+                                        description: matchExpressions is a list of\n+                                          label selector requirements. The requirements\n+                                          are ANDed.\n+                                        items:\n+                                          properties:\n+                                            key:\n+                                              description: key is the label key that\n+                                                the selector applies to.\n+                                              type: string\n+                                            operator:\n+                                              description: operator represents a key's\n+                                                relationship to a set of values. Valid\n+                                                operators are In, NotIn, Exists and\n+                                                DoesNotExist.\n+                                              type: string\n+                                            values:\n+                                              description: values is an array of string\n+                                                values. If the operator is In or NotIn,\n+                                                the values array must be non-empty.\n+                                                If the operator is Exists or DoesNotExist,\n+                                                the values array must be empty. This\n+                                                array is replaced during a strategic\n+                                                merge patch.\n+                                              items:\n+                                                type: string\n+                                              type: array\n+                                          required:\n+                                          - key\n+                                          - operator\n+                                          type: object\n+                                        type: array\n+                                      matchLabels:\n+                                        additionalProperties:\n+                                          type: string\n+                                        description: matchLabels is a map of {key,value}\n+                                          pairs. A single {key,value} in the matchLabels\n+                                          map is equivalent to an element of matchExpressions,\n+                                          whose key field is \"key\", the operator is\n+                                          \"In\", and the values array contains only\n+                                          \"value\". The requirements are ANDed.\n+                                        type: object\n+                                    type: object\n+                                  namespaces:\n+                                    description: namespaces specifies which namespaces\n+                                      the labelSelector applies to (matches against);\n+                                      null or empty list means \"this pod's namespace\"\n+                                    items:\n+                                      type: string\n+                                    type: array\n+                                  topologyKey:\n+                                    description: This pod should be co-located (affinity)\n+                                      or not co-located (anti-affinity) with the pods\n+                                      matching the labelSelector in the specified\n+                                      namespaces, where co-located is defined as running\n+                                      on a node whose value of the label with key\n+                                      topologyKey matches that of any node on which\n+                                      any of the selected pods is running. Empty topologyKey\n+                                      is not allowed.\n+                                    type: string\n+                                required:\n+                                - topologyKey\n+                                type: object\n+                              weight:\n+                                description: weight associated with matching the corresponding\n+                                  podAffinityTerm, in the range 1-100.\n+                                format: int32\n+                                type: integer\n+                            required:\n+                            - weight\n+                            - podAffinityTerm\n+                            type: object\n+                          type: array\n+                        requiredDuringSchedulingIgnoredDuringExecution:\n+                          description: If the anti-affinity requirements specified\n+                            by this field are not met at scheduling time, the pod\n+                            will not be scheduled onto the node. If the anti-affinity\n+                            requirements specified by this field cease to be met at\n+                            some point during pod execution (e.g. due to a pod label\n+                            update), the system may or may not try to eventually evict\n+                            the pod from its node. When there are multiple elements,\n+                            the lists of nodes corresponding to each podAffinityTerm\n+                            are intersected, i.e. all terms must be satisfied.\n+                          items:\n+                            properties:\n+                              labelSelector:\n+                                description: A label query over a set of resources,\n+                                  in this case pods.\n+                                properties:\n+                                  matchExpressions:\n+                                    description: matchExpressions is a list of label\n+                                      selector requirements. The requirements are\n+                                      ANDed.\n+                                    items:\n+                                      properties:\n+                                        key:\n+                                          description: key is the label key that the\n+                                            selector applies to.\n+                                          type: string\n+                                        operator:\n+                                          description: operator represents a key's\n+                                            relationship to a set of values. Valid\n+                                            operators are In, NotIn, Exists and DoesNotExist.\n+                                          type: string\n+                                        values:\n+                                          description: values is an array of string\n+                                            values. If the operator is In or NotIn,\n+                                            the values array must be non-empty. If\n+                                            the operator is Exists or DoesNotExist,\n+                                            the values array must be empty. This array\n+                                            is replaced during a strategic merge patch.\n+                                          items:\n+                                            type: string\n+                                          type: array\n+                                      required:\n+                                      - key\n+                                      - operator\n+                                      type: object\n+                                    type: array\n+                                  matchLabels:\n+                                    additionalProperties:\n+                                      type: string\n+                                    description: matchLabels is a map of {key,value}\n+                                      pairs. A single {key,value} in the matchLabels\n+                                      map is equivalent to an element of matchExpressions,\n+                                      whose key field is \"key\", the operator is \"In\",\n+                                      and the values array contains only \"value\".\n+                                      The requirements are ANDed.\n+                                    type: object\n+                                type: object\n+                              namespaces:\n+                                description: namespaces specifies which namespaces\n+                                  the labelSelector applies to (matches against);\n+                                  null or empty list means \"this pod's namespace\"\n+                                items:\n+                                  type: string\n+                                type: array\n+                              topologyKey:\n+                                description: This pod should be co-located (affinity)\n+                                  or not co-located (anti-affinity) with the pods\n+                                  matching the labelSelector in the specified namespaces,\n+                                  where co-located is defined as running on a node\n+                                  whose value of the label with key topologyKey matches\n+                                  that of any node on which any of the selected pods\n+                                  is running. Empty topologyKey is not allowed.\n+                                type: string\n+                            required:\n+                            - topologyKey\n+                            type: object\n+                          type: array\n+                      type: object\n+                  type: object\n+                nodeSelector:\n+                  additionalProperties:\n+                    type: string\n+                  description: 'NodeSelector is a selector which must be true for\n+                    the pod to fit on a node. Selector which must match a node''s\n+                    labels for the pod to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/'\n+                  type: object\n+                params:\n+                  description: Params is a list of parameter names and values.\n+                  items:\n+                    properties:\n+                      name:\n+                        type: string\n+                      value:\n+                        type: string\n+                    required:\n+                    - name\n+                    - value\n+                    type: object\n+                  type: array\n+                pipelineRef:\n+                  properties:\n+                    apiVersion:\n+                      description: API version of the referent\n+                      type: string\n+                    name:\n+                      description: 'Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names'\n+                      type: string\n+                  type: object\n+                resources:\n+                  description: Resources is a list of bindings specifying which actual\n+                    instances of PipelineResources to use for the resources the Pipeline\n+                    has declared it needs.\n+                  items:\n+                    properties:\n+                      name:\n+                        description: Name is the name of the PipelineResource in the\n+                          Pipeline's declaration\n+                        type: string\n+                      resourceRef:\n+                        description: ResourceRef is a reference to the instance of\n+                          the actual PipelineResource that should be used\n+                        properties:\n+                          apiVersion:\n+                            description: API version of the referent\n+                            type: string\n+                          name:\n+                            description: 'Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names'\n+                            type: string\n+                        type: object\n+                    type: object\n+                  type: array\n+                results:\n+                  properties:\n+                    type:\n+                      type: string\n+                    url:\n+                      type: string\n+                  required:\n+                  - type\n+                  - url\n+                  type: object\n+                serviceAccount:\n+                  type: string\n+                status:\n+                  description: Used for cancelling a pipelinerun (and maybe more later\n+                    on)\n+                  type: string\n+                timeout:\n+                  description: 'Time after which the Pipeline times out. Defaults\n+                    to never. Refer to Go''s ParseDuration documentation for expected\n+                    format: https://golang.org/pkg/time/#ParseDuration'\n+                  type: string\n+                tolerations:\n+                  description: If specified, the pod's tolerations.\n+                  items:\n+                    properties:\n+                      effect:\n+                        description: Effect indicates the taint effect to match. Empty\n+                          means match all taint effects. When specified, allowed values\n+                          are NoSchedule, PreferNoSchedule and NoExecute.\n+                        type: string\n+                      key:\n+                        description: Key is the taint key that the toleration applies\n+                          to. Empty means match all taint keys. If the key is empty,\n+                          operator must be Exists; this combination means to match\n+                          all values and all keys.\n+                        type: string\n+                      operator:\n+                        description: Operator represents a key's relationship to the\n+                          value. Valid operators are Exists and Equal. Defaults to\n+                          Equal. Exists is equivalent to wildcard for value, so that\n+                          a pod can tolerate all taints of a particular category.\n+                        type: string\n+                      tolerationSeconds:\n+                        description: TolerationSeconds represents the period of time\n+                          the toleration (which must be of effect NoExecute, otherwise\n+                          this field is ignored) tolerates the taint. By default,\n+                          it is not set, which means tolerate the taint forever (do\n+                          not evict). Zero and negative values will be treated as\n+                          0 (evict immediately) by the system.\n+                        format: int64\n+                        type: integer\n+                      value:\n+                        description: Value is the taint value the toleration matches\n+                          to. If the operator is Exists, the value should be empty,\n+                          otherwise just a regular string.\n+                        type: string\n+                    type: object\n+                  type: array\n+              required:\n+              - pipelineRef\n+              - serviceAccount\n+              type: object\n+            secretToken:\n+              description: SecretToken is the Kubernetes secret containing the Gogs\n+                secret token\n+              properties:\n+                secretKeyRef:\n+                  description: The Secret key to select from.\n+                  properties:\n+                    key:\n+                      description: The key of the secret to select from.  Must be\n+                        a valid secret key.\n+                      type: string\n+                    name:\n+                      description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names\n+                        TODO: Add other useful fields. apiVersion, kind, uid?'\n+                      type: string\n+                    optional:\n+                      description: Specify whether the Secret or it's key must be\n+                        defined\n+                      type: boolean\n+                  required:\n+                  - key\n+                  type: object\n+              type: object\n+            serviceAccountName:\n+              description: ServiceAccountName holds the name of the Kubernetes service\n+                account as which the underlying K8s resources should be run. If unspecified\n+                this will default to the \"default\" service account for the namespace\n+                in which the GitHook exists.\n+              type: string\n+            sslverify:\n+              description: SslVerify if true configure webhook so the ssl verification\n+                is done when triggering the hook\n+              type: boolean\n+          required:\n+          - projectUrl\n+          - gitProvider\n+          - eventTypes\n+          - accessToken\n+          - secretToken\n+          - runspec\n+          type: object\n+        status:\n+          properties:\n+            Id:\n+              description: ID of the project hook registered with Gogs\n+              type: string\n+          type: object\n+      type: object\n+  versions:\n+  - name: v1alpha1\n+    served: true\n+    storage: true\n+status:\n+  acceptedNames:\n+    kind: \"\"\n+    plural: \"\"\n+  conditions: []\n+  storedVersions: []"}, {"sha": "8af5868d80afad7a3d83360bede24694b914ad9e", "filename": "TEST_ARTIFACTS/hello.values.yaml", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhello.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhello.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fhello.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,68 @@\n+apps:\n+  coffee:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 2\n+  tea:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 3\n+\n+ingress:\n+  name: cafe-ingress\n+  rules:\n+  - host: cafe.example.com\n+    paths:\n+    - path: /tea\n+      app: tea\n+    - path: /coffee\n+      app: coffee\n+  tlsSecrets:\n+  - name: cafe-tls-secret\n+    crt: |\n+      -----BEGIN CERTIFICATE-----\n+      MIIDWTCCAkECFHb8EN0l0QwiR4eKKIW6h172z+JrMA0GCSqGSIb3DQEBCwUAMGgx\n+      CzAJBgNVBAYTAkRFMRAwDgYDVQQIDAdIYW1idXJnMRAwDgYDVQQHDAdIYW1idXJn\n+      MRowGAYDVQQKDBFHcmVlbiBNaWRnZXQgQ2FmZTEZMBcGA1UEAwwQY2FmZS5leGFt\n+      cGxlLmNvbTAgFw0yMDA1MDQxNzA5NTlaGA8yMTIwMDQxMDE3MDk1OVowaDELMAkG\n+      A1UEBhMCREUxEDAOBgNVBAgMB0hhbWJ1cmcxEDAOBgNVBAcMB0hhbWJ1cmcxGjAY\n+      BgNVBAoMEUdyZWVuIE1pZGdldCBDYWZlMRkwFwYDVQQDDBBjYWZlLmV4YW1wbGUu\n+      Y29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAts0HCq6fq9gv0uEa\n+      3iOruZ3GnctdCoeGjrQQ4Fh2cQoMm/i3pkDUt6x2pLTQhxlN3oH3WEo1a24r/3S8\n+      Xfy6Xf0Pti+dDiCqAwMd6veu56RItVMO1pmx1wDjGFTuplpnPRtz8EKsaKYfjZd1\n+      BabdhkWhsA9g3nns8+lqeNbvebhk7hiv9lpgDWAnBie+hioan4WQdPZm1/bANH6o\n+      +oWDu1o6Gdrk/iaj2pR73VTFsR2UEmSTpXa35W7/nsmgADIc4RovU+9ho1I4/fSy\n+      jgVlZVBz29yLaDyNuoZljzNhvGqq1wW6Jq/v1uBOPxNH1k3ZQJl4jlG0tsoASnm7\n+      mr9hewIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQBMNCVYMTdlaNaTjJ5Cznk9Gd+u\n+      TSIFmOCetTOt3l0Xe0bSTxboT6Oz9nFDMP2A2HRK/GTp25ec+Ek1iiCIF47RcsGp\n+      Cdug+x4wQVP3pxakJ/odFN1ReZGZCjNwBltxlRXwJhArK5PWmQppmMZPrW1UYW8y\n+      x+m5UREzOzWga6EIlhpMEfgNa0BNCL/2gPaz2MpKXq5We93IDe2O0nlRrrVoDHU2\n+      GFMhTpWSLkloaMzIMlcKR0IGyezG9waVgsliS00bYKp8eRJ5SqCUYvCMuApjoyzW\n+      N2w59p6t5xE7Ktb0cmhZg83ISPTBlGqVJxF0clLob5nWyeutXNkP/KOi38PI\n+      -----END CERTIFICATE-----\n+    key: |\n+      -----BEGIN RSA PRIVATE KEY-----\n+      MIIEpAIBAAKCAQEAts0HCq6fq9gv0uEa3iOruZ3GnctdCoeGjrQQ4Fh2cQoMm/i3\n+      pkDUt6x2pLTQhxlN3oH3WEo1a24r/3S8Xfy6Xf0Pti+dDiCqAwMd6veu56RItVMO\n+      1pmx1wDjGFTuplpnPRtz8EKsaKYfjZd1BabdhkWhsA9g3nns8+lqeNbvebhk7hiv\n+      9lpgDWAnBie+hioan4WQdPZm1/bANH6o+oWDu1o6Gdrk/iaj2pR73VTFsR2UEmST\n+      pXa35W7/nsmgADIc4RovU+9ho1I4/fSyjgVlZVBz29yLaDyNuoZljzNhvGqq1wW6\n+      Jq/v1uBOPxNH1k3ZQJl4jlG0tsoASnm7mr9hewIDAQABAoIBAES7vsQTeNIijYjb\n+      P0D7ZJx8aKv4RVmqL7wElLvmR1KllqwmztbiVZlibZHssuO5bgAWGizGamOkn0KE\n+      YDduyZyBhKDaMlGXkpVjXKJ20vsiWHxlaJTkYWwYV0tU1A8UuvDNG8DhMPaAUCjr\n+      JAMmBPFxySPsBF5itefYgkJBfvXi7sobaCM6A75D+dBLMeq2q+YbIQH/cAojHYfV\n+      7ypyQ1QaY+wsDiCM6n9Qjk4krmHZ/z39y8mO71ytFcMfJJad8LKM5J4p9Qu99qeb\n+      IRDOT/Sb9QXLXWTeCDv5JWPYyFH2u3e/8GsvQLbXYYbfWLNoU6RDaFSc2wmkOwUH\n+      U8pSCDECgYEA3KIQcme//6B2jP31Coa2f8hsENd0nL+EDR9erXLSUga2l0YNPJZj\n+      W6VnNdaeGq92B7Wxgj+dSeeSBdIRhXwABOHHjruG+gotdRRyoO1ldw7mJjN/q3Wx\n+      A1fpJ+J00S1ZO1FbukKZmR7smTS7i73a8V7At3dyjCG6WxErP3N5NM8CgYEA1Bp5\n+      yYIH8oJmPsuJt501k9nU4SdxxQJpb6uZ9QCBqbEsGkWE3vtLErlU8Rnm2HuirMvD\n+      8Q3OsuoupdCTChrJJ04oL/2r60oTGapeDe4BuRM+DRAZ2trCwXy3nT26bZ/DJtur\n+      Hqvt0tey9ee9MiVHWF2biZejd+KMUxPCCoZVS5UCgYEApbz8m+SCH3Yb+DgB7oFZ\n+      8M3PGCuxxto7SVxKVANQKRwv551Q7jWOt9adnJz3Mdai1JHRoaVF87GISOUQEnUe\n+      0owEy5zlfUlN8oiEv4z1zqUbkJDZFCUZ7wgH9tUvqb7mLCAmxtmm5paLZ19sj0H0\n+      iaMDJA8PtmLTyfswwL5uy5MCgYEArdBMgU+nx5oIw+j0IJ4aK+FUzHYQi4vgb3zG\n+      m7ogh7kDFTxnGHwCF4P9Ed9SB5G5y7ToC4BvJLs4IvX7qUouEaHA2SMeYaDAakXs\n+      8albjBkyvm21Yl3nP7w+lALj5bYIrK1TW701FZVhuJaBurhF8So0rdqwQSxMJkCI\n+      wSs4dskCgYBr1LO3GINSwGHt73ueZDtnvFvO+EFDaOFFbsEd14O1mluM4+WrIZky\n+      inZCvygJWzgHF9LCOpoAZxHykMNrEomidpxViAlpBzb/C5CnpzlfiVBqLN3NvOxG\n+      zdkoq6BiZnznsVgoHyP7TQlUX94ahVT01yZ0njPk2aYVipPWUoHQMQ==\n+      -----END RSA PRIVATE KEY-----"}, {"sha": "bff501acc71f8844d26474372bf45613e7af9234", "filename": "TEST_ARTIFACTS/helm.values.yaml", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhelm.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhelm.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fhelm.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1 @@\n+# redacted"}, {"sha": "3ba3d915f8529bca6fc474dcd699ad682fa0b671", "filename": "TEST_ARTIFACTS/host-ipc-pid-true.yaml", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhost-ipc-pid-true.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhost-ipc-pid-true.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fhost-ipc-pid-true.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,11 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: pod-with-host-pid-and-ipc\n+spec:\n+  hostPID: true\n+  hostIPC: true\n+  containers:\n+  - name: main\n+    image: alpine\n+    command: [\"/bin/sleep\", \"999999\"]"}, {"sha": "c242973d6ae00ae895c62149e87679adca82c59e", "filename": "TEST_ARTIFACTS/host-net-tp.yaml", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhost-net-tp.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fhost-net-tp.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fhost-net-tp.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,80 @@\n+apiVersion: apps/v1beta2\n+kind: DaemonSet\n+metadata:\n+  labels:\n+    app: node-exporter\n+  name: node-exporter\n+  namespace: nn-mon\n+spec:\n+  selector:\n+    matchLabels:\n+      app: node-exporter\n+  template:\n+    metadata:\n+      labels:\n+        app: node-exporter\n+    spec:\n+      containers:\n+      - args:\n+        - --web.listen-address=127.0.0.1:9101\n+        - --path.procfs=/host/proc\n+        - --path.sysfs=/host/sys\n+        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)\n+        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$\n+        image: quay.io/prometheus/node-exporter:v0.16.0\n+        name: node-exporter\n+        resources:\n+          limits:\n+            cpu: 102m\n+            memory: 180Mi\n+          requests:\n+            cpu: 102m\n+            memory: 180Mi\n+        volumeMounts:\n+        - mountPath: /host/proc\n+          name: proc\n+          readOnly: false\n+        - mountPath: /host/sys\n+          name: sys\n+          readOnly: false\n+        - mountPath: /host/root\n+          mountPropagation: HostToContainer\n+          name: root\n+          readOnly: true\n+      - args:\n+        - --secure-listen-address=:9100\n+        - --upstream=http://127.0.0.1:9101/\n+        image: quay.io/coreos/kube-rbac-proxy:v0.3.1\n+        name: kube-rbac-proxy\n+        ports:\n+        - containerPort: 9100\n+          hostPort: 9100\n+          name: https\n+        resources:\n+          limits:\n+            cpu: 20m\n+            memory: 40Mi\n+          requests:\n+            cpu: 10m\n+            memory: 20Mi\n+      hostNetwork: true\n+      hostPID: true\n+      nodeSelector:\n+        beta.kubernetes.io/os: linux\n+      securityContext:\n+        runAsNonRoot: true\n+        runAsUser: 65534\n+      serviceAccountName: node-exporter\n+      tolerations:\n+      - effect: NoSchedule\n+        key: node-role.kubernetes.io/master\n+      volumes:\n+      - hostPath:\n+          path: /proc\n+        name: proc\n+      - hostPath:\n+          path: /sys\n+        name: sys\n+      - hostPath:\n+          path: /\n+        name: root"}, {"sha": "f806268b5e5461c0fdfb9c82c7c08bb933ede8e8", "filename": "TEST_ARTIFACTS/k8s.doc.network.yaml", "status": "added", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fk8s.doc.network.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fk8s.doc.network.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fk8s.doc.network.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,34 @@\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: test-network-policy\n+  namespace: default\n+spec:\n+  podSelector:\n+    matchLabels:\n+      role: db\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  - from:\n+    - ipBlock:\n+        cidr: 172.17.0.0/16\n+        except:\n+        - 172.17.1.0/24\n+    - namespaceSelector:\n+        matchLabels:\n+          project: myproject\n+    - podSelector:\n+        matchLabels:\n+          role: frontend\n+    ports:\n+    - protocol: TCP\n+      port: 6379\n+  egress:\n+  - to:\n+    - ipBlock:\n+        cidr: 10.0.0.0/24\n+    ports:\n+    - protocol: TCP\n+      port: 5978\n\\ No newline at end of file"}, {"sha": "5dc2aaf64d81370935e514bd73d5819eec3e9315", "filename": "TEST_ARTIFACTS/keycloak.values.yaml", "status": "added", "additions": 317, "deletions": 0, "changes": 317, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fkeycloak.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fkeycloak.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fkeycloak.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,317 @@\n+init:\n+  image:\n+    repository: alpine\n+    tag: 3.8\n+    pullPolicy: IfNotPresent\n+  resources: {}\n+    # limits:\n+    #   cpu: \"10m\"\n+    #   memory: \"32Mi\"\n+    # requests:\n+    #   cpu: \"10m\"\n+    #   memory: \"32Mi\"\n+\n+clusterDomain: cluster.local\n+\n+keycloak:\n+  replicas: 2\n+\n+  image:\n+    repository: jboss/keycloak\n+    tag: 5.0.0\n+    pullPolicy: IfNotPresent\n+\n+    ## Optionally specify an array of imagePullSecrets.\n+    ## Secrets must be manually created in the namespace.\n+    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n+    ##\n+    pullSecrets: []\n+    # - myRegistrKeySecretName\n+\n+  hostAliases: []\n+  #  - ip: \"1.2.3.4\"\n+  #    hostnames:\n+  #      - \"my.host.com\"\n+\n+  enableServiceLinks: false\n+\n+  restartPolicy: Always\n+\n+  serviceAccount:\n+    # Specifies whether a service account should be created\n+    create: false\n+    # The name of the service account to use.\n+    # If not set and create is true, a name is generated using the fullname template\n+    name:\n+\n+  securityContext:\n+    fsGroup: 1000\n+\n+  containerSecurityContext:\n+    runAsUser: 1000\n+    runAsNonRoot: true\n+\n+  ## The path keycloak will be served from. To serve keycloak from the root path, use two quotes (e.g. \"\").\n+  basepath: auth\n+\n+  ## Additional init containers, e. g. for providing custom themes\n+  extraInitContainers: |\n+\n+  ## Additional sidecar containers, e. g. for a database proxy, such as Google's cloudsql-proxy\n+  extraContainers: |\n+\n+  ## Custom script that is run before Keycloak is started.\n+  preStartScript:\n+\n+  ## lifecycleHooks defines the container lifecycle hooks\n+  lifecycleHooks: |\n+    # postStart:\n+    #   exec:\n+    #     command: [\"/bin/sh\", \"-c\", \"ls\"]\n+\n+  ## Additional arguments to start command e.g. -Dkeycloak.import= to load a realm\n+  extraArgs: \"\"\n+\n+  ## Username for the initial Keycloak admin user\n+  username: keycloak\n+\n+  ## Password for the initial Keycloak admin user. Applicable only if existingSecret is not set.\n+  ## If not set, a random 10 characters password will be used\n+  password: \"\"\n+\n+  # Specifies an existing secret to be used for the admin password\n+  #existingSecret: \"\"\n+\n+  # The key in the existing secret that stores the password\n+  #existingSecretKey: password\n+\n+  ## Allows the specification of additional environment variables for Keycloak\n+  extraEnv: |\n+    - name: PROXY_ADDRESS_FORWARDING\n+      value: \"true\"\n+    # - name: KEYCLOAK_LOGLEVEL\n+    #   value: DEBUG\n+    # - name: WILDFLY_LOGLEVEL\n+    #   value: DEBUG\n+    # - name: CACHE_OWNERS\n+    #   value: \"2\"\n+    # - name: DB_QUERY_TIMEOUT\n+    #   value: \"60\"\n+    # - name: DB_VALIDATE_ON_MATCH\n+    #   value: true\n+    # - name: DB_USE_CAST_FAIL\n+    #   value: false\n+\n+  affinity: |\n+    podAntiAffinity:\n+      requiredDuringSchedulingIgnoredDuringExecution:\n+        - labelSelector:\n+            matchLabels:\n+              app:  {{ template \"keycloak.name\" . }}\n+              release: \"{{ .Release.Name }}\"\n+            matchExpressions:\n+              - key: role\n+                operator: NotIn\n+                values:\n+                  - test\n+          topologyKey: kubernetes.io/hostname\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+        - weight: 100\n+          podAffinityTerm:\n+            labelSelector:\n+              matchLabels:\n+                app:  {{ template \"keycloak.name\" . }}\n+                release: \"{{ .Release.Name }}\"\n+              matchExpressions:\n+                - key: role\n+                  operator: NotIn\n+                  values:\n+                    - test\n+            topologyKey: failure-domain.beta.kubernetes.io/zone\n+\n+  nodeSelector: {}\n+  priorityClassName: \"\"\n+  tolerations: []\n+\n+  ## Additional pod labels\n+  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n+  podLabels: {}\n+\n+  ## Extra Annotations to be added to pod\n+  podAnnotations: {}\n+\n+  livenessProbe:\n+    initialDelaySeconds: 120\n+    timeoutSeconds: 5\n+  readinessProbe:\n+    initialDelaySeconds: 30\n+    timeoutSeconds: 1\n+\n+  resources: {}\n+    # limits:\n+    #   cpu: \"100m\"\n+    #   memory: \"1024Mi\"\n+    # requests:\n+    #   cpu: \"100m\"\n+    #   memory: \"1024Mi\"\n+\n+  ## WildFly CLI configurations. They all end up in the file 'keycloak.cli' configured in the configmap which is\n+  ## executed on server startup.\n+  cli:\n+    nodeIdentifier: |\n+      {{ .Files.Get \"scripts/node-identifier.cli\" }}\n+\n+    logging: |\n+      {{ .Files.Get \"scripts/logging.cli\" }}\n+\n+    reverseProxy: |\n+      {{ .Files.Get \"scripts/reverse-proxy.cli\" }}\n+\n+    ha: |\n+      {{ .Files.Get \"scripts/ha.cli\" }}\n+\n+    datasource: |\n+      {{ .Files.Get \"scripts/datasource.cli\" }}\n+\n+    # Custom CLI script\n+    custom: |\n+\n+  ## Add additional volumes and mounts, e. g. for custom themes\n+  extraVolumes: |\n+  extraVolumeMounts: |\n+\n+  ## Add additional ports, eg. for custom admin console\n+  extraPorts: |\n+\n+  podDisruptionBudget: {}\n+    # maxUnavailable: 1\n+    # minAvailable: 1\n+\n+  service:\n+    annotations: {}\n+    # service.beta.kubernetes.io/aws-load-balancer-internal: \"0.0.0.0/0\"\n+\n+    labels: {}\n+    # key: value\n+\n+    ## ServiceType\n+    ## ref: https://kubernetes.io/docs/user-guide/services/#publishing-services---service-types\n+    type: ClusterIP\n+\n+    ## Optional static port assignment for service type NodePort.\n+    # nodePort: 30000\n+\n+    port: 80\n+\n+    # Optional: jGroups port for high availability clustering\n+    jgroupsPort: 7600\n+\n+  ## Ingress configuration.\n+  ## ref: https://kubernetes.io/docs/user-guide/ingress/\n+  ingress:\n+    enabled: true\n+    path: /\n+\n+    annotations: {}\n+      # kubernetes.io/ingress.class: nginx\n+      # kubernetes.io/tls-acme: \"true\"\n+      # ingress.kubernetes.io/affinity: cookie\n+\n+    labels: {}\n+    # key: value\n+\n+    ## List of hosts for the ingress\n+    hosts:\n+      - auth.corp.justin-tech.com\n+      - auth.justin-tech.com\n+\n+    ## TLS configuration\n+    tls: []\n+    # - hosts:\n+    #     - keycloak.example.com\n+    #   secretName: tls-keycloak\n+\n+  ## OpenShift route configuration.\n+  ## ref: https://docs.openshift.com/container-platform/3.11/architecture/networking/routes.html\n+  route:\n+    enabled: false\n+    path: /\n+\n+    annotations: {}\n+      # kubernetes.io/tls-acme: \"true\"\n+      # haproxy.router.openshift.io/disable_cookies: \"true\"\n+      # haproxy.router.openshift.io/balance: roundrobin\n+\n+    labels: {}\n+      # key: value\n+\n+    # Host name for the route\n+    host:\n+\n+    # TLS configuration\n+    tls:\n+      enabled: true\n+      insecureEdgeTerminationPolicy: Redirect\n+      termination: edge\n+\n+  ## Persistence configuration\n+  persistence:\n+    # If true, the Postgres chart is deployed\n+    deployPostgres: true\n+\n+    # The database vendor. Can be either \"postgres\", \"mysql\", \"mariadb\", or \"h2\"\n+    dbVendor: postgres\n+\n+    ## The following values only apply if \"deployPostgres\" is set to \"false\"\n+\n+    # Specifies an existing secret to be used for the database password\n+    #existingSecret: \"\"\n+\n+    # The key in the existing secret that stores the password\n+    #existingSecretKey: password\n+\n+    #dbName: keycloak\n+    #dbHost: mykeycloak\n+    #dbPort: 5432\n+    #dbUser: keycloak\n+\n+    # Only used if no existing secret is specified. In this case a new secret is created\n+    dbPassword: \"\"\n+\n+postgresql:\n+  ### PostgreSQL User to create.\n+  ##\n+  postgresUser: keycloak\n+\n+  ## PostgreSQL Password for the new user.\n+  ## If not set, a random 10 characters password will be used.\n+  ##\n+  postgresPassword: \"\"\n+\n+  ## PostgreSQL Database to create.\n+  ##\n+  postgresDatabase: keycloak\n+\n+  ## Persistent Volume Storage configuration.\n+  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes\n+  ##\n+  persistence:\n+    ## Enable PostgreSQL persistence using Persistent Volume Claims.\n+    ##\n+    enabled: true\n+    storageClass: \"nfs-client\"\n+    accessMode: ReadWriteOnce\n+    size: 10Gi\n+\n+test:\n+  enabled: true\n+  image:\n+    repository: unguiculus/docker-python3-phantomjs-selenium\n+    tag: v1\n+    pullPolicy: IfNotPresent\n+  securityContext:\n+    fsGroup: 1000\n+  containerSecurityContext:\n+    runAsUser: 1000\n+    runAsNonRoot: true\n+"}, {"sha": "1ff040bc64980b21e764272a9cbb9d8fa5c1245b", "filename": "TEST_ARTIFACTS/kubecf.values.yaml", "status": "added", "additions": 340, "deletions": 0, "changes": 340, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fkubecf.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fkubecf.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fkubecf.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,340 @@\n+system_domain: ~\n+\n+# Set or override job properties. The first level of the map is the instance group name. The second\n+# level of the map is the job name. E.g.:\n+#  properties:\n+#    adapter:\n+#      adapter:\n+#        scalablesyslog:\n+#          adapter:\n+#            logs:\n+#              addr: kubecf-log-api:8082\n+#\n+#  Eirini Persistence Broker setup example:\n+#\n+# properties:\n+#   eirini:\n+#     eirini-persi-broker:\n+#       eirini-persi-broker:\n+#         service_plans:\n+#           - id: \"default\"\n+#             name: \"default\"\n+#             description: \"Persistence storage service broker for applications.\"\n+#             free: true\n+#             kube_storage_class: \"default\"\n+#             default_size: \"1Gi\"\n+properties: {}\n+\n+kube:\n+  # The storage class to be used for the instance groups that need it (e.g. bits, database and\n+  # singleton-blobstore). If it's not set, the default storage class will be used.\n+  storage_class: ~\n+  # The service_cluster_ip_range and pod_cluster_ip_range are used by the internal security group\n+  # definition to allow apps to communicate with internal service brokers (e.g. credhub).\n+  # service_cluster_ip_range can be fetched with the following command, assuming that the API\n+  # server started with the `--service-cluster-ip-range` flag:\n+  # kubectl cluster-info dump --output yaml \\\n+  #   | awk 'match($0, /service-cluster-ip-range=(.*)/, range) { print range[1] }'\n+  # The default value for `--service-cluster-ip-range` is 10.0.0.0/24.\n+  service_cluster_ip_range: ~\n+  # pod_cluster_ip_range can be fetched with the following command, assuming that the controller\n+  # manager started with the `--cluster-cidr` flag:\n+  # kubectl cluster-info dump --output yaml \\\n+  #   | awk 'match($0, /cluster-cidr=(.*)/, range) { print range[1] }'\n+  # There is no default value for `--cluster-cidr`.\n+  pod_cluster_ip_range: ~\n+  # The psp key contains the configuration related to Pod Security Policies. By default, a PSP will\n+  # be generated with the necessary permissions for running KubeCF. To pass an existing PSP and\n+  # prevent KubeCF from creating a new one, set the kube.psp.default with the PSP name.\n+  psp:\n+    default: ~\n+\n+releases:\n+  # The defaults for all releases, where we do not otherwise override them.\n+  defaults:\n+    url: docker.io/cfcontainerization\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.8-7.0.0_374.gb8e8e6af\n+  app-autoscaler:\n+    version: 3.0.0\n+  bits-service:\n+    version: 2.28.0\n+  # TODO: brains-tests must switch to SLE15 stemcell once the compilation failures are resolved.\n+  # check: https://github.com/SUSE/kubecf/issues/270\n+  brain-tests:\n+    version: v0.0.6\n+    stemcell:\n+      os: opensuse-42.3\n+      version: 36.g03b4653-30.80-7.0.0_372.ge3509601\n+  cf-acceptance-tests:\n+    version: 0.0.9\n+  # TODO: cf-mysql must switch to SLE15 stemcell once the compilation failures are resolved.\n+  cf-mysql:\n+    version: 36.19.0\n+    stemcell:\n+      os: opensuse-42.3\n+      version: 36.g03b4653-30.80-7.0.0_360.g0ec8d681\n+  eirini:\n+    version: 0.0.25\n+  loggregator:\n+    version: \"105.6\"\n+  postgres:\n+    version: \"39\"\n+  sle15:\n+    version: \"10.93\"\n+  sync-integration-tests:\n+    version: v0.0.3\n+  staticfile-buildpack:\n+    file: staticfile-buildpack/packages/staticfile-buildpack-cflinuxfs3/staticfile_buildpack-cflinuxfs3-v1.5.3.zip\n+  java-buildpack:\n+    file: java-buildpack/packages/java-buildpack-cflinuxfs3/java-buildpack-cflinuxfs3-v4.26.zip\n+  ruby-buildpack:\n+    file: ruby-buildpack/packages/ruby-buildpack-cflinuxfs3/ruby_buildpack-cflinuxfs3-v1.8.8.zip\n+  dotnet-core-buildpack:\n+    file: dotnet-core-buildpack/packages/dotnet-core-buildpack-cflinuxfs3/dotnet-core_buildpack-cflinuxfs3-v2.3.4.zip\n+  nodejs-buildpack:\n+    file: nodejs-buildpack/packages/nodejs-buildpack-cflinuxfs3/nodejs_buildpack-cflinuxfs3-v1.7.9.zip\n+  go-buildpack:\n+    file: go-buildpack/packages/go-buildpack-cflinuxfs3/go_buildpack-cflinuxfs3-v1.9.5.zip\n+  python-buildpack:\n+    file: python-buildpack/packages/python-buildpack-cflinuxfs3/python_buildpack-cflinuxfs3-v1.7.6.zip\n+  php-buildpack:\n+    file: php-buildpack/packages/php-buildpack-cflinuxfs3/php_buildpack-cflinuxfs3-v4.4.6.zip\n+  nginx-buildpack:\n+    file: nginx-buildpack/packages/nginx-buildpack-cflinuxfs3/nginx_buildpack-cflinuxfs3-v1.1.4.zip\n+  r-buildpack:\n+    file: r-buildpack/packages/r-buildpack-cflinuxfs3/r_buildpack-cflinuxfs3-v1.1.1.zip\n+  binary-buildpack:\n+    file: binary-buildpack/packages/binary-buildpack-cflinuxfs3/binary_buildpack-cflinuxfs3-v1.0.36.zip\n+  suse-staticfile-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.5.2.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-staticfile-buildpack/packages/staticfile-buildpack-sle15/staticfile-buildpack-sle15-v1.5.2.1-1.1-2c315eb8.zip\n+  suse-java-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"4.27.0.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-java-buildpack/packages/java-buildpack-sle15/java-buildpack-sle15-v4.27.0.1-7975f85e.zip\n+  suse-ruby-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.8.3.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-ruby-buildpack/packages/ruby-buildpack-sle15/ruby-buildpack-sle15-v1.8.3.1-1.1-a08b9b7a.zip\n+  suse-dotnet-core-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"2.3.0.2\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-dotnet-core-buildpack/packages/dotnet-core-buildpack-sle15/dotnet-core-buildpack-sle15-v2.3.0.1-1.1-d1344b0e.zip\n+  suse-nodejs-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.7.7.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-nodejs-buildpack/packages/nodejs-buildpack-sle15/nodejs-buildpack-sle15-v1.7.7.1-1.1-856d35fb.zip\n+  suse-go-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.9.4.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.7-7.0.0_374.gb8e8e6af\n+    file: suse-go-buildpack/packages/go-buildpack-sle15/go-buildpack-sle15-v1.9.4.1-1.1-436eaf5d.zip\n+  suse-python-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.7.4.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-python-buildpack/packages/python-buildpack-sle15/python-buildpack-sle15-v1.7.4.1-1.1-79c8afbe.zip\n+  suse-php-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"4.4.2.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.7-7.0.0_374.gb8e8e6af\n+    file: suse-php-buildpack/packages/php-buildpack-sle15/php-buildpack-sle15-v4.4.2.1-1.1-905fbac1.zip\n+  suse-nginx-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.1.3.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-nginx-buildpack/packages/nginx-buildpack-sle15/nginx-buildpack-sle15-v1.1.3.1-1.1-bdd184c6.zip\n+  suse-binary-buildpack:\n+    url: registry.suse.com/cap-staging\n+    version: \"1.0.36.1\"\n+    stemcell:\n+      os: SLE_15_SP1\n+      version: 23.1-7.0.0_374.gb8e8e6af\n+    file: suse-binary-buildpack/packages/binary-buildpack-sle15/binary-buildpack-sle15-v1.0.36.1-1.1-37ec2cbf.zip\n+\n+multi_az: false\n+high_availability: false\n+\n+# Sizing takes precedence over the high_availability property. I.e. setting the instance count\n+# for an instance group greater than 1 will make it highly available.\n+sizing:\n+  adapter:\n+    instances: ~\n+  api:\n+    instances: ~\n+  asactors:\n+    instances: ~\n+  asapi:\n+    instances: ~\n+  asmetrics:\n+    instances: ~\n+  asnozzle:\n+    instances: ~\n+  auctioneer:\n+    instances: ~\n+  bits:\n+    instances: ~\n+  cc_worker:\n+    instances: ~\n+  credhub:\n+    instances: ~\n+  diego_api:\n+    instances: ~\n+  diego_cell:\n+    instances: ~\n+  eirini:\n+    instances: ~\n+  log_api:\n+    instances: ~\n+  nats:\n+    instances: ~\n+  router:\n+    instances: ~\n+  routing_api:\n+    instances: ~\n+  scheduler:\n+    instances: ~\n+  uaa:\n+    instances: ~\n+  tcp_router:\n+    instances: ~\n+\n+# Service is only valid to set a external endpoints for the instance groups if\n+# features.ingress.enabled is false.\n+services:\n+  router:\n+    annotations: ~\n+    type: LoadBalancer\n+    externalIPs: []\n+    clusterIP: ~\n+  ssh-proxy:\n+    annotations: ~\n+    type: LoadBalancer\n+    externalIPs: []\n+    clusterIP: ~\n+  tcp-router:\n+    annotations: ~\n+    type: LoadBalancer\n+    externalIPs: []\n+    clusterIP: ~\n+    port_range:\n+      start: 20000\n+      end: 20008\n+\n+features:\n+  eirini:\n+    enabled: false\n+    registry:\n+      service:\n+        nodePort: 32123\n+  ingress:\n+    enabled: false\n+    tls:\n+      crt: ~\n+      key: ~\n+    annotations: {}\n+    labels: {}\n+  suse_buildpacks:\n+    enabled: true\n+  autoscaler:\n+    enabled: false\n+\n+  # external_database disables the embedded database and allows using an external, already seeded,\n+  # database.\n+  # The database type can be either 'mysql' or 'postgres'.\n+  external_database:\n+    enabled: false\n+    require_ssl: false\n+    ca_cert: ~\n+    type: ~\n+    host: ~\n+    port: ~\n+    databases:\n+      uaa:\n+        name: uaa\n+        password: ~\n+        username: ~\n+      cc:\n+        name: cloud_controller\n+        password: ~\n+        username: ~\n+      bbs:\n+        name: diego\n+        password: ~\n+        username: ~\n+      routing_api:\n+        name: routing-api\n+        password: ~\n+        username: ~\n+      policy_server:\n+        name: network_policy\n+        password: ~\n+        username: ~\n+      silk_controller:\n+        name: network_connectivity\n+        password: ~\n+        username: ~\n+      locket:\n+        name: locket\n+        password: ~\n+        username: ~\n+      credhub:\n+        name: credhub\n+        password: ~\n+        username: ~\n+\n+# Enable or disable instance groups for the different test suites.\n+# Only smoke tests should be run in production environments.\n+testing:\n+  brain_tests:\n+    enabled: false\n+  cf_acceptance_tests:\n+    enabled: false\n+  smoke_tests:\n+    enabled: true\n+  sync_integration_tests:\n+    enabled: false\n+\n+ccdb:\n+  encryption:\n+    rotation:\n+      # Key labels must be <= 240 characters long. Each label will be prepended with the\n+      # \"ccdb_key_label_\" value.\n+      key_labels:\n+      - encryption_key_0\n+      current_key_label: encryption_key_0\n+\n+operations:\n+  # A list of configmap names that should be applied to the BOSH manifest.\n+  custom: []\n+\n+k8s-host-url: \"\"\n+k8s-service-token: \"\"\n+k8s-service-username: \"\"\n+k8s-node-ca: \"\""}, {"sha": "28aa38acddfc6a7299253d442ba446d341fd1eed", "filename": "TEST_ARTIFACTS/list.k8s.yml", "status": "added", "additions": 291, "deletions": 0, "changes": 291, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Flist.k8s.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Flist.k8s.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Flist.k8s.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,291 @@\n+apiVersion: v1\n+kind: List\n+items:\n+# rolebindings\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRoleBinding\n+   metadata:\n+     name: system:cloud-node-controller\n+   roleRef:\n+     apiGroup: rbac.authorization.k8s.io\n+     kind: ClusterRole\n+     name: system:cloud-node-controller\n+   subjects:\n+   - kind: ServiceAccount\n+     name: cloud-node-controller\n+     namespace: kube-system\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRoleBinding\n+   metadata:\n+     name: system:pvl-controller\n+   roleRef:\n+     apiGroup: rbac.authorization.k8s.io\n+     kind: ClusterRole\n+     name: system:pvl-controller\n+   subjects:\n+   - kind: ServiceAccount\n+     name: pvl-controller\n+     namespace: kube-system\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRoleBinding\n+   metadata:\n+     name: system:cloud-controller-manager\n+   roleRef:\n+     apiGroup: rbac.authorization.k8s.io\n+     kind: ClusterRole\n+     name: system:cloud-controller-manager\n+   subjects:\n+   - kind: ServiceAccount\n+     name: cloud-controller-manager\n+     namespace: kube-system\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRoleBinding\n+   metadata:\n+     name: system:service-controller\n+   roleRef:\n+     apiGroup: rbac.authorization.k8s.io\n+     kind: ClusterRole\n+     name: system:service-controller\n+   subjects:\n+   - kind: ServiceAccount\n+     name: service-controller\n+     namespace: kube-system\n+# roles\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRole\n+   metadata:\n+     name: system:cloud-controller-manager\n+   rules:\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - events\n+     verbs:\n+     - create\n+     - patch\n+     - update\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - nodes\n+     verbs:\n+     - '*'\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - nodes/status\n+     verbs:\n+     - patch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - services/status\n+     verbs:\n+     - patch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - services\n+     verbs:\n+     - list\n+     - patch\n+     - update\n+     - watch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - serviceaccounts\n+     verbs:\n+     - create\n+     - get\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - persistentvolumes\n+     verbs:\n+     - '*'\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - endpoints\n+     verbs:\n+     - create\n+     - get\n+     - list\n+     - watch\n+     - update\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - configmaps\n+     verbs:\n+     - get\n+     - list\n+     - watch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - secrets\n+     verbs:\n+     - list\n+     - get\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRole\n+   metadata:\n+     name: system:cloud-node-controller\n+   rules:\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - nodes\n+     verbs:\n+     - delete\n+     - get\n+     - patch\n+     - update\n+     - list\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - nodes/status\n+     verbs:\n+     - patch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - events\n+     verbs:\n+     - create\n+     - patch\n+     - update\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRole\n+   metadata:\n+     name: system:pvl-controller\n+   rules:\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - persistentvolumes\n+     verbs:\n+     - '*'\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - events\n+     verbs:\n+     - create\n+     - patch\n+     - update\n+ - apiVersion: rbac.authorization.k8s.io/v1\n+   kind: ClusterRole\n+   metadata:\n+     name: system:service-controller\n+   rules:\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - services\n+     verbs:\n+     - delete\n+     - get\n+     - patch\n+     - update\n+     - list\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - services/status\n+     verbs:\n+     - patch\n+   - apiGroups:\n+     - \"\"\n+     resources:\n+     - events\n+     verbs:\n+     - create\n+     - patch\n+     - update\n+# serviceaccount\n+ - apiVersion: v1\n+   kind: ServiceAccount\n+   metadata:\n+     name: cloud-controller-manager\n+     namespace: kube-system\n+# daemonset\n+ - apiVersion: apps/v1\n+   kind: DaemonSet\n+   metadata:\n+     name: openstack-cloud-controller-manager\n+     namespace: kube-system\n+     labels:\n+       k8s-app: openstack-cloud-controller-manager\n+   spec:\n+     selector:\n+       matchLabels:\n+         k8s-app: openstack-cloud-controller-manager\n+     updateStrategy:\n+       type: RollingUpdate\n+     template:\n+       metadata:\n+         labels:\n+           k8s-app: openstack-cloud-controller-manager\n+       spec:\n+         nodeSelector:\n+           node-role.kubernetes.io/master: \"\"\n+         securityContext:\n+           runAsUser: 1001\n+         tolerations:\n+         - key: node.cloudprovider.kubernetes.io/uninitialized\n+           value: \"true\"\n+           effect: NoSchedule\n+         - key: node-role.kubernetes.io/master\n+           effect: NoSchedule\n+         serviceAccountName: cloud-controller-manager\n+         containers:\n+           - name: openstack-cloud-controller-manager\n+             image: docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1.14.0\n+             #command: [\"/bin/sh\", \"-ec\", \"sleep 1000\"]\n+             args:\n+               - /bin/openstack-cloud-controller-manager\n+               - --v=4\n+               - --cloud-config=$(CLOUD_CONFIG)\n+               - --cloud-provider=openstack\n+               - --external-cloud-volume-plugin\n+               - --use-service-account-credentials=true\n+               - --address=127.0.0.1\n+             volumeMounts:\n+               - mountPath: /etc/kubernetes/pki\n+                 name: k8s-certs\n+                 readOnly: true\n+               - mountPath: /etc/ssl/certs\n+                 name: ca-certs\n+                 readOnly: true\n+               - mountPath: /etc/config\n+                 name: cloud-config-volume\n+                 readOnly: true\n+               - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec\n+                 name: flexvolume-dir\n+             resources:\n+               requests:\n+                 cpu: 200m\n+             env:\n+               - name: CLOUD_CONFIG\n+                 value: /etc/config/cloud-config\n+         hostNetwork: true\n+         volumes:\n+         - hostPath:\n+             path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec\n+             type: DirectoryOrCreate\n+           name: flexvolume-dir\n+         - hostPath:\n+             path: /etc/kubernetes/pki\n+             type: DirectoryOrCreate\n+           name: k8s-certs\n+         - hostPath:\n+             path: /etc/ssl/certs\n+             type: DirectoryOrCreate\n+           name: ca-certs\n+         - name: cloud-config-volume\n+           secret:\n+             secretName: cloud-config"}, {"sha": "2a15b688b35c2d3f960610a97557490c96258d37", "filename": "TEST_ARTIFACTS/minecraft.values.yaml", "status": "added", "additions": 150, "deletions": 0, "changes": 150, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fminecraft.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fminecraft.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fminecraft.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,150 @@\n+# ref: https://hub.docker.com/r/itzg/minecraft-server/\n+image: itzg/minecraft-server\n+imageTag: latest\n+\n+## Configure resource requests and limits\n+## ref: http://kubernetes.io/docs/user-guide/compute-resources/\n+##\n+resources:\n+  requests:\n+    memory: 2048Mi\n+    cpu: 2000m\n+  limits:\n+    memory: 5120Mi\n+    cpu: 4000m\n+\n+securityContext:\n+  # Security context settings\n+  runAsUser: 1000\n+  fsGroup: 1000\n+# Most of these map to environment variables. See Minecraft for details:\n+# https://hub.docker.com/r/itzg/minecraft-server/\n+livenessProbe:\n+  command:\n+    - mcstatus\n+    - localhost:25565\n+    - status\n+  initialDelaySeconds: 60\n+  periodSeconds: 5\n+readinessProbe:\n+  command:\n+    - mcstatus\n+    - localhost:25565\n+    - status\n+  initialDelaySeconds: 60\n+  periodSeconds: 5\n+minecraftServer:\n+  # This must be overridden, since we can't accept this for the user.\n+  eula: \"TRUE\"\n+  # One of: LATEST, SNAPSHOT, or a specific version (ie: \"1.7.9\").\n+  version: \"1.14.3\"\n+  # This can be one of empty string, \"FORGE\", \"SPIGOT\", \"BUKKIT\", \"PAPER\", \"FTB\", \"SPONGEVANILLA\"; empty string will produce a vanilla server\n+  type: \"VANILLA\" #\"FTB\"\n+  # If type is set to FORGE, this sets the version; this is ignored if forgeInstallerUrl is set\n+  forgeVersion:\n+  # If type is set to SPONGEVANILLA, this sets the version\n+  spongeVersion:\n+  # If type is set to FORGE, this sets the URL to download the Forge installer\n+  forgeInstallerUrl:\n+  # If type is set to BUKKIT, this sets the URL to download the Bukkit package\n+  bukkitDownloadUrl:\n+  # If type is set to SPIGOT, this sets the URL to download the Spigot package\n+  spigotDownloadUrl:\n+  # If type is set to PAPER, this sets the URL to download the PaperSpigot package\n+  paperDownloadUrl:\n+  # If type is set to FTB, this sets the server mod to run\n+  ftbServerMod: #https://www.feed-the-beast.com/projects/ftb-revelation/files/2712061\n+  # Set to true if running Feed The Beast and get an error like \"unable to launch forgemodloader\"\n+  ftbLegacyJavaFixer: false\n+  # One of: peaceful, easy, normal, and hard\n+  difficulty: easy\n+  # A comma-separated list of player names to whitelist.\n+  whitelist:\n+  # A comma-separated list of player names who should be admins.\n+  ops: changeme\n+  # A server icon URL for server listings. Auto-scaled and transcoded.\n+  icon:\n+  # Max connected players.\n+  maxPlayers: 20\n+  # This sets the maximum possible size in blocks, expressed as a radius, that the world border can obtain.\n+  maxWorldSize: 10000\n+  # Allows players to travel to the Nether.\n+  allowNether: true\n+  # Allows server to announce when a player gets an achievement.\n+  announcePlayerAchievements: true\n+  # Enables command blocks.\n+  enableCommandBlock: true\n+  # If true, players will always join in the default gameMode even if they were previously set to something else.\n+  forcegameMode: false\n+  # Defines whether structures (such as villages) will be generated.\n+  generateStructures: true\n+  # If set to true, players will be set to spectator mode if they die.\n+  hardcore: false\n+  # The maximum height in which building is allowed.\n+  maxBuildHeight: 256\n+  # The maximum number of milliseconds a single tick may take before the server watchdog stops the server with the message. -1 disables this entirely.\n+  maxTickTime: 60000\n+  # Determines if animals will be able to spawn.\n+  spawnAnimals: true\n+  # Determines if monsters will be spawned.\n+  spawnMonsters: true\n+  # Determines if villagers will be spawned.\n+  spawnNPCs: true\n+  # Max view distance (in chunks).\n+  viewDistance: 10\n+  # Define this if you want a specific map generation seed.\n+  levelSeed:\n+  # One of: creative, survival, adventure, spectator\n+  gameMode: survival\n+  # Message of the Day\n+  motd: \"Welcome to Minecraft hosted by Justin-Tech!\"\n+  # If true, enable player-vs-player damage.\n+  pvp: false\n+  # One of: DEFAULT, FLAT, LARGEBIOMES, AMPLIFIED, CUSTOMIZED\n+  levelType: DEFAULT\n+  # When levelType == FLAT or CUSTOMIZED, this can be used to further customize map generation.\n+  # ref: https://hub.docker.com/r/itzg/minecraft-server/\n+  generatorSettings:\n+  worldSaveName: world\n+  # If set, this URL will be downloaded at startup and used as a starting point\n+  downloadWorldUrl:\n+  # force re-download of server file\n+  forceReDownload: false\n+  # If set, the modpack at this URL will be downloaded at startup\n+  downloadModpackUrl:\n+  # If true, old versions of downloaded mods will be replaced with new ones from downloadModpackUrl\n+  removeOldMods: false\n+  # Check accounts against Minecraft account service.\n+  onlineMode: true\n+  # If you adjust this, you may need to adjust resources.requests above to match.\n+  jvmOpts: \"-Xmx4096M -Xms2048M\"\n+  serviceType: LoadBalancer\n+  rcon:\n+    # If you enable this, make SURE to change your password below.\n+    enabled: false\n+    port: 25575\n+    password: \"CHANGEME!\"\n+    serviceType: LoadBalancer\n+\n+  query:\n+    # If you enable this, your server will be \"published\" to Gamespy\n+    enabled: false\n+    port: 25565\n+\n+## Additional minecraft container environment variables\n+##\n+extraEnv: {}\n+\n+persistence:\n+  ## minecraft data Persistent Volume Storage Class\n+  ## If defined, storageClassName: <storageClass>\n+  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n+  ## If undefined (the default) or set to null, no storageClassName spec is\n+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n+  ##   GKE, AWS & OpenStack)\n+  ##\n+  storageClass: \"nfs-client\"\n+  dataDir:\n+    # Set this to false if you don't care to persist state between restarts.\n+    enabled: true\n+    Size: 1Gi"}, {"sha": "a4b6320022432569005cfc6402ce939a741ecdfb", "filename": "TEST_ARTIFACTS/multi.doc.yaml", "status": "added", "additions": 119, "deletions": 0, "changes": 119, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fmulti.doc.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fmulti.doc.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fmulti.doc.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,119 @@\n+apiVersion: v1\n+kind: Service\n+metadata:\n+ name: nfs-server\n+ labels:\n+   app: nfs-server\n+spec:\n+ ports:\n+ - port: 111\n+   protocol: TCP\n+   name: nfs-111-tcp\n+ - port: 111\n+   protocol: UDP\n+   name: nfs-111-udp\n+ - port: 2049\n+   protocol: TCP\n+   name: nfs-2049-tcp\n+ #sessionAffinity: ClientIP\n+ #clusterIP: None\n+ #type: NodePort # Or LoadBalancer in production w/ proper security\n+#  type: LoadBalancer\n+ selector:\n+   app: nfs-server\n+\n+---\n+\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: nfs-server\n+spec:\n+  nodeSelector:\n+    nfs-server: \"true\"\n+  restartPolicy: Always\n+  initContainers:\n+  - name: wait1\n+    #imagePullPolicy: Always\n+    imagePullPolicy: IfNotPresent\n+    image: call518/oaas-init-container:1.0\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    volumeMounts:\n+    - name: init-container-scripts\n+      mountPath: /init-container-scripts\n+    command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-etcd.sh\"]\n+  containers:\n+  - name: nfs-server\n+    image: call518/oaas-nfs-server:1.0\n+    securityContext:\n+      privileged: true\n+    ports:\n+    - containerPort: 111\n+      protocol: TCP\n+    - containerPort: 111\n+      protocol: UDP\n+    - containerPort: 2049\n+      protocol: TCP\n+    volumeMounts:\n+    - name: pvc-nfs-server\n+      mountPath: /data\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    env:\n+    - name: MY_POD_IP\n+      valueFrom:\n+        fieldRef:\n+          fieldPath: status.podIP\n+    - name: SHARED_DIRECTORY\n+      value: /data\n+    - name: SYNC\n+      value: \"true\"\n+    - name: FSID\n+      value: \"true\"\n+    command:\n+      - \"bash\"\n+      - \"-c\"\n+      - |\n+        until [ \"$CHECK_ETCD_NFS_SERVER_IP\" == \"$MY_POD_IP\" ];\n+        do\n+          echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` Putting nfs-server to etcd....... waiting...\";\n+          curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XPUT -d value=\"$MY_POD_IP\";\n+          CHECK_ETCD_NFS_SERVER_IP=$(curl --connect-timeout 3 -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XGET | jq -r .node.value)\n+          sleep 5;\n+        done;\n+        echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ etcd for nfs-server is ready~~ (etcd's nfs-server IP: $CHECK_ETCD_NFS_SERVER_IP)\";\n+        rm -rf /data/*;\n+        mkdir -p /data/pv/galera-{0,1,2};\n+        mkdir -p /data/pv/mongodb-{0,1,2};\n+        mkdir -p /data/pv/rabbitmq-{0,1,2};\n+        mkdir -p /data/pv/glance-images;\n+        mkdir -p /data/pv/zookeeper-{0,1,2};\n+        mkdir -p /data/pv/cinder-volumes;\n+        mkdir -p /data/pv/cinder-backups;\n+        #mkdir -p /data/pv/cinder-lock_path;\n+        #mkdir -p /data/pv/nova-server-lock_path;\n+        #mkdir -p /data/pv/nova-compute-lock_path;\n+        mkdir -p /data/pv/nova-compute-images;\n+        mkdir -p /data/pv/nova-compute-instances;\n+        mkdir -p /data/pv/ceilometer-gnocchi;\n+        chmod 777 /data/pv/cinder-* /data/pv/zookeeper-*\n+        /usr/bin/nfsd.sh;\n+    lifecycle:\n+      preStop:\n+        exec:\n+          command:\n+          - /bin/sh\n+          - -c\n+          - >\n+            curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XDELETE;\n+  volumes:\n+  - name: init-container-scripts\n+    configMap:\n+      name: init-container-scripts\n+      defaultMode: 0755\n+  - name: pvc-nfs-server\n+    persistentVolumeClaim:\n+      claimName: pvc-nfs-server"}, {"sha": "8595f553b28bc6a48d54efd17e3bee55d248e386", "filename": "TEST_ARTIFACTS/nextcloud.values.yaml", "status": "added", "additions": 118, "deletions": 0, "changes": 118, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnextcloud.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnextcloud.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fnextcloud.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,118 @@\n+## Official nextcloud image version\n+## ref: https://hub.docker.com/r/library/nextcloud/tags/\n+##\n+image:\n+  repository: nextcloud\n+  tag: 15.0.2-apache\n+  pullPolicy: IfNotPresent\n+  # pullSecrets:\n+  #   - myRegistrKeySecretName\n+\n+nameOverride: \"\"\n+fullnameOverride: \"\"\n+\n+# Number of replicas to be deployed\n+replicaCount: 1\n+\n+## Allowing use of ingress controllers\n+## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/\n+##\n+ingress:\n+  enabled: true\n+  annotations: {}\n+\n+nextcloud:\n+  host: nextcloud.corp.justin-tech.com\n+  username: admin\n+  password: changeme\n+\n+\n+internalDatabase:\n+  enabled: true\n+  name: nextcloud\n+\n+\n+##\n+## External database configuration\n+##\n+externalDatabase:\n+  enabled: false\n+\n+  ## Database host\n+  host:\n+\n+  ## Database user\n+  user: nextcloud\n+\n+  ## Database password\n+  password:\n+\n+  ## Database name\n+  database: nextcloud\n+\n+##\n+## MariaDB chart configuration\n+##\n+mariadb:\n+  ## Whether to deploy a mariadb server to satisfy the applications database requirements. To use an external database set this to false and configure the externalDatabase parameters\n+  enabled: true\n+\n+  db:\n+    name: nextcloud\n+    user: nextcloud\n+    password: changeme\n+\n+  ## Enable persistence using Persistent Volume Claims\n+  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n+  ##\n+  persistence:\n+    enabled: true\n+    storageClass: \"nfs-client\"\n+    accessMode: ReadWriteOnce\n+    size: 8Gi\n+\n+service:\n+  type: ClusterIP\n+  port: 8080\n+  loadBalancerIP: nil\n+\n+\n+## Enable persistence using Persistent Volume Claims\n+## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n+##\n+persistence:\n+  enabled: true\n+  ## nextcloud data Persistent Volume Storage Class\n+  ## If defined, storageClassName: <storageClass>\n+  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n+  ## If undefined (the default) or set to null, no storageClassName spec is\n+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n+  ##   GKE, AWS & OpenStack)\n+  ##\n+  storageClass: \"nfs-client\"\n+\n+  ## A manually managed Persistent Volume and Claim\n+  ## Requires persistence.enabled: true\n+  ## If defined, PVC must be created manually before volume will be bound\n+  # existingClaim:\n+\n+  accessMode: ReadWriteOnce\n+  size: 8Gi\n+\n+resources: {}\n+  # We usually recommend not to specify default resources and to leave this as a conscious\n+  # choice for the user. This also increases chances charts run on environments with little\n+  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n+  # limits:\n+  #  cpu: 100m\n+  #  memory: 128Mi\n+  # requests:\n+  #  cpu: 100m\n+  #  memory: 128Mi\n+\n+nodeSelector: {}\n+\n+tolerations: []\n+\n+affinity: {}"}, {"sha": "138031bea60eaeb718bd4fda6f14817dd0cb3421", "filename": "TEST_ARTIFACTS/nginx.deployment.result.yaml", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnginx.deployment.result.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnginx.deployment.result.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fnginx.deployment.result.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,68 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    deployment.kubernetes.io/revision: \"1\"\n+    kubectl.kubernetes.io/last-applied-configuration: |\n+      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.16\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":8080}]}]}}}}\n+  creationTimestamp: \"2020-01-24T10:54:56Z\"\n+  generation: 1\n+  labels:\n+    app: nginx\n+  name: nginx-deployment\n+  namespace: default\n+  resourceVersion: \"96574\"\n+  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment\n+  uid: e1075fa3-6468-43d0-83c0-63fede0dae51\n+spec:\n+  progressDeadlineSeconds: 600\n+  replicas: 2\n+  revisionHistoryLimit: 10\n+  selector:\n+    matchLabels:\n+      app: nginx\n+  strategy:\n+    rollingUpdate:\n+      maxSurge: 25%\n+      maxUnavailable: 25%\n+    type: RollingUpdate\n+  template:\n+    metadata:\n+      creationTimestamp: null\n+      labels:\n+        app: nginx\n+    spec:\n+      containers:\n+      - image: nginx:1.16\n+        imagePullPolicy: IfNotPresent\n+        name: nginx\n+        ports:\n+        - containerPort: 8080\n+          protocol: TCP\n+        resources: {}\n+        terminationMessagePath: /dev/termination-log\n+        terminationMessagePolicy: File\n+      dnsPolicy: ClusterFirst\n+      restartPolicy: Always\n+      schedulerName: default-scheduler\n+      securityContext: {}\n+      terminationGracePeriodSeconds: 30\n+status:\n+  availableReplicas: 2\n+  conditions:\n+  - lastTransitionTime: \"2020-01-24T10:54:59Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: Deployment has minimum availability.\n+    reason: MinimumReplicasAvailable\n+    status: \"True\"\n+    type: Available\n+  - lastTransitionTime: \"2020-01-24T10:54:56Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: ReplicaSet \"nginx-deployment-7d64f4b574\" has successfully progressed.\n+    reason: NewReplicaSetAvailable\n+    status: \"True\"\n+    type: Progressing\n+  observedGeneration: 1\n+  readyReplicas: 2\n+  replicas: 2\n+  updatedReplicas: 2"}, {"sha": "8c768d039fa7ec2a874b74092d937ec59ea82081", "filename": "TEST_ARTIFACTS/nginx.present.yaml", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnginx.present.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fnginx.present.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fnginx.present.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,53 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: nginx\n+  labels:\n+    app: blog-demo\n+    serviceType: nginx\n+spec:\n+  revisionHistoryLimit: 3\n+  minReadySeconds: 20\n+  progressDeadlineSeconds: 60\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxSurge: 20%\n+      maxUnavailable: 0%\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      serviceType: nginx\n+  template:\n+    metadata:\n+      labels:\n+        serviceType: nginx\n+        app: blog-demo\n+    spec:\n+      containers:\n+      - name: nginx\n+        securityContext:\n+          runAsUser: 100\n+        readinessProbe:\n+          httpGet:\n+            path: /_/health\n+            port: 8000\n+          initialDelaySeconds: 5\n+          periodSeconds: 15\n+          timeoutSeconds: 10\n+        livenessProbe:\n+          httpGet:\n+            path: /_/health\n+            port: 8000\n+          initialDelaySeconds: 5\n+          periodSeconds: 15\n+          timeoutSeconds: 5\n+        image: registry.gitlab.com/obtao/blog-demo/nginx:latest\n+        imagePullPolicy: Always\n+        resources:\n+          limits:\n+            cpu: 50m\n+            memory: 64Mi\n+          requests:\n+            cpu: 50m\n+            memory: 64Mi"}, {"sha": "9d45a0ccc56247f7e66c97cd677110118d44cd2d", "filename": "TEST_ARTIFACTS/no.kind.yaml", "status": "added", "additions": 193, "deletions": 0, "changes": 193, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.kind.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.kind.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fno.kind.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,193 @@\n+apiVersion: v1\n+entries:\n+  archiver:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1439545+02:00\"\n+    description: A Helm chart for deploying the HDB++ archiver for the MVP on Kubernetes\n+    digest: 605d7cb7d42885eb6ccf8929ec24e4cd20e1a3d21af107e764f1c079a1e03a39\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: archiver\n+    urls:\n+    - archiver-0.2.0.tgz\n+    version: 0.2.0\n+  auth:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1456296+02:00\"\n+    description: A Helm chart for RBAC SKA\n+    digest: a911d979968ff966c60ef7c632f883a619a795a6816461b128fb64c9b63479cf\n+    name: auth\n+    urls:\n+    - auth-0.1.0.tgz\n+    version: 0.1.0\n+  cbf-proto:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1466261+02:00\"\n+    description: A Helm chart for deploying the CSP_Mid.LMC CBF prototype on Kubernetes\n+    digest: 490dab1d04cd366cc6d3f7afd0fb29ae9cc8e71dd03782ff0b1183416a216131\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: cbf-proto\n+    urls:\n+    - cbf-proto-0.4.0.tgz\n+    version: 0.4.0\n+  csp-proto:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1480843+02:00\"\n+    description: A Helm chart for deploying the Mid_CSP prototype on Kubernetes\n+    digest: 241a6c94fa13839012428bb93300b3baabfc61d9631bb52c14838c3f4bdb2fc6\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: csp-proto\n+    urls:\n+    - csp-proto-0.5.3.tgz\n+    version: 0.5.3\n+  dsh-lmc-prototype:\n+  - apiVersion: v2\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1511111+02:00\"\n+    description: A Helm chart for deploying the DSH LMC prototype on Kubernetes\n+    digest: 0f185124ea7165cff9999b96aab29a6128e9f76dcc9f9cf205d126b7fe0e4a1d\n+    home: https://gitlab.com/ska-telescope/dsh-lmc-prototype/\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: dsh-lmc-prototype\n+    sources:\n+    - https://gitlab.com/ska-telescope/dsh-lmc-prototype/\n+    urls:\n+    - dsh-lmc-prototype-0.0.1.tgz\n+    version: 0.0.1\n+  logging:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1522409+02:00\"\n+    description: A Helm chart for deploying the EFK stack\n+    digest: 3815fa8eee351c3747bb68e67aa889ad1375edd704824fbae65fb7f23195aee3\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    maintainers:\n+    - email: aventer@ska.ac.za\n+      name: Johan Venter\n+    - email: swai@ska.ac.za\n+      name: Sett Wai\n+    name: logging\n+    sources:\n+    - https://gitlab.com/ska-telescope/skampi\n+    urls:\n+    - logging-0.1.0.tgz\n+    version: 0.1.0\n+  oet:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1536831+02:00\"\n+    description: A Helm chart for deploying the Observation Execution Tool on Kubernetes\n+    digest: ce07cf68daddfaba121810950b6d1790a3bed8b4960ea728a59df8420373be0c\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: oet\n+    urls:\n+    - oet-0.1.0.tgz\n+    version: 0.1.0\n+  sdp-prototype:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1546591+02:00\"\n+    description: Helm chart to deploy the SDP Prototype\n+    digest: 81441ffd3c623d682c4ff61fa2909f1151115a553f5112edc27e288a0b973a5c\n+    home: https://developer.skatelescope.org/projects/sdp-prototype/\n+    icon: http://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    maintainers:\n+    - email: pw410@cam.ac.uk\n+      name: Peter Wortmann\n+    - email: vla22@mrao.cam.ac.uk\n+      name: vla22\n+    - email: maja1@mrao.cam.ac.uk\n+      name: Mark Ashdown\n+    name: sdp-prototype\n+    sources:\n+    - https://gitlab.com/ska-telescope/sdp-prototype\n+    urls:\n+    - sdp-prototype-0.4.0.tgz\n+    version: 0.4.0\n+  skampi:\n+  - apiVersion: v2\n+    appVersion: 0.1.0\n+    created: \"2020-07-24T09:30:13.164379+02:00\"\n+    description: A Helm chart for deploying the skampi collection\n+    digest: ae891f6f4ffeeeda31bb8b488447dceec78b715f12f2ad9c0c791e7a56e9f07a\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: skampi\n+    type: application\n+    urls:\n+    - skampi-0.1.0.tgz\n+    version: 0.1.0\n+  skuid:\n+  - apiVersion: v1\n+    appVersion: \"0.1\"\n+    created: \"2020-07-24T09:30:13.1651826+02:00\"\n+    description: Service that returns unique IDs for use by SKA\n+    digest: 43620502bdd9497448fc8cc16fad6dd781fd30486efe77b7ef3a2cbd8bfd6f28\n+    home: https://gitlab.com/ska-telescope/skuid/\n+    icon: http://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    keywords:\n+    - SKA\n+    - UID\n+    - skuid\n+    name: skuid\n+    sources:\n+    - https://gitlab.com/ska-telescope/skuid/\n+    urls:\n+    - skuid-0.0.1.tgz\n+    version: 0.0.1\n+  tango-base:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1676317+02:00\"\n+    description: A Helm chart for deploying the TANGO base system on Kubernetes\n+    digest: 1c398002decf511a7fe14e904105cca0ddee024f4d69efc29c842cfe973feaf4\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: tango-base\n+    urls:\n+    - tango-base-0.1.1.tgz\n+    version: 0.1.1\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1663993+02:00\"\n+    description: A Helm chart for deploying the TANGO base system on Kubernetes\n+    digest: 402e4b7056701055c404bc4b18831c42e610ffe95b3f1c0ced2d3a2899baa3b4\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: tango-base\n+    urls:\n+    - tango-base-0.1.0.tgz\n+    version: 0.1.0\n+  tests:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1682809+02:00\"\n+    description: A Helm chart for integration testing\n+    digest: de59d6176479babf15fc0b912a68024729459cbd1b6c32614f1fba409bc5937c\n+    name: tests\n+    urls:\n+    - tests-0.1.0.tgz\n+    version: 0.1.0\n+  tmc-proto:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1722902+02:00\"\n+    description: A Helm chart for deploying the TMC prototype on Kubernetes\n+    digest: efd717c5ce39ada77e8e6c591b2dd55fd257c0e4a405d7a214af602e305be44b\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: tmc-proto\n+    urls:\n+    - tmc-proto-0.1.0.tgz\n+    version: 0.1.0\n+  webjive:\n+  - apiVersion: v1\n+    appVersion: \"1.0\"\n+    created: \"2020-07-24T09:30:13.1732776+02:00\"\n+    description: A Helm chart for deploying the WebJive on Kubernetes\n+    digest: d2b25200e206bba9f1c6a539e00d5cda0f8f9367e9425a9254f64df0c8d32ed3\n+    icon: https://www.skatelescope.org/wp-content/uploads/2016/07/09545_NEW_LOGO_2014.png\n+    name: webjive\n+    urls:\n+    - webjive-0.1.0.tgz\n+    version: 0.1.0\n+generated: \"2020-07-24T09:30:13.1429001+02:00\""}, {"sha": "52db33ed6decb57aa88b14f33e4d94550ee76555", "filename": "TEST_ARTIFACTS/no.secu.nfs.yaml", "status": "added", "additions": 119, "deletions": 0, "changes": 119, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.secu.nfs.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.secu.nfs.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fno.secu.nfs.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,119 @@\n+#apiVersion: v1\n+#kind: Service\n+#metadata:\n+#  name: nfs-server\n+#  labels:\n+#    app: nfs-server\n+#spec:\n+#  ports:\n+#  - port: 111\n+#    protocol: TCP\n+#    name: nfs-111-tcp\n+#  - port: 111\n+#    protocol: UDP\n+#    name: nfs-111-udp\n+#  - port: 2049\n+#    protocol: TCP\n+#    name: nfs-2049-tcp\n+#  #sessionAffinity: ClientIP\n+#  #clusterIP: None\n+#  #type: NodePort # Or LoadBalancer in production w/ proper security\n+#  #type: LoadBalancer\n+#  selector:\n+#    app: nfs-server\n+#\n+#---\n+\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: nfs-server\n+spec:\n+  nodeSelector:\n+    nfs-server: \"true\"\n+  restartPolicy: Always\n+  initContainers:\n+  - name: wait1\n+    #imagePullPolicy: Always\n+    imagePullPolicy: IfNotPresent\n+    image: call518/oaas-init-container:1.0\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    volumeMounts:\n+    - name: init-container-scripts\n+      mountPath: /init-container-scripts\n+    command: [\"/bin/bash\",\"-c\",\"/init-container-scripts/init-check-etcd.sh\"]\n+  containers:\n+  - name: nfs-server\n+    image: call518/oaas-nfs-server:1.0\n+    securityContext:\n+      privileged: true\n+    ports:\n+    - containerPort: 111\n+      protocol: TCP\n+    - containerPort: 111\n+      protocol: UDP\n+    - containerPort: 2049\n+      protocol: TCP\n+    volumeMounts:\n+    - name: pvc-nfs-server\n+      mountPath: /data\n+    envFrom:\n+      - configMapRef:\n+          name: env-common\n+    env:\n+    - name: MY_POD_IP\n+      valueFrom:\n+        fieldRef:\n+          fieldPath: status.podIP\n+    - name: SHARED_DIRECTORY\n+      value: /data\n+    - name: SYNC\n+      value: \"true\"\n+    - name: FSID\n+      value: \"true\"\n+    command:\n+      - \"bash\"\n+      - \"-c\"\n+      - |\n+        until [ \"$CHECK_ETCD_NFS_SERVER_IP\" == \"$MY_POD_IP\" ];\n+        do\n+          echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` Putting nfs-server to etcd....... waiting...\";\n+          curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XPUT -d value=\"$MY_POD_IP\";\n+          CHECK_ETCD_NFS_SERVER_IP=$(curl --connect-timeout 3 -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XGET | jq -r .node.value)\n+          sleep 5;\n+        done;\n+        echo \"`date +\"[%Y-%m-%d %H:%M:%S]\"` OK~ etcd for nfs-server is ready~~ (etcd's nfs-server IP: $CHECK_ETCD_NFS_SERVER_IP)\";\n+        rm -rf /data/*;\n+        mkdir -p /data/pv/galera-{0,1,2};\n+        mkdir -p /data/pv/mongodb-{0,1,2};\n+        mkdir -p /data/pv/rabbitmq-{0,1,2};\n+        mkdir -p /data/pv/glance-images;\n+        mkdir -p /data/pv/zookeeper-{0,1,2};\n+        mkdir -p /data/pv/cinder-volumes;\n+        mkdir -p /data/pv/cinder-backups;\n+        #mkdir -p /data/pv/cinder-lock_path;\n+        #mkdir -p /data/pv/nova-server-lock_path;\n+        #mkdir -p /data/pv/nova-compute-lock_path;\n+        mkdir -p /data/pv/nova-compute-images;\n+        mkdir -p /data/pv/nova-compute-instances;\n+        mkdir -p /data/pv/ceilometer-gnocchi;\n+        chmod 777 /data/pv/cinder-* /data/pv/zookeeper-*\n+        /usr/bin/nfsd.sh;\n+    lifecycle:\n+      preStop:\n+        exec:\n+          command:\n+          - /bin/sh\n+          - -c\n+          - >\n+            curl -s -L \"http://$DISCOVERY_SERVICE/v2/keys/oaas/$K8S_NFS_SERVER_IP_ETC_KEY\" -XDELETE;\n+  volumes:\n+  - name: init-container-scripts\n+    configMap:\n+      name: init-container-scripts\n+      defaultMode: 0755\n+  - name: pvc-nfs-server\n+    persistentVolumeClaim:\n+      claimName: pvc-nfs-server"}, {"sha": "2a4135407505078949b47472f73510947dceac04", "filename": "TEST_ARTIFACTS/no.secu.present.yaml", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.secu.present.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fno.secu.present.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fno.secu.present.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,43 @@\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: gitlab-runner-docker\n+  namespace: gitlab\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        name: docker-runner\n+        app: gitlab-runner\n+    spec:\n+      containers:\n+      - name: gitlab-runner-docker\n+        image: gitlab/gitlab-runner:alpine-v9.0.0\n+        command: [\"/bin/bash\", \"/scripts/entrypoint\"]\n+        imagePullPolicy: IfNotPresent\n+        env:\n+        - name: REGISTRATION_TOKEN\n+          valueFrom:\n+            secretKeyRef:\n+              name: gitlab-secrets\n+              key: initial_shared_runners_registration_token\n+        resources:\n+          limits:\n+            memory: 500Mi\n+            cpu: 600m\n+          requests:\n+            memory: 500Mi\n+            cpu: 600m\n+        volumeMounts:\n+        - name: scripts\n+          mountPath: /scripts\n+        - name: var-run-docker-sock\n+          mountPath: /var/run/docker.sock\n+      volumes:\n+      - name: var-run-docker-sock\n+        hostPath:\n+          path: /var/run/docker.sock\n+      - name: scripts\n+        configMap:\n+          name: gitlab-runner-scripts"}, {"sha": "15b8af13f41521cf9a59f819ed0b5adc5e766c67", "filename": "TEST_ARTIFACTS/php.roll.present.yaml", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fphp.roll.present.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fphp.roll.present.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fphp.roll.present.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,63 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: php\n+  labels:\n+    app: blog-demo\n+    serviceType: php\n+spec:\n+  revisionHistoryLimit: 3\n+  minReadySeconds: 20\n+  progressDeadlineSeconds: 60\n+  strategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxSurge: 20%\n+      maxUnavailable: 0%\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      serviceType: php\n+  template:\n+    metadata:\n+      labels:\n+        serviceType: php\n+        app: blog-demo\n+    spec:\n+      containers:\n+      - name: php\n+        readinessProbe:\n+          exec:\n+            command:\n+              [\"cgi-fcgi\", \"-bind\", \"-connect\", \"127.0.0.1:9000\"]\n+          initialDelaySeconds: 5\n+          periodSeconds: 15\n+          timeoutSeconds: 10\n+        livenessProbe:\n+          tcpSocket:\n+            port: 9000\n+          initialDelaySeconds: 5\n+          periodSeconds: 15\n+          timeoutSeconds: 5\n+        image: registry.gitlab.com/obtao/blog-demo/php:latest\n+        imagePullPolicy: Always\n+        env:\n+        #PROBE DEFAULT VALUES\n+        - name: SCRIPT_NAME\n+          value: \"index.php\"\n+        - name: SCRIPT_FILENAME\n+          value: /srv/app/public/index.php\n+        - name: REQUEST_METHOD\n+          value: GET\n+        envFrom:\n+        - configMapRef:\n+            name: fpm-app-config\n+        resources:\n+          limits:\n+            cpu: 100m\n+            memory: 256Mi\n+          requests:\n+            cpu: 100m\n+            memory: 256Mi\n+        securityContext:\n+          runAsUser: 82"}, {"sha": "f738df9f0179b9ec529d79633c8f2f6e8b04367e", "filename": "TEST_ARTIFACTS/present.default.ksql.yaml", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.default.ksql.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.default.ksql.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fpresent.default.ksql.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,42 @@\n+###\n+# Example pod with containers for using KSQL on Kubernetes. Not for production.\n+#\n+# Before you run:\n+# - Note the bootstrap servers are `my-confluent-oss-cp-kafka:9092`. You may need to change this with your own connection strings\n+#\n+# Run the pod:\n+#   $ kubectl apply -f examples/ksql-demo.yaml\n+#\n+# Run KSQL CLI:\n+#   $ kubectl exec -it ksql-demo --container ksql -- /bin/bash ksql\n+#   ksql> list topics ;\n+#   ksql> print 'pageviews';\n+#\n+#   Then create any query: https://docs.confluent.io/current/ksql/docs/tutorials/basics-docker.html#create-a-stream-and-table\n+#\n+###\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: ksql-demo\n+  namespace: default\n+spec:\n+  containers:\n+  - name: ksql-datagen-pageviews\n+    image: confluentinc/ksql-examples:5.3.0\n+    command:\n+      - sh\n+      - -c\n+      - \"exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\"\n+  - name: ksql-datagen-users\n+    image: confluentinc/ksql-examples:5.2.0\n+    command:\n+      - sh\n+      - -c\n+      - \"ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\"\n+  - name: ksql\n+    image: confluentinc/ksql-cli:5.2.0\n+    command:\n+      - sh\n+      - -c\n+      - \"exec tail -f /dev/null\""}, {"sha": "138031bea60eaeb718bd4fda6f14817dd0cb3421", "filename": "TEST_ARTIFACTS/present.deployment.default.yaml", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.deployment.default.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.deployment.default.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fpresent.deployment.default.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,68 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    deployment.kubernetes.io/revision: \"1\"\n+    kubectl.kubernetes.io/last-applied-configuration: |\n+      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.16\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":8080}]}]}}}}\n+  creationTimestamp: \"2020-01-24T10:54:56Z\"\n+  generation: 1\n+  labels:\n+    app: nginx\n+  name: nginx-deployment\n+  namespace: default\n+  resourceVersion: \"96574\"\n+  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment\n+  uid: e1075fa3-6468-43d0-83c0-63fede0dae51\n+spec:\n+  progressDeadlineSeconds: 600\n+  replicas: 2\n+  revisionHistoryLimit: 10\n+  selector:\n+    matchLabels:\n+      app: nginx\n+  strategy:\n+    rollingUpdate:\n+      maxSurge: 25%\n+      maxUnavailable: 25%\n+    type: RollingUpdate\n+  template:\n+    metadata:\n+      creationTimestamp: null\n+      labels:\n+        app: nginx\n+    spec:\n+      containers:\n+      - image: nginx:1.16\n+        imagePullPolicy: IfNotPresent\n+        name: nginx\n+        ports:\n+        - containerPort: 8080\n+          protocol: TCP\n+        resources: {}\n+        terminationMessagePath: /dev/termination-log\n+        terminationMessagePolicy: File\n+      dnsPolicy: ClusterFirst\n+      restartPolicy: Always\n+      schedulerName: default-scheduler\n+      securityContext: {}\n+      terminationGracePeriodSeconds: 30\n+status:\n+  availableReplicas: 2\n+  conditions:\n+  - lastTransitionTime: \"2020-01-24T10:54:59Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: Deployment has minimum availability.\n+    reason: MinimumReplicasAvailable\n+    status: \"True\"\n+    type: Available\n+  - lastTransitionTime: \"2020-01-24T10:54:56Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: ReplicaSet \"nginx-deployment-7d64f4b574\" has successfully progressed.\n+    reason: NewReplicaSetAvailable\n+    status: \"True\"\n+    type: Progressing\n+  observedGeneration: 1\n+  readyReplicas: 2\n+  replicas: 2\n+  updatedReplicas: 2"}, {"sha": "138031bea60eaeb718bd4fda6f14817dd0cb3421", "filename": "TEST_ARTIFACTS/present.roll.yaml", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.roll.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.roll.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fpresent.roll.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,68 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  annotations:\n+    deployment.kubernetes.io/revision: \"1\"\n+    kubectl.kubernetes.io/last-applied-configuration: |\n+      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"nginx\"},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.16\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":8080}]}]}}}}\n+  creationTimestamp: \"2020-01-24T10:54:56Z\"\n+  generation: 1\n+  labels:\n+    app: nginx\n+  name: nginx-deployment\n+  namespace: default\n+  resourceVersion: \"96574\"\n+  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment\n+  uid: e1075fa3-6468-43d0-83c0-63fede0dae51\n+spec:\n+  progressDeadlineSeconds: 600\n+  replicas: 2\n+  revisionHistoryLimit: 10\n+  selector:\n+    matchLabels:\n+      app: nginx\n+  strategy:\n+    rollingUpdate:\n+      maxSurge: 25%\n+      maxUnavailable: 25%\n+    type: RollingUpdate\n+  template:\n+    metadata:\n+      creationTimestamp: null\n+      labels:\n+        app: nginx\n+    spec:\n+      containers:\n+      - image: nginx:1.16\n+        imagePullPolicy: IfNotPresent\n+        name: nginx\n+        ports:\n+        - containerPort: 8080\n+          protocol: TCP\n+        resources: {}\n+        terminationMessagePath: /dev/termination-log\n+        terminationMessagePolicy: File\n+      dnsPolicy: ClusterFirst\n+      restartPolicy: Always\n+      schedulerName: default-scheduler\n+      securityContext: {}\n+      terminationGracePeriodSeconds: 30\n+status:\n+  availableReplicas: 2\n+  conditions:\n+  - lastTransitionTime: \"2020-01-24T10:54:59Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: Deployment has minimum availability.\n+    reason: MinimumReplicasAvailable\n+    status: \"True\"\n+    type: Available\n+  - lastTransitionTime: \"2020-01-24T10:54:56Z\"\n+    lastUpdateTime: \"2020-01-24T10:54:59Z\"\n+    message: ReplicaSet \"nginx-deployment-7d64f4b574\" has successfully progressed.\n+    reason: NewReplicaSetAvailable\n+    status: \"True\"\n+    type: Progressing\n+  observedGeneration: 1\n+  readyReplicas: 2\n+  replicas: 2\n+  updatedReplicas: 2"}, {"sha": "685e026c5de5631cf4eb3c7b5ee62fa68d208071", "filename": "TEST_ARTIFACTS/present.varnish.yaml", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.varnish.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fpresent.varnish.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fpresent.varnish.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,98 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: varnish\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: varnish-ingress\n+  template:\n+    metadata:\n+      labels:\n+        app: varnish-ingress\n+    spec:\n+      serviceAccountName: varnish-ingress\n+      securityContext:\n+        # group varnish in the varnish and haproxy containers\n+        # The varnish and haproxy users belong to this group.\n+        fsGroup: 998\n+      containers:\n+      - image: varnish-ingress/varnish\n+        imagePullPolicy: IfNotPresent\n+        name: varnish-ingress\n+        ports:\n+        - name: http\n+          containerPort: 80\n+        - name: k8s\n+          containerPort: 8080\n+        volumeMounts:\n+        - name: adm-secret\n+          mountPath: \"/var/run/varnish\"\n+          readOnly: true\n+        - name: varnish-home\n+          mountPath: \"/var/run/varnish-home\"\n+        - name: offload\n+          mountPath: \"/var/run/offload\"\n+        livenessProbe:\n+          exec:\n+            command:\n+            - /usr/bin/pgrep\n+            - -P\n+            - \"0\"\n+            - varnishd\n+        readinessProbe:\n+          httpGet:\n+            path: /ready\n+            port: k8s\n+        args:\n+          - -n\n+          - /var/run/varnish-home\n+      - image: varnish-ingress/haproxy\n+        imagePullPolicy: IfNotPresent\n+        name: varnish-ingress-offloader\n+        ports:\n+        - name: tls\n+          containerPort: 443\n+        - name: k8s\n+          containerPort: 8443\n+        volumeMounts:\n+        - name: tls-cert\n+          mountPath: \"/etc/ssl/private\"\n+        - name: offload\n+          mountPath: \"/var/run/offload\"\n+        env:\n+          - name: SECRET_DATAPLANEAPI\n+            valueFrom:\n+              secretKeyRef:\n+                name: adm-secret\n+                key: dataplaneapi\n+          - name: POD_NAMESPACE\n+            valueFrom:\n+              fieldRef:\n+                fieldPath: metadata.namespace\n+        livenessProbe:\n+          exec:\n+            command:\n+            - /usr/bin/pgrep\n+            - -P\n+            - \"0\"\n+            - haproxy\n+        readinessProbe:\n+          httpGet:\n+            path: /healthz\n+            port: k8s\n+      volumes:\n+      - name: adm-secret\n+        secret:\n+          secretName: adm-secret\n+          items:\n+          - key: admin\n+            path: _.secret\n+      - name: tls-cert\n+        emptyDir: {}\n+      - name: varnish-home\n+        emptyDir:\n+          medium: \"Memory\"\n+      - name: offload\n+        emptyDir: {}"}, {"sha": "64f7cfb78baa20dc1f188af63b538dca169bbaaf", "filename": "TEST_ARTIFACTS/reso.app.yml", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Freso.app.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Freso.app.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Freso.app.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,23 @@\n+server.port: 8083\n+\n+# Logging\n+logging.level.org.springframework: INFO\n+\n+# Spring Config\n+spring:\n+  application:\n+    name: owners-service\n+  cloud:\n+    refresh:\n+      refreshable: false\n+\n+customers-service:\n+  url: \"http://localhost:8080\"\n+  ribbon:\n+    eureka:\n+      enabled: false\n+visits-service:\n+  url: \"http://localhost:8085\"\n+  ribbon:\n+    eureka:\n+      enabled: false"}, {"sha": "e4bbd74f33a340137ca0c074a07c7916fa96e998", "filename": "TEST_ARTIFACTS/roll.present.demo.yaml", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Froll.present.demo.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Froll.present.demo.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Froll.present.demo.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,24 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: my-app\n+  labels:\n+    app: my-app\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: my-app\n+  template:\n+    metadata:\n+      labels:\n+        app: my-app\n+    spec:\n+      containers:\n+        - name: my-app\n+          image: my-image\n+          env:\n+            - name: SOME_ENV\n+              value: $SOME_ENV\n+          ports:\n+            - containerPort: 8080"}, {"sha": "06839bd16e04b4649030eebcc976cfeacfb1dcc3", "filename": "TEST_ARTIFACTS/roll.present.deploy.yaml", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Froll.present.deploy.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Froll.present.deploy.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Froll.present.deploy.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,21 @@\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: nginx-deployment\n+  labels:\n+    app: nginx\n+spec:\n+  replicas: 2\n+  selector:\n+    matchLabels:\n+      app: nginx\n+  template:\n+    metadata:\n+      labels:\n+        app: nginx\n+    spec:\n+      containers:\n+      - name: nginx\n+        image: nginx:1.16\n+        ports:\n+        - containerPort: 8080"}, {"sha": "1f750618ac61b6b5c20e0f33d671ce1d00e6c109", "filename": "TEST_ARTIFACTS/sample-nfs-server.yaml", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fsample-nfs-server.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fsample-nfs-server.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fsample-nfs-server.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,32 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+ name: sample-nfs-server\n+spec:\n+ #hostNetwork: true\n+ containers:\n+ - name: sample-nfs-server\n+   image: call518/oaas-nfs-server\n+   securityContext:\n+     privileged: true\n+#   ports:\n+#   - containerPort: 3306\n+   command:\n+     - \"bash\"\n+     - \"-c\"\n+     - |\n+       service rsyslog restart;\n+       tail -F /var/log/messages;\n+   env:\n+#     - name: MY_POD_NAME\n+#       valueFrom:\n+#         fieldRef:\n+#           fieldPath: metadata.name\n+#     - name: MY_POD_NAMESPACE\n+#       valueFrom:\n+#         fieldRef:\n+#           fieldPath: metadata.namespace\n+#     - name: MY_POD_IP\n+#       valueFrom:\n+#         fieldRef:\n+#           fieldPath: status.podIP"}, {"sha": "a47cef126d74fbadaf977e2f01305d3bceac9140", "filename": "TEST_ARTIFACTS/skampi.values.yaml", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fskampi.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fskampi.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fskampi.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,36 @@\n+# Default values for dsh-lmc-prototype.\n+# This is a YAML-formatted file.\n+# Declare variables to be passed into your templates.\n+\n+dshlmc:\n+  enabled: false\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-telescope\n+    image: dsh_lmc_prototype\n+    tag: 0.1.0\n+    pullPolicy: IfNotPresent\n+  db:\n+    db: tango\n+    user: tango\n+    password: tango\n+\n+  # These numbers are fixed,\n+  # data/configuration.json will have to be updated should these change\n+  dishes:\n+    - 5\n+    - 6\n+    - 7\n+    - 8\n+\n+dsconfig:\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-dsconfig\n+    tag: 1.2.5.1\n+    pullPolicy: IfNotPresent\n+\n+nodeSelector: {}\n+\n+affinity: {}\n+\n+tolerations: []"}, {"sha": "6eddeb7d78a81df118b611c3c1de105f578bcc63", "filename": "TEST_ARTIFACTS/sni.values.yaml", "status": "added", "additions": 168, "deletions": 0, "changes": 168, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fsni.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fsni.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fsni.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,168 @@\n+apps:\n+  coffee:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 2\n+  tea:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 3\n+  whiskey:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 2\n+  vodka:\n+    image: nginxdemos/hello:plain-text\n+    replicas: 3\n+\n+ingress:\n+  name: beverage-ingress\n+  rules:\n+  - host: cafe.example.com\n+    paths:\n+    - path: /tea\n+      app: tea\n+    - path: /coffee\n+      app: coffee\n+  - host: bar.example.com\n+    paths:\n+    - path: /vodka\n+      app: vodka\n+    - path: /whiskey\n+      app: whiskey\n+  tlsSecrets:\n+  - name: cafe-tls-secret\n+    crt: |\n+      -----BEGIN CERTIFICATE-----\n+      MIIDWTCCAkECFHb8EN0l0QwiR4eKKIW6h172z+JrMA0GCSqGSIb3DQEBCwUAMGgx\n+      CzAJBgNVBAYTAkRFMRAwDgYDVQQIDAdIYW1idXJnMRAwDgYDVQQHDAdIYW1idXJn\n+      MRowGAYDVQQKDBFHcmVlbiBNaWRnZXQgQ2FmZTEZMBcGA1UEAwwQY2FmZS5leGFt\n+      cGxlLmNvbTAgFw0yMDA1MDQxNzA5NTlaGA8yMTIwMDQxMDE3MDk1OVowaDELMAkG\n+      A1UEBhMCREUxEDAOBgNVBAgMB0hhbWJ1cmcxEDAOBgNVBAcMB0hhbWJ1cmcxGjAY\n+      BgNVBAoMEUdyZWVuIE1pZGdldCBDYWZlMRkwFwYDVQQDDBBjYWZlLmV4YW1wbGUu\n+      Y29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAts0HCq6fq9gv0uEa\n+      3iOruZ3GnctdCoeGjrQQ4Fh2cQoMm/i3pkDUt6x2pLTQhxlN3oH3WEo1a24r/3S8\n+      Xfy6Xf0Pti+dDiCqAwMd6veu56RItVMO1pmx1wDjGFTuplpnPRtz8EKsaKYfjZd1\n+      BabdhkWhsA9g3nns8+lqeNbvebhk7hiv9lpgDWAnBie+hioan4WQdPZm1/bANH6o\n+      +oWDu1o6Gdrk/iaj2pR73VTFsR2UEmSTpXa35W7/nsmgADIc4RovU+9ho1I4/fSy\n+      jgVlZVBz29yLaDyNuoZljzNhvGqq1wW6Jq/v1uBOPxNH1k3ZQJl4jlG0tsoASnm7\n+      mr9hewIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQBMNCVYMTdlaNaTjJ5Cznk9Gd+u\n+      TSIFmOCetTOt3l0Xe0bSTxboT6Oz9nFDMP2A2HRK/GTp25ec+Ek1iiCIF47RcsGp\n+      Cdug+x4wQVP3pxakJ/odFN1ReZGZCjNwBltxlRXwJhArK5PWmQppmMZPrW1UYW8y\n+      x+m5UREzOzWga6EIlhpMEfgNa0BNCL/2gPaz2MpKXq5We93IDe2O0nlRrrVoDHU2\n+      GFMhTpWSLkloaMzIMlcKR0IGyezG9waVgsliS00bYKp8eRJ5SqCUYvCMuApjoyzW\n+      N2w59p6t5xE7Ktb0cmhZg83ISPTBlGqVJxF0clLob5nWyeutXNkP/KOi38PI\n+      -----END CERTIFICATE-----\n+    key: |\n+      -----BEGIN RSA PRIVATE KEY-----\n+      MIIEpAIBAAKCAQEAts0HCq6fq9gv0uEa3iOruZ3GnctdCoeGjrQQ4Fh2cQoMm/i3\n+      pkDUt6x2pLTQhxlN3oH3WEo1a24r/3S8Xfy6Xf0Pti+dDiCqAwMd6veu56RItVMO\n+      1pmx1wDjGFTuplpnPRtz8EKsaKYfjZd1BabdhkWhsA9g3nns8+lqeNbvebhk7hiv\n+      9lpgDWAnBie+hioan4WQdPZm1/bANH6o+oWDu1o6Gdrk/iaj2pR73VTFsR2UEmST\n+      pXa35W7/nsmgADIc4RovU+9ho1I4/fSyjgVlZVBz29yLaDyNuoZljzNhvGqq1wW6\n+      Jq/v1uBOPxNH1k3ZQJl4jlG0tsoASnm7mr9hewIDAQABAoIBAES7vsQTeNIijYjb\n+      P0D7ZJx8aKv4RVmqL7wElLvmR1KllqwmztbiVZlibZHssuO5bgAWGizGamOkn0KE\n+      YDduyZyBhKDaMlGXkpVjXKJ20vsiWHxlaJTkYWwYV0tU1A8UuvDNG8DhMPaAUCjr\n+      JAMmBPFxySPsBF5itefYgkJBfvXi7sobaCM6A75D+dBLMeq2q+YbIQH/cAojHYfV\n+      7ypyQ1QaY+wsDiCM6n9Qjk4krmHZ/z39y8mO71ytFcMfJJad8LKM5J4p9Qu99qeb\n+      IRDOT/Sb9QXLXWTeCDv5JWPYyFH2u3e/8GsvQLbXYYbfWLNoU6RDaFSc2wmkOwUH\n+      U8pSCDECgYEA3KIQcme//6B2jP31Coa2f8hsENd0nL+EDR9erXLSUga2l0YNPJZj\n+      W6VnNdaeGq92B7Wxgj+dSeeSBdIRhXwABOHHjruG+gotdRRyoO1ldw7mJjN/q3Wx\n+      A1fpJ+J00S1ZO1FbukKZmR7smTS7i73a8V7At3dyjCG6WxErP3N5NM8CgYEA1Bp5\n+      yYIH8oJmPsuJt501k9nU4SdxxQJpb6uZ9QCBqbEsGkWE3vtLErlU8Rnm2HuirMvD\n+      8Q3OsuoupdCTChrJJ04oL/2r60oTGapeDe4BuRM+DRAZ2trCwXy3nT26bZ/DJtur\n+      Hqvt0tey9ee9MiVHWF2biZejd+KMUxPCCoZVS5UCgYEApbz8m+SCH3Yb+DgB7oFZ\n+      8M3PGCuxxto7SVxKVANQKRwv551Q7jWOt9adnJz3Mdai1JHRoaVF87GISOUQEnUe\n+      0owEy5zlfUlN8oiEv4z1zqUbkJDZFCUZ7wgH9tUvqb7mLCAmxtmm5paLZ19sj0H0\n+      iaMDJA8PtmLTyfswwL5uy5MCgYEArdBMgU+nx5oIw+j0IJ4aK+FUzHYQi4vgb3zG\n+      m7ogh7kDFTxnGHwCF4P9Ed9SB5G5y7ToC4BvJLs4IvX7qUouEaHA2SMeYaDAakXs\n+      8albjBkyvm21Yl3nP7w+lALj5bYIrK1TW701FZVhuJaBurhF8So0rdqwQSxMJkCI\n+      wSs4dskCgYBr1LO3GINSwGHt73ueZDtnvFvO+EFDaOFFbsEd14O1mluM4+WrIZky\n+      inZCvygJWzgHF9LCOpoAZxHykMNrEomidpxViAlpBzb/C5CnpzlfiVBqLN3NvOxG\n+      zdkoq6BiZnznsVgoHyP7TQlUX94ahVT01yZ0njPk2aYVipPWUoHQMQ==\n+      -----END RSA PRIVATE KEY-----\n+  - name: bar-tls-secret\n+    crt: |\n+      -----BEGIN CERTIFICATE-----\n+      MIIFtTCCA52gAwIBAgIUL23Y5IPw50RNOsyXUg4Yk9elHw0wDQYJKoZIhvcNAQEL\n+      BQAwajELMAkGA1UEBhMCREUxEDAOBgNVBAgMB0hhbWJ1cmcxEDAOBgNVBAcMB0hh\n+      bWJ1cmcxHTAbBgNVBAoMFFRoZSBOZXh0IFdoaXNrZXkgQmFyMRgwFgYDVQQDDA9i\n+      YXIuZXhhbXBsZS5jb20wHhcNMjAwNTEzMTI0NDMzWhcNNDAwNTA4MTI0NDMzWjBq\n+      MQswCQYDVQQGEwJERTEQMA4GA1UECAwHSGFtYnVyZzEQMA4GA1UEBwwHSGFtYnVy\n+      ZzEdMBsGA1UECgwUVGhlIE5leHQgV2hpc2tleSBCYXIxGDAWBgNVBAMMD2Jhci5l\n+      eGFtcGxlLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAK4LQefd\n+      TO7Q14fhxfWwZ3pM/VUCS1SEo2+efFx11kwXUY4h3R3uV8Ezx7advFnMWANsxA1s\n+      NUNnVoPZ6jZ17DIp/oBbBHUbZ0m/UlH4frQluZX4RfbOtdHGjns1HtfZOkE7qW1J\n+      qht2taLdCjxGob8vJc/EBfKtp4Tbl9U1rlk9E2rxk1KVpVoxgKrgnmbABYDb/yA+\n+      KsDqHmPLnQiVeZ3CoflI9aFMAmcNnw18CfGfnUWyd5vVSgUCSyaw28bTwRJtWhWD\n+      6fB9Rus2E5k5VaBqAL49bFharlV4zivfc2vcXdah1/W5+Vrp/vW/OgpU1p4s7454\n+      kyItYZb2GGKxKVtH1tNatb3X+aY7aFfYTAiSeeCtpkMV1a14sA6B3jw3e7+UzwSc\n+      E9/ljgV+h/JOtbxOexYRCZJel3OLxfKKq9YrdH2tUjCTWJhZH91wzRWLcidNlFL0\n+      I1Q8xlu1Cif46piYTBkgyco4Mj3UHxSpQnVG4+3A7ZT5alemS4iukOZIUEs97CuI\n+      RCC9UgKDHDzXE3GN0Qh7syWiyvqszdrCnB9iLNsv52eBC4DG1D65C9wY/1VdI+k1\n+      6vJ+LqWh2aoKMcctLiq6o6z32uJff3FUkB8q8bWQ/OemDwtfWt9WW3qxRy9NvBlX\n+      N7w1YejTlrAeIAEehZszgbBaDQXUaa60cngHAgMBAAGjUzBRMB0GA1UdDgQWBBTu\n+      4JHYJOrVGPlooPrLZL2TzDeUhjAfBgNVHSMEGDAWgBTu4JHYJOrVGPlooPrLZL2T\n+      zDeUhjAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4ICAQBvaUAyKqMv\n+      1/77rX8bOkBSgscl2T50/Em4HOXo0WFTV6Lef3LIbNNu5xASf/TVX9Ckfrr42CKP\n+      vS1B/H F27L+kB1cmd76BaaLd2mqJ9SYiww8y7N280IzoVUTc72kEzCVUtY/y9zCJ\n+      olbt8zetZ0B4w5cba0TqRQYScDCAWmqnRUGF37IDSlvN3bNnoGOv8PVFsvswtn1l\n+      pIKuyCyO1wCk7BPkdltLXysxe2m+cfIbosdCBKpCKj+iso1FqrPVXaoHiVHGvc9C\n+      36vge9gNhR69sbrePbQrEB1mKp3HVf38qp0mlinOcNbwdRVxwaK33Q7kDO2w7JL9\n+      oucFgd8w/HNqNU/HiemKPKjXrrJGQGQDltvtGEhnWLro8ez0bZZqANSnWLZdXpJX\n+      84Lhb58bMuxBG9jnUc2wcmMbjiISpq8oGhajUAATnkc/B8B1vHZ73lNSdIUL61VA\n+      o7lOZrYW6PSGh1QixHa7D1Nid5hcj6aaymNKyi7ESj5XTlbqJaBb+8zeVOR64HxJ\n+      BFJG0FzRjk/TheVL8aO1Y7cj8woPcWGJj0ZJhBY6kuEN44nv7NbsXkiW4hJ1wHVQ\n+      gWLNsQYCwyES3pIgliBkog54uFMGjpyeUJeATBcZpkvztjXS9GrVQhI6L3jEgN0C\n+      4sa0SgvX/NR/L52KBSUWb4PX+VYWHw01nA==\n+      -----END CERTIFICATE-----\n+    key: |\n+      -----BEGIN PRIVATE KEY-----\n+      MIIJQwIBADANBgkqhkiG9w0BAQEFAASCCS0wggkpAgEAAoICAQCuC0Hn3Uzu0NeH\n+      4cX1sGd6TP1VAktUhKNvnnxcddZMF1GOId0d7lfBM8e2nbxZzFgDbMQNbDVDZ1aD\n+      2eo2dewyKf6AWwR1G2dJv1JR+H60JbmV+EX2zrXRxo57NR7X2TpBO6ltSaobdrWi\n+      3Qo8RqG/LyXPxAXyraeE25fVNa5ZPRNq8ZNSlaVaMYCq4J5mwAWA2/8gPirA6h5j\n+      y50IlXmdwqH5SPWhTAJnDZ8NfAnxn51Fsneb1UoFAksmsNvG08ESbVoVg+nwfUbr\n+      NhOZOVWgagC+PWxYWq5VeM4r33Nr3F3Wodf1ufla6f71vzoKVNaeLO+OeJMiLWGW\n+      9hhisSlbR9bTWrW91/mmO2hX2EwIknngraZDFdWteLAOgd48N3u/lM8EnBPf5Y4F\n+      fofyTrW8TnsWEQmSXpdzi8XyiqvWK3R9rVIwk1iYWR/dcM0Vi3InTZRS9CNUPMZb\n+      tQon+OqYmEwZIMnKODI91B8UqUJ1RuPtwO2U+WpXpkuIrpDmSFBLPewriEQgvVIC\n+      gxw81xNxjdEIe7Mlosr6rM3awpwfYizbL+dngQuAxtQ+uQvcGP9VXSPpNeryfi6l\n+      odmqCjHHLS4quqOs99riX39xVJAfKvG1kPznpg8LX1rfVlt6sUcvTbwZVze8NWHo\n+      05awHiABHoWbM4GwWg0F1GmutHJ4BwIDAQABAoICAEd+5GIFbNcl/4QYYSPehYOe\n+      IOtM9/kOS71Mk7W/ynqTkbMbgiQLhw0c4kvIXFlfMkCl65u/+dlomAet+yLIKnEp\n+      Ax1jRl99FF8dMwntVM9YN/a9eLA8lkBImrtORQ9SczXc9mqoujJx/4eZ2dyM/2D0\n+      U0oYMoFQiOJw+txhIvARwOpLtsNUKgr1DvAjOa7n7trShOmP4CxDgJxqRmYCUWVX\n+      UQaAzDaobMw8sjvt2n/hm8/H0o63faK1IH4SZRY2YrfZKApymCVssTdqjX6CKQSu\n+      xwNfZCSfi8Ic0EUBk/6ZFgtXjMmqzh5kxZHaLlOUKl3sA7S5H2gI0HAdREM2l8/0\n+      MgBC7z1k9+31NoLhZ5sVPQ0nS1Em+SAO0I6+NjJRy7GkKWkp8KwFEMBD4f9Ruupw\n+      05aGmIu9U9gBOEr79smhYhPvplAcglBw8Kbjq63d+Noxj4QG9I9fzjdNcoWvcH4z\n+      DAMWFTETkrSAM4nRzRa1bloOqE0kRhgKLO/acOlrzJUq+8J47K2X6AIeG/ZOOFdR\n+      mUEaK5XLBbZFYBIz5TshRR9cJAjGh9VpRExI2yNv6gUSI+AGcOovAx2AIR0LZ5eQ\n+      fuLflH+kp68MgskhC4cBKSq6pii9Eve77rPHZUQvKKjOSKEv13rglj5wZ3aDqFnN\n+      jiMfJvum30nFe6f4j7qRAoIBAQDVZRGgqa6Yz/4LAOWrkUy41WYKMObA633pIIIQ\n+      rq52H1BEwcfNH6tt0BdT6cTyCoK5+J/ih4Bqug2Qh3U9Gami7qLNT46dxt9dNn40\n+      TeQQkeoYMNggCM7Z5+YHXsLiEpCa7gF0xuxw7ZeYI47+3fmzr6heH4KO3IhtekV6\n+      ZsA3LygUzZald5isJbtRqlMS9VjKJSOWoYMu9ENm45dHjXE9gQagMs9xANmUwEax\n+      e5bJWLXDOtXG7oWGv+Jm9w+uEjk+tSLyYGMe6GrMXExzCTepnuBeO1UA/Xhz3kAi\n+      Ufg2va9RIcEw75BbhOfUniyLTWNefio5J6QdDNqBiZpv/sglAoIBAQDQyuiIjFno\n+      trkVyyft/Bf735ocH2BhN3vXmD52HxkjOiHCUf5g+ZZf2R0AZUiAzPYH7dLpRdF2\n+      zpvZWfMKWEmNkL1codpSDJt00Snx8PZ4gsiWOwN8mL1fFpvoV2arB5kyHwcgHOQM\n+      Cfp5maMEOXWZNClrh+D21lc8RPeYMQjUYt9/wZbWmPgTtMT1GbREWWFC25WYei0k\n+      8CsoakIS3RdAHJTbvqoubSqZT1jWtkjlQDjAOPfzHQ3vTLc3x4eCGJDNalXnRJur\n+      pyPWSoO5kGmtGeTthcRw4uVy8nqETUFlNcOzVREwcz1xI9rXA9vhy7yi1k/HiPA1\n+      D66FWrFaa6G7AoIBAQCc7krcYGzqLGujI/HDDoPhme4EqJnKXmSmQSXlptDeRYD+\n+      T5PkIdosU9AUAeK4LUqeAV1zdjrWQiUfmL57RJggHmbTniI/nbU+E4kUZgPGu8fw\n+      KluGk3OrhIMCAIpJP2Xgyg+AFZpkIhZN6DiM7iloH1IuhfW5oi0idb0Kmu3Yp3FO\n+      ezLCVQWN8+Gh2SRm2M+HOXDGodibez7mN5FVKYuRs4Vv4m3zqLBaWFykwULOp9Jj\n+      1KzKMzc3NX4GQsLhPL2khAlDPecnH70KtQXzw1+P+ir+oZuNstoWO+fmVWm4uB5q\n+      B+zPVB5Rb5geIISZnTvqjdX3WlOymXVHti5BFpmRAoIBAAfkM2e9zkQia9psBEVV\n+      at6lM+DuOqlR/IdIhMvYHw4ay13Z1YB6znku7o6uRVBA7ueb0IXqkqEn6/IKGUqB\n+      zb3hA5c1ste5DEMdCLXRQq+JWeV7s4UJDNdENn5Ql1vNfLfNPmqzTNc7pVDlQqkN\n+      NumkdBBRYWpS7ZckkCsbZ1cHqaTdf0L7Ix0zjuIop4yRyEBLplrN+1jTDv6HDZpC\n+      6vcMXX/0s9/vVlXXDueGmji39a0mOhDhPz6VKrOcAf4jyY1KAJcuG6ggOBWIWXQx\n+      Bh15xhJIJQWTPdLbYVAQz3Dw2EW16GFpaaAWF9ZamfvtxGJvMTK8dT+8KP93Tw64\n+      1LMCggEBALFfYL4a3f85pG8lNbqxH3Sf/Ca7EL8+PIXHqhF2qwEaUmf3CVfinXkF\n+      l9h37PmnJgdiE7fZKMhF8lkDvun9wbLO6Do4hu13U72EAsBhL8bH9RM7XU0AeZbi\n+      wlT2wyPnVCKS27pT6ZjbiBX6fNK2dNPu71f0OF89UCrIPm60GZ6/6/MqFWPHu5nl\n+      ubnSQwz1zPYr/6/A2i9ITXt+t8ysxL6ASuGN9JRM2M2sjz7A4iFeoAE13ez5SWcu\n+      SakM9r1U7hGdgw8j9tWp8D4WDwEayg+LHqw/veerjSP+iv47zM1eO2X3bzeS/q1K\n+      sv2NYF2XBWr0oPa9xPvwOZMWFcKSRzw=\n+      -----END PRIVATE KEY-----"}, {"sha": "e87c7762cc866cf46f83693e6a4636885ce95db0", "filename": "TEST_ARTIFACTS/special.secret1.yaml", "status": "added", "additions": 106, "deletions": 0, "changes": 106, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fspecial.secret1.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fspecial.secret1.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fspecial.secret1.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,106 @@\n+#reff: https://github.com/piomin/course-kubernetes-microservices/blob/master/best-practices/k8s/deployment.yaml\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: postgresql\n+data:\n+  postgresql-name: test\n+  postgresql-user: test\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: postgresql\n+type: Opaque\n+data:\n+  postgresql-password: dGVzdDEyMw==\n+---\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: rabbitmq\n+data:\n+  rabbitmq-user: test\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: rabbitmq\n+type: Opaque\n+data:\n+  rabbitmq-password: dGVzdDEyMw==\n+---\n+apiVersion: apps/v1\n+kind: Deployment\n+metadata:\n+  name: best-practices-on-kubernetes-deployment\n+spec:\n+  selector:\n+    matchLabels:\n+      app: best-practices-on-kubernetes\n+  template:\n+    metadata:\n+      labels:\n+        app: best-practices-on-kubernetes\n+    spec:\n+      containers:\n+      - name: best-practices-on-kubernetes\n+        image: piomin/best-practices-on-kubernetes\n+        ports:\n+        - containerPort: 8080\n+        env:\n+          - name: POSTGRES_DATABASE\n+            valueFrom:\n+              configMapKeyRef:\n+                name: postgresql\n+                key: postgresql-name\n+          - name: POSTGRES_USERNAME\n+            valueFrom:\n+              configMapKeyRef:\n+                name: postgresql\n+                key: postgresql-user\n+          - name: POSTGRES_PASSWORD\n+            valueFrom:\n+              secretKeyRef:\n+                name: postgresql\n+                key: postgresql-password\n+          - name: RABBITMQ_USER\n+            valueFrom:\n+              configMapKeyRef:\n+                name: rabbitmq\n+                key: rabbitmq-user\n+          - name: RABBITMQ_PASSWORD\n+            valueFrom:\n+              secretKeyRef:\n+                name: rabbitmq\n+                key: rabbitmq-password\n+        livenessProbe:\n+          httpGet:\n+            port: 8080\n+            path: /actuator/health/liveness\n+            scheme: HTTP\n+          periodSeconds: 3\n+          initialDelaySeconds: 20\n+          failureThreshold: 3\n+          timeoutSeconds: 1\n+        readinessProbe:\n+          httpGet:\n+            port: 8080\n+            path: /actuator/health/readiness\n+            scheme: HTTP\n+          periodSeconds: 3\n+          initialDelaySeconds: 20\n+          failureThreshold: 3\n+          timeoutSeconds: 1\n+---\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: best-practices-on-kubernetes-service\n+spec:\n+  type: ClusterIP\n+  selector:\n+    app: best-practices-on-kubernetes\n+  ports:\n+  - port: 8080\n+    name: http\n\\ No newline at end of file"}, {"sha": "77398009c37c8ff7234f525e9eb2bd4eea2c2772", "filename": "TEST_ARTIFACTS/start_vault_temp.yaml", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fstart_vault_temp.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Fstart_vault_temp.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Fstart_vault_temp.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,42 @@\n+---\n+- name: bootstrap/start_vault_temp | Ensure vault-temp isn't already running\n+  shell: if docker rm -f {{ vault_temp_container_name }} 2>&1 1>/dev/null;then echo true;else echo false;fi\n+  register: vault_temp_stop_check\n+  changed_when: \"'true' in vault_temp_stop_check.stdout\"\n+\n+- name: bootstrap/start_vault_temp | Start single node Vault with file backend\n+  command: >\n+          docker run -d --cap-add=IPC_LOCK --name {{ vault_temp_container_name }}\n+          -p {{ vault_port }}:{{ vault_port }}\n+          -e 'VAULT_LOCAL_CONFIG={{ vault_temp_config|to_json }}'\n+          -v /etc/vault:/etc/vault\n+          {{ vault_image_repo }}:{{ vault_version }} server\n+\n+- name: bootstrap/start_vault_temp | Start again single node Vault with file backend\n+  command: docker start {{ vault_temp_container_name }}\n+\n+- name: bootstrap/start_vault_temp | Initialize vault-temp\n+  hashivault_init:\n+    url: \"http://localhost:{{ vault_port }}/\"\n+    secret_shares: 1\n+    secret_threshold: 1\n+  until: \"vault_temp_init is succeeded\"\n+  retries: 4\n+  delay: \"{{ retry_stagger | random + 3 }}\"\n+  register: vault_temp_init\n+\n+# NOTE: vault_headers and vault_url are used by subsequent issue calls\n+- name: bootstrap/start_vault_temp | Set needed vault facts\n+  set_fact:\n+    vault_leader_url: \"http://{{ inventory_hostname }}:{{ vault_port }}\"\n+    vault_temp_unseal_keys: \"{{ vault_temp_init.keys_base64 }}\"\n+    vault_root_token: \"{{ vault_temp_init.root_token }}\"\n+    vault_headers: \"{{ vault_client_headers|combine({'X-Vault-Token': vault_temp_init.root_token}) }}\"\n+\n+- name: bootstrap/start_vault_temp | Unseal vault-temp\n+  hashivault_unseal:\n+    url: \"http://localhost:{{ vault_port }}/\"\n+    token: \"{{ vault_root_token }}\"\n+    keys: \"{{ item }}\"\n+  with_items: \"{{ vault_temp_unseal_keys|default([]) }}\"\n+  no_log: true"}, {"sha": "cf4a9f776baf973e4831fa767fb700a1bc6b4598", "filename": "TEST_ARTIFACTS/tango.values.yaml", "status": "added", "additions": 268, "deletions": 0, "changes": 268, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftango.values.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftango.values.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftango.values.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,268 @@\n+# Default values for tango-base.\n+# This is a YAML-formatted file.\n+# Declare variables to be passed into your templates.\n+\n+display: \":0\"\n+xauthority: \"~/.Xauthority\"\n+minikube: true\n+homeDir: /home/ubuntu\n+\n+system: SW-infrastructure\n+subsystem: tango-base\n+telescope: SKA-mid\n+\n+tangodb:\n+  enabled: true\n+  use_pv: false\n+  component: tangodb\n+  function: tango-device-configuration\n+  domain: tango-configuration\n+  intent: production\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-db\n+    tag: 10.4.10\n+    pullPolicy: IfNotPresent\n+  db:\n+    rootpw: secret\n+    db: tango\n+    user: tango\n+    password: tango\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 1Gi\n+    limits:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 2Gi\n+  livenessProbe:\n+    enabled: false\n+    initialDelaySeconds: 0\n+    periodSeconds: 10\n+    timeoutSeconds: 1\n+    successThreshold: 1\n+    failureThreshold: 3\n+  readinessProbe:\n+    enabled: false\n+    initialDelaySeconds: 0\n+    periodSeconds: 10\n+    timeoutSeconds: 1\n+    successThreshold: 1\n+    failureThreshold: 3\n+\n+databaseds:\n+  enabled: true\n+  # domain tag was the .Release.Name set it as default empty\n+  # so that the old behaviour can still happen in the chart\n+  component: databaseds\n+  function: tangodb-interface\n+  domain: tango-configuration\n+  domainTag:\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-cpp\n+    tag: 9.3.3\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 128Mi # 128Mi = 0.125 GB mem\n+      ephemeral-storage: 512Mi\n+    limits:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 1Gi\n+  livenessProbe:\n+    enabled: false\n+    initialDelaySeconds: 0\n+    periodSeconds: 10\n+    timeoutSeconds: 1\n+    successThreshold: 1\n+    failureThreshold: 3\n+  readinessProbe:\n+    enabled: false\n+    initialDelaySeconds: 0\n+    periodSeconds: 10\n+    timeoutSeconds: 1\n+    successThreshold: 1\n+    failureThreshold: 3\n+\n+itango:\n+  enabled: false\n+  component: itango-console\n+  function: generic-tango-console\n+  domain: interactive-testing\n+  intent: enabling\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-itango\n+    tag: 9.3.1\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 00m = 0.1 CPU\n+      memory: 128Mi # 128Mi = 0.125 GB mem\n+      ephemeral-storage: 512Mi\n+    limits:\n+      cpu: 100m     # 00m = 0.1 CPU\n+      memory: 128Mi # 128Mi = 0.125 GB mem\n+      ephemeral-storage: 512Mi\n+\n+tangotest:\n+  enabled: false\n+  component: testdevice\n+  function: tango-client-validation-testing\n+  domain: interactive-testing\n+  intent: enabling\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-java\n+    tag: 9.3.3\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 1Gi\n+    limits:\n+      cpu: 500m     # 500m = 0.5 CPU\n+      memory: 512Mi # 512Mi = 0.5 GB mem\n+      ephemeral-storage: 1Gi\n+\n+jive:\n+  enabled: false\n+  component: jive-gui\n+  function: generic-tango-jive-gui\n+  domain: interactive-testing\n+  intent: enabling\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-java\n+    tag: 9.3.3\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+    limits:\n+      cpu: 500m     # 500m = 0.5 CPU\n+      memory: 512Mi # 512Mi = 0.5 GB mem\n+      ephemeral-storage: 256Mi\n+\n+vnc:\n+  enabled: false\n+  component: vnc-gui\n+  function: generic-tango-vnc-gui\n+  domain: interactive-testing\n+  intent: enabling\n+  nodeport_enabled: false\n+  nodeport_vnc: 32081\n+  nodeport_novnc: 32082\n+  replicas: 3\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-vnc\n+    tag: latest\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+    limits:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+\n+vscode:\n+  enabled: false\n+  component: vscode-remote\n+  function: remote-developement\n+  domain: interactive-testing\n+  intent: enabling\n+  nodeport_enabled: false\n+  nodeport: 32080\n+  replicas: 1\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-vscode\n+    tag: latest\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+    limits:\n+      cpu: 200m     # 200m = 0.2 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+\n+tangorest:\n+  enabled: false\n+  replicas: 1\n+  component: tango-rest\n+  function: tango-http-interface\n+  domain: tango-configuration\n+  intent: enabling\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-rest\n+    tag: 1.14\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+    limits:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+\n+logviewer:\n+  enabled: false\n+  component: logviewer\n+  function: tango-log-inspection\n+  domain: interactive-testing\n+  intent: enabling\n+  image:\n+    registry: nexus.engageska-portugal.pt/ska-docker\n+    image: tango-java\n+    tag: 9.3.3\n+    pullPolicy: IfNotPresent\n+  resources:\n+    requests:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 128Mi # 128Mi = 0.125 GB mem\n+      ephemeral-storage: 256Mi\n+    limits:\n+      cpu: 100m     # 100m = 0.1 CPU\n+      memory: 256Mi # 256Mi = 0.25 GB mem\n+      ephemeral-storage: 256Mi\n+\n+# Configure Ingress resource that allow you to access the Tango REST API\n+ingress:\n+  enabled: false\n+  hostname: tango-base.minikube.local\n+\n+  # Ingress annotations\n+  annotations:\n+    kubernetes.io/ingress.class: traefik\n+\n+  # Ingress TLS configuration\n+  #\n+  tls:\n+    enabled: false\n+    secretname: \"tls-secret-tango-base-{{ .Release.Name }}\"\n+    hostname: \"{{ .Values.ingress.hostname }}\"\n+\n+\n+nodeSelector: {}\n+\n+affinity: {}\n+\n+tolerations: []"}, {"sha": "5b63a6b37d071dc63ac4235e3aafb269571fe8d8", "filename": "TEST_ARTIFACTS/tasks.main.yml", "status": "added", "additions": 88, "deletions": 0, "changes": 88, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftasks.main.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftasks.main.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftasks.main.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,88 @@\n+---\n+- name: set_fact distro_setup\n+  set_fact:\n+    distro_setup: \"{{ distro_settings[node_distro] }}\"\n+\n+- name: set_fact other distro settings\n+  set_fact:\n+    distro_image: \"{{ distro_setup['image'] }}\"\n+    distro_init: \"{{ distro_setup['init'] }}\"\n+    distro_pid1_exe: \"{{ distro_setup['pid1_exe'] }}\"\n+    distro_raw_setup: \"{{ distro_setup['raw_setup'] }}\"\n+    distro_raw_setup_done: \"{{ distro_setup['raw_setup_done'] }}\"\n+    distro_agetty_svc: \"{{ distro_setup['agetty_svc'] }}\"\n+\n+- name: Create dind node containers from \"containers\" inventory section\n+  docker_container:\n+    image: \"{{ distro_image }}\"\n+    name: \"{{ item }}\"\n+    state: started\n+    hostname: \"{{ item }}\"\n+    command: \"{{ distro_init }}\"\n+    # recreate: yes\n+    privileged: true\n+    tmpfs:\n+      - /sys/module/nf_conntrack/parameters\n+    volumes:\n+      - /boot:/boot\n+      - /lib/modules:/lib/modules\n+      - \"{{ item }}:/dind/docker\"\n+  register: containers\n+  with_items: \"{{ groups.containers }}\"\n+  tags:\n+    - addresses\n+\n+- name: Gather list of containers IPs\n+  set_fact:\n+    addresses: \"{{ containers.results | map(attribute='ansible_facts') | map(attribute='docker_container') | map(attribute='NetworkSettings') | map(attribute='IPAddress') | list }}\"\n+  tags:\n+    - addresses\n+\n+- name: Create inventory_builder helper already set with the list of node containers' IPs\n+  template:\n+    src: inventory_builder.sh.j2\n+    dest: /tmp/kubespray.dind.inventory_builder.sh\n+    mode: 0755\n+  tags:\n+    - addresses\n+\n+- name: Install needed packages into node containers via raw, need to wait for possible systemd packages to finish installing\n+  raw: |\n+    # agetty processes churn a lot of cpu time failing on inexistent ttys, early STOP them, to rip them in below task\n+    pkill -STOP agetty || true\n+    {{ distro_raw_setup_done }}  && echo SKIPPED && exit 0\n+    until [ \"$(readlink /proc/1/exe)\" = \"{{ distro_pid1_exe }}\" ] ; do sleep 1; done\n+    {{ distro_raw_setup }}\n+  delegate_to: \"{{ item._ansible_item_label|default(item.item) }}\"\n+  with_items: \"{{ containers.results }}\"\n+  register: result\n+  changed_when: result.stdout.find(\"SKIPPED\") < 0\n+\n+- name: Remove gettys from node containers\n+  raw: |\n+    until test -S /var/run/dbus/system_bus_socket; do sleep 1; done\n+    systemctl disable {{ distro_agetty_svc }}\n+    systemctl stop {{ distro_agetty_svc }}\n+  delegate_to: \"{{ item._ansible_item_label|default(item.item) }}\"\n+  with_items: \"{{ containers.results }}\"\n+  changed_when: false\n+\n+# Running systemd-machine-id-setup doesn't create a unique id for each node container on Debian,\n+# handle manually\n+- name: Re-create unique machine-id (as we may just get what comes in the docker image), needed by some CNIs for mac address seeding (notably weave)  # noqa 301\n+  raw: |\n+    echo {{ item | hash('sha1') }} > /etc/machine-id.new\n+    mv -b /etc/machine-id.new /etc/machine-id\n+    cmp /etc/machine-id /etc/machine-id~ || true\n+    systemctl daemon-reload\n+  delegate_to: \"{{ item._ansible_item_label|default(item.item) }}\"\n+  with_items: \"{{ containers.results }}\"\n+\n+- name: Early hack image install to adapt for DIND\n+  # noqa 302 - this task uses the raw module intentionally\n+  raw: |\n+    rm -fv /usr/bin/udevadm /usr/sbin/udevadm\n+  delegate_to: \"{{ item._ansible_item_label|default(item.item) }}\"\n+  with_items: \"{{ containers.results }}\"\n+  register: result\n+  changed_when: result.stdout.find(\"removed\") >= 0"}, {"sha": "99859c3851979dc41d9428623bd950ae822244c6", "filename": "TEST_ARTIFACTS/test.sample.pod.yaml", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftest.sample.pod.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftest.sample.pod.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftest.sample.pod.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,54 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+ name: sample-pod\n+spec:\n+ #hostNetwork: true\n+ containers:\n+ - name: sample-pod\n+   #image: call518/oaas-init-container\n+   image: call518/oaas-ocata\n+   securityContext:\n+     privileged: true\n+#   ports:\n+#   - containerPort: 3306\n+   command:\n+     - \"bash\"\n+     - \"-c\"\n+     - |\n+       service rsyslog restart;\n+       printenv MY_NODE_NAME MY_NODE_IP MY_POD_NAME MY_POD_NAMESPACE;\n+       printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;\n+       tail -F /var/log/syslog;\n+   envFrom:\n+     - configMapRef:\n+         name: env-common\n+   env:\n+     - name: MY_NODE_NAME\n+       valueFrom:\n+         fieldRef:\n+           fieldPath: spec.nodeName\n+     - name: MY_NODE_IP\n+       valueFrom:\n+         fieldRef:\n+           fieldPath: status.hostIP\n+     - name: MY_POD_NAME\n+       valueFrom:\n+         fieldRef:\n+           fieldPath: metadata.name\n+     - name: MY_POD_NAMESPACE\n+       valueFrom:\n+         fieldRef:\n+           fieldPath: metadata.namespace\n+     - name: MY_POD_IP\n+       valueFrom:\n+         fieldRef:\n+           fieldPath: status.podIP\n+   volumeMounts:\n+   - name: openstack-openrc\n+     mountPath: /root/openrc\n+ volumes:\n+ - name: openstack-openrc\n+   configMap:\n+     name: openstack-openrc\n+     defaultMode: 0755"}, {"sha": "b3fcccc663b9745d05194f05dc48412463564744", "filename": "TEST_ARTIFACTS/tp.default.nspace.yaml", "status": "added", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.default.nspace.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.default.nspace.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftp.default.nspace.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,34 @@\n+# reff: https://github.com/IBM/cloud-native-starter/blob/master/security/articles-secure/deployment/articles-sa.yaml\n+kind: Deployment\n+apiVersion: apps/v1\n+metadata:\n+  name: articles\n+  namespace: default\n+  labels:\n+    app: articles\n+spec:\n+  selector:\n+    matchLabels:\n+      app: articles\n+  replicas: 1\n+  template:\n+    metadata:\n+      annotations: \n+        sidecar.istio.io/inject: \"true\"    \n+      labels:\n+        app: articles\n+        version: v1\n+    spec:\n+      serviceAccountName: articles\n+      containers:\n+      - name: articles\n+        image: docker.io/haraldu/articles:secure-v1\n+        imagePullPolicy: Always  \n+        ports:\n+        - containerPort: 8082\n+        env:\n+        - name: QUARKUS_OIDC_AUTH_SERVER_URL\n+          valueFrom:\n+            configMapKeyRef:\n+              name: security-url-config\n+              key: QUARKUS_OIDC_AUTH_SERVER_URL\n\\ No newline at end of file"}, {"sha": "363feb0b93ece5d7b7f363390644936b2e4fc565", "filename": "TEST_ARTIFACTS/tp.host.net2.yaml", "status": "added", "additions": 245, "deletions": 0, "changes": 245, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.host.net2.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.host.net2.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftp.host.net2.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,245 @@\n+# Source: calico/templates/calico-node.yaml\n+# This manifest installs the calico-node container, as well\n+# as the CNI plugins and network config on\n+# each master and worker node in a Kubernetes cluster.\n+kind: DaemonSet\n+apiVersion: apps/v1\n+metadata:\n+  name: calico-node\n+  namespace: kube-system\n+  labels:\n+    k8s-app: calico-node\n+spec:\n+  selector:\n+    matchLabels:\n+      k8s-app: calico-node\n+  updateStrategy:\n+    type: RollingUpdate\n+    rollingUpdate:\n+      maxUnavailable: 1\n+  template:\n+    metadata:\n+      labels:\n+        k8s-app: calico-node\n+      annotations:\n+        # This, along with the CriticalAddonsOnly toleration below,\n+        # marks the pod as a critical add-on, ensuring it gets\n+        # priority scheduling and that its resources are reserved\n+        # if it ever gets evicted.\n+        scheduler.alpha.kubernetes.io/critical-pod: ''\n+    spec:\n+      nodeSelector:\n+        beta.kubernetes.io/os: linux\n+      hostNetwork: true\n+      tolerations:\n+        # Make sure calico-node gets scheduled on all nodes.\n+        - effect: NoSchedule\n+          operator: Exists\n+        # Mark the pod as a critical add-on for rescheduling.\n+        - key: CriticalAddonsOnly\n+          operator: Exists\n+        - effect: NoExecute\n+          operator: Exists\n+      serviceAccountName: calico-node\n+      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n+      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n+      terminationGracePeriodSeconds: 0\n+      priorityClassName: system-node-critical\n+      initContainers:\n+        # This container performs upgrade from host-local IPAM to calico-ipam.\n+        # It can be deleted if this is a fresh installation, or if you have already\n+        # upgraded to use calico-ipam.\n+        - name: upgrade-ipam\n+          image: calico/cni:v3.8.0\n+          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n+          env:\n+            - name: KUBERNETES_NODE_NAME\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: spec.nodeName\n+            - name: CALICO_NETWORKING_BACKEND\n+              valueFrom:\n+                configMapKeyRef:\n+                  name: calico-config\n+                  key: calico_backend\n+          volumeMounts:\n+            - mountPath: /var/lib/cni/networks\n+              name: host-local-net-dir\n+            - mountPath: /host/opt/cni/bin\n+              name: cni-bin-dir\n+        # This container installs the CNI binaries\n+        # and CNI network config file on each node.\n+        - name: install-cni\n+          image: calico/cni:v3.8.0\n+          command: [\"/install-cni.sh\"]\n+          env:\n+            # Name of the CNI config file to create.\n+            - name: CNI_CONF_NAME\n+              value: \"10-calico.conflist\"\n+            # The CNI network config to install on each node.\n+            - name: CNI_NETWORK_CONFIG\n+              valueFrom:\n+                configMapKeyRef:\n+                  name: calico-config\n+                  key: cni_network_config\n+            # Set the hostname based on the k8s node name.\n+            - name: KUBERNETES_NODE_NAME\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: spec.nodeName\n+            # CNI MTU Config variable\n+            - name: CNI_MTU\n+              valueFrom:\n+                configMapKeyRef:\n+                  name: calico-config\n+                  key: veth_mtu\n+            # Prevents the container from sleeping forever.\n+            - name: SLEEP\n+              value: \"false\"\n+          volumeMounts:\n+            - mountPath: /host/opt/cni/bin\n+              name: cni-bin-dir\n+            - mountPath: /host/etc/cni/net.d\n+              name: cni-net-dir\n+        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n+        # to communicate with Felix over the Policy Sync API.\n+        - name: flexvol-driver\n+          image: calico/pod2daemon-flexvol:v3.8.0\n+          volumeMounts:\n+          - name: flexvol-driver-host\n+            mountPath: /host/driver\n+      containers:\n+        # Runs calico-node container on each Kubernetes node.  This\n+        # container programs network policy and routes on each\n+        # host.\n+        - name: calico-node\n+          image: calico/node:v3.8.0\n+          env:\n+            # Use Kubernetes API as the backing datastore.\n+            - name: DATASTORE_TYPE\n+              value: \"kubernetes\"\n+            # Wait for the datastore.\n+            - name: WAIT_FOR_DATASTORE\n+              value: \"true\"\n+            # Set based on the k8s node name.\n+            - name: NODENAME\n+              valueFrom:\n+                fieldRef:\n+                  fieldPath: spec.nodeName\n+            # Choose the backend to use.\n+            - name: CALICO_NETWORKING_BACKEND\n+              valueFrom:\n+                configMapKeyRef:\n+                  name: calico-config\n+                  key: calico_backend\n+            # Cluster type to identify the deployment type\n+            - name: CLUSTER_TYPE\n+              value: \"k8s,kubeadm,bgp\"\n+            # Auto-detect the BGP IP address.\n+            - name: IP\n+              value: \"autodetect\"\n+            - name: IP_AUTODETECTION_METHOD\n+              value: \"interface=eth0\"\n+            # Enable IPIP\n+            - name: CALICO_IPV4POOL_IPIP\n+              value: \"Never\"\n+            # Set MTU for tunnel device used if ipip is enabled\n+            - name: FELIX_IPINIPMTU\n+              valueFrom:\n+                configMapKeyRef:\n+                  name: calico-config\n+                  key: veth_mtu\n+            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n+            # chosen from this range. Changing this value after installation will have\n+            # no effect. This should fall within `--cluster-cidr`.\n+            - name: CALICO_IPV4POOL_CIDR\n+              value: \"10.64.0.0/12\"\n+            - name: CALICO_ADVERTISE_CLUSTER_IPS\n+              value: \"10.80.0.0/12\"\n+            # Disable file logging so `kubectl logs` works.\n+            - name: CALICO_DISABLE_FILE_LOGGING\n+              value: \"true\"\n+            # Set Felix endpoint to host default action to ACCEPT.\n+            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n+              value: \"ACCEPT\"\n+            # Disable IPv6 on Kubernetes.\n+            - name: FELIX_IPV6SUPPORT\n+              value: \"false\"\n+            # Set Felix logging to \"info\"\n+            - name: FELIX_LOGSEVERITYSCREEN\n+              value: \"info\"\n+            - name: FELIX_HEALTHENABLED\n+              value: \"true\"\n+          securityContext:\n+            privileged: true\n+          resources:\n+            requests:\n+              cpu: 250m\n+          livenessProbe:\n+            httpGet:\n+              path: /liveness\n+              port: 9099\n+              host: localhost\n+            periodSeconds: 10\n+            initialDelaySeconds: 10\n+            failureThreshold: 6\n+          readinessProbe:\n+            exec:\n+              command:\n+              - /bin/calico-node\n+              - -bird-ready\n+              - -felix-ready\n+            periodSeconds: 10\n+          volumeMounts:\n+            - mountPath: /lib/modules\n+              name: lib-modules\n+              readOnly: true\n+            - mountPath: /run/xtables.lock\n+              name: xtables-lock\n+              readOnly: false\n+            - mountPath: /var/run/calico\n+              name: var-run-calico\n+              readOnly: false\n+            - mountPath: /var/lib/calico\n+              name: var-lib-calico\n+              readOnly: false\n+            - name: policysync\n+              mountPath: /var/run/nodeagent\n+      volumes:\n+        # Used by calico-node.\n+        - name: lib-modules\n+          hostPath:\n+            path: /lib/modules\n+        - name: var-run-calico\n+          hostPath:\n+            path: /var/run/calico\n+        - name: var-lib-calico\n+          hostPath:\n+            path: /var/lib/calico\n+        - name: xtables-lock\n+          hostPath:\n+            path: /run/xtables.lock\n+            type: FileOrCreate\n+        # Used to install CNI.\n+        - name: cni-bin-dir\n+          hostPath:\n+            path: /opt/cni/bin\n+        - name: cni-net-dir\n+          hostPath:\n+            path: /etc/cni/net.d\n+        # Mount in the directory for host-local IPAM allocations. This is\n+        # used when upgrading from host-local to calico-ipam, and can be removed\n+        # if not using the upgrade-ipam init container.\n+        - name: host-local-net-dir\n+          hostPath:\n+            path: /var/lib/cni/networks\n+        # Used to create per-pod Unix Domain Sockets\n+        - name: policysync\n+          hostPath:\n+            type: DirectoryOrCreate\n+            path: /var/run/nodeagent\n+        # Used to install Flex Volume Driver\n+        - name: flexvol-driver-host\n+          hostPath:\n+            type: DirectoryOrCreate\n+            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n\\ No newline at end of file"}, {"sha": "b70b845cf2af977c242b69b5210c1a8c271798e2", "filename": "TEST_ARTIFACTS/tp.nsp.dflt.yaml", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.nsp.dflt.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.nsp.dflt.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftp.nsp.dflt.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,14 @@\n+# reff: https://github.com/the-gigi/hands-on-microservices-with-kubernetes-code/blob/master/ch1/nginx-hpa.yaml\n+apiVersion: autoscaling/v1\n+kind: HorizontalPodAutoscaler\n+metadata:\n+  name: nginx\n+  namespace: default\n+spec:\n+  maxReplicas: 4\n+  minReplicas: 2\n+  targetCPUUtilizationPercentage: 90\n+  scaleTargetRef:\n+    apiVersion: apps/v1\n+    kind: Deployment\n+    name: nginx\n\\ No newline at end of file"}, {"sha": "81c11ab816511a322ab1c78cb93d98932363f1e9", "filename": "TEST_ARTIFACTS/tp.seccomp.unconfined.yaml", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.seccomp.unconfined.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp.seccomp.unconfined.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftp.seccomp.unconfined.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,18 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: audit-pod\n+  labels:\n+    app: audit-pod\n+spec:\n+  securityContext:\n+    seccompProfile:\n+      type: Unconfined\n+  containers:\n+  - name: test-container\n+    image: hashicorp/http-echo:0.2.3\n+    args:\n+    - \"-text=just made some syscalls!\"\n+    securityContext:\n+      allowPrivilegeEscalation: false\n+# reff: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ and https://kubernetes.io/docs/tutorials/clusters/seccomp/\n\\ No newline at end of file"}, {"sha": "e8fa011778d0aea60c51ef65c83eb0db269d5b9a", "filename": "TEST_ARTIFACTS/tp_secu_context_miss.yaml", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp_secu_context_miss.yaml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_ARTIFACTS%2Ftp_secu_context_miss.yaml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_ARTIFACTS%2Ftp_secu_context_miss.yaml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,16 @@\n+apiVersion: extensions/v1beta1\n+kind: Pod\n+metadata:\n+  name: hello-dep\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        app: hello-dep\n+    spec:\n+      containers:\n+      - name: hello-dep\n+        image: gcr.io/google_containers/echoserver:1.4\n+        ports:\n+        - containerPort: 8080"}, {"sha": "830d28d6ce682c137c87687cd106db4b04ba6c3e", "filename": "TEST_CONSTANTS.py", "status": "added", "additions": 145, "deletions": 0, "changes": 145, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_CONSTANTS.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_CONSTANTS.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_CONSTANTS.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,145 @@\n+'''\n+Akond Rahman \n+April 30, 2021 \n+Placeholder to save test constants \n+'''\n+\n+_test_yaml          = 'TEST_ARTIFACTS/dataimage.airflowimage.manifests.deployment.yaml'\n+_secret_yaml1       = 'TEST_ARTIFACTS/helm.values.yaml'\n+_secret_yaml2       = 'TEST_ARTIFACTS/tango.values.yaml'\n+_secret_yaml3       = 'TEST_ARTIFACTS/charts.values.yaml'\n+_secret_yaml4       = 'TEST_ARTIFACTS/skampi.values.yaml'\n+_secret_yaml5       = 'TEST_ARTIFACTS/minecraft.values.yaml'\n+_secret_yaml6       = 'TEST_ARTIFACTS/kubecf.values.yaml'\n+_secret_yaml7       = 'TEST_ARTIFACTS/nextcloud.values.yaml'\n+_secret_yaml8       = 'TEST_ARTIFACTS/keycloak.values.yaml'\n+_secret_yaml9       = 'TEST_ARTIFACTS/empty.yml'\n+_secret_yaml10      = 'TEST_ARTIFACTS/kubecf.values.yaml'\n+_cert_yaml          = 'TEST_ARTIFACTS/sni.values.yaml'\n+_rsa_key_yaml       = 'TEST_ARTIFACTS/hello.values.yaml' \n+_helm_script1       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/stackgres/stackgres-k8s/install/helm/stackgres-operator/values.yaml'\n+_helm_script2       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/tango-base/values.yaml'\n+_helm_script3       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/archiver/values.yaml'\n+_helm_script4       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/charts/skampi/charts/dsh-lmc-prototype/values.yaml' \n+_helm_script5       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/minecraft/values.yaml'\n+_fp_script1         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/deploy/helm/kubecf/values.yaml'\n+_fp_script2         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/nextcloud/values.yaml'\n+_fp_script3         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/postgres/values.yaml'\n+_fp_script4         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/skampi/partial-deployments/tango-db-standalone/values.yaml'\n+_fp_script5         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/postgres/values-production.yaml'\n+_fp_script6         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/services/keycloak/values.yaml'\n+_fp_script7         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubecf/deploy/helm/kubecf/values.yaml'\n+_fp_script8         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/architectures/multi-controller/varnish-tea.yaml'\n+_fp_script9         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/sni/values.yaml'\n+_fp_script10        = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/k8s-ingress/examples/tls/hello/values.yaml'\n+_privi_scrip1       = 'TEST_ARTIFACTS/test.sample.pod.yaml'\n+_privi_scrip2       = 'TEST_ARTIFACTS/sample-nfs-server.yaml'\n+_privi_scrip3       = 'TEST_ARTIFACTS/artifact.nfs.server.yaml'\n+_privi_scrip4       = 'TEST_ARTIFACTS/deamonset1.yaml'\n+_privi_scrip5       = 'TEST_ARTIFACTS/calico.yaml'\n+_http_script1       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-glance-setup.yaml'\n+_http_script2       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-horizon-setup.yaml'\n+_http_script3       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-neutron-server-setup.yaml'\n+_http_script4       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-init-container-scripts.yaml'\n+_http_script5       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-init-container-scripts.yaml'\n+_http_script6       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-openstack-openrc.yaml'\n+_http_script7       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-ocata/configMap-ceilometer-central-setup.yaml'\n+_http_script8       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-ceilometer-central-setup.yaml'\n+_http_script9       = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-queens/configMap-glance-setup.yaml'\n+_http_script10      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/OpenStack-on-Kubernetes/src-pike/configMap-aodh-setup.yaml'\n+_http_script11      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-elastic-logging/kibana-deployment.yaml'\n+_http_script12      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/gitlab/gitlab-deployment.yml'\n+_http_script13      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/koris/addons/prometheus/00_operator_prometheusCustomResourceDefinition.yaml'\n+_http_script14      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/advanced-kubernetes-workshop/lb/glb-configmap-var.yaml'\n+_http_script15      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-gitlab-demo/gitlab-runner/gitlab-runner-docker-configmap.yml'\n+_http_script16      = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/justin@kubernetes/src/configuration/grafana/values.yaml'\n+_no_secu_cont_yaml1 = 'TEST_ARTIFACTS/no.secu.nfs.yaml'\n+_no_secu_cont_yaml2 = 'TEST_ARTIFACTS/no.secu.present.yaml'\n+_dflt_nspace_yaml1  = 'TEST_ARTIFACTS/present.deployment.default.yaml'\n+_dflt_nspace_yaml2  = 'TEST_ARTIFACTS/present.default.ksql.yaml'\n+_dflt_nspace_yaml3  = 'TEST_ARTIFACTS/absent.default1.yaml'\n+_dflt_nspace_yaml4  = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/manifests/services.yml'\n+test_sink_nspace_yam= '/Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/manifests/deployment.yaml'\n+reso_yaml1          = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/data-image/airflow_image/manifests/deployment.yaml'\n+reso_yaml2          = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes-tutorial-series-youtube/kubernetes-configuration-file-explained/nginx-deployment-result.yaml'\n+reso_yaml3          = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/kubernetes/configurations/deployment.yml'\n+reso_yaml4          = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/obtao@kubernetes/kubernetes/app/nginx-deployment.yaml'\n+fp_rolling_yaml1    = 'TEST_ARTIFACTS/present.roll.yaml'\n+fp_rolling_yaml2    = 'TEST_ARTIFACTS/php.roll.present.yaml'\n+fp_rolling_yaml3    = 'TEST_ARTIFACTS/nginx.present.yaml'\n+fp_rolling_yaml4    = 'TEST_ARTIFACTS/bakis.rs.yaml'\n+fp_rolling_yaml5    = 'TEST_ARTIFACTS/absent.ingress.yaml'\n+fp_rolling_yaml6    = 'TEST_ARTIFACTS/absent.prome.yaml'\n+fp_rolling_yaml7    = 'TEST_ARTIFACTS/present.varnish.yaml'\n+fp_rolling_yaml8    = 'TEST_ARTIFACTS/gcr.deployment.yaml'\n+fp_rolling_yaml9    = 'TEST_ARTIFACTS/roll.present.deploy.yaml'\n+fp_rolling_yaml10   = 'TEST_ARTIFACTS/roll.present.demo.yaml'\n+fp_rolling_yaml11   = 'TEST_ARTIFACTS/fp.concourse.yaml'\n+net_policy_yaml     = 'TEST_ARTIFACTS/k8s.doc.network.yaml'\n+tp_secucont_no_yaml = 'TEST_ARTIFACTS/tp_secu_context_miss.yaml'\n+fp_secucont_no_yaml = 'TEST_ARTIFACTS/fp_secu_context_miss.yaml'\n+secu_cont_fp_yaml   = 'TEST_ARTIFACTS/fp.no.secu.cont.yaml'\n+tp_host_ipc_yaml    = 'TEST_ARTIFACTS/host-ipc-pid-true.yaml'\n+tp_docker_sock_yaml = 'TEST_ARTIFACTS/docker.sock.yaml'\n+tp_host_net_yaml    = 'TEST_ARTIFACTS/host-net-tp.yaml'\n+tp_cap_sys_yaml     = 'TEST_ARTIFACTS/cap.sys.yaml'\n+tp_host_net_yaml2   = 'TEST_ARTIFACTS/tp.host.net2.yaml'\n+tp_allow_privilege  = 'TEST_ARTIFACTS/allow.privilege.yaml'\n+fp_seccomp_unconf   = 'TEST_ARTIFACTS/fp.seccomp.unconfined.yaml'\n+tp_seccomp_unconf   = 'TEST_ARTIFACTS/tp.seccomp.unconfined.yaml'\n+another_dockersock  = 'TEST_ARTIFACTS/ANOTHER.DOCKERSOCK.yaml'\n+fp_nspace_yaml_1    = 'TEST_ARTIFACTS/fp.nspace1.yaml'\n+fp_nspace_yaml_2    = 'TEST_ARTIFACTS/fp.nspace2.yaml'\n+fp_nspace_yaml_3    = 'TEST_ARTIFACTS/fp.nspace3.yaml'\n+tp_nspace_yaml      = 'TEST_ARTIFACTS/tp.default.nspace.yaml'\n+tp_nspace_default   = 'TEST_ARTIFACTS/tp.nsp.dflt.yaml'\n+fp_no_reso_yaml1    = 'TEST_ARTIFACTS/fp.no.reso1.yaml'\n+fp_no_reso_yaml2    = 'TEST_ARTIFACTS/fp.no.reso2.yaml'\n+fp_no_reso_yaml3    = 'TEST_ARTIFACTS/fp.no.reso3.yaml'\n+fp_no_reso_yaml4    = 'TEST_ARTIFACTS/fp.no.reso4.yaml'\n+fp_no_reso_yaml5    = 'TEST_ARTIFACTS/fp.no.reso5.yaml'\n+fp_no_reso_yaml6    = 'TEST_ARTIFACTS/fp.no.reso6.yaml'\n+fp_no_reso_yaml7    = 'TEST_ARTIFACTS/fp.no.reso7.yaml'\n+fp_no_reso_yaml8    = 'TEST_ARTIFACTS/fp.no.reso8.yaml'\n+fp_no_reso_yaml9    = 'TEST_ARTIFACTS/fp.no.reso9.yaml'\n+fp_no_reso_yaml10   = 'TEST_ARTIFACTS/fp.no.reso10.yaml'\n+special_secret_1    = 'TEST_ARTIFACTS/special.secret1.yaml'\n+cap_module_script   = 'TEST_ARTIFACTS/cap-module-ostk.yaml'\n+\n+_value_for_key      = '/usr/local/airflow/analytics'\n+_mount_path_kw      = 'mountPath' \n+_spec_kw            = 'spec'\n+_valid_test_yaml    = 'TEST_ARTIFACTS/nginx.deployment.result.yaml'\n+_invalid_test_yaml1 = 'TEST_ARTIFACTS/bootstrap.debian.yaml'\n+_invalid_test_yaml2 = 'TEST_ARTIFACTS/githooks.yaml'\n+_invalid_test_yaml3 = 'TEST_ARTIFACTS/start_vault_temp.yaml'\n+_invalid_test_yaml4 = 'TEST_ARTIFACTS/list.k8s.yml'\n+_invalid_test_yaml5 = 'TEST_ARTIFACTS/tasks.main.yml'\n+_invalid_test_yaml6 = 'TEST_ARTIFACTS/cluster.svc.v.yaml'\n+_invalid_test_yaml7 = 'TEST_ARTIFACTS/reso.app.yml'\n+_invalid_test_yaml8 = 'TEST_ARTIFACTS/no.kind.yaml'\n+_true_kw            = 'TRUE'\n+_false_kw           = 'FALSE'\n+_key_kw             = 'key'\n+_delay_kw           = 'initialDelaySeconds'\n+_privilege_kw       = 'allowPrivilegeEscalation'\n+_name_kw            = 'name'\n+_none_kw            = 'NONE'\n+\n+_sample_output_dir  = '/Users/arahman/K8S_REPOS/TEST_REPOS/'\n+_sample_df_field1   = 'INSECURE_HTTP'\n+df_yaml_path_field  = 'YAML_FULL_PATH'\n+_integ_fp_scrip1    = '/Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/glance-pv.yaml'\n+_integ_fp_scrip2    = '/Users/arahman/K8S_REPOS/TEST_REPOS/OpenStack-on-Kubernetes/src-newton/nfs-server.yaml'\n+_integ_fp_scrip3    = '/Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-tutorial-series-youtube/container-communication-k8s-networking/nginx-sidecar-container.yaml'\n+_integ_fp_scrip4    = '/Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/gitlab-config-storage.yml'\n+_integ_fp_scrip5    = '/Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab/postgresql-configmap.yml'\n+_integ_fp_scrip6    = '/Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/gitlab-runner/gitlab-runner-docker-deployment.yml'\n+_integ_fp_scrip7    = '/Users/arahman/K8S_REPOS/TEST_REPOS/kubernetes-gitlab-demo/load-balancer/nginx/tcp-configmap.yaml'\n+_sample_df_field2   = 'WITHIN_MANIFEST_SECRET'\n+\n+multi_doc_script1   = 'TEST_ARTIFACTS/multi.doc.yaml'\n+multi_doc_script2   = 'TEST_ARTIFACTS/empty.yml'\n+\n+\n+_common_error_string = 'SHOULD BE '\n\\ No newline at end of file"}, {"sha": "6e470df8aecb7594908003996cfd5b1ec802ec2b", "filename": "TEST_GRAPH.py", "status": "added", "additions": 193, "deletions": 0, "changes": 193, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_GRAPH.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_GRAPH.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_GRAPH.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,193 @@\n+'''\n+Akond Rahman \n+May 05, 2021\n+Testing for graph utilities \n+'''\n+\n+import unittest \n+import TEST_CONSTANTS \n+import parser\n+import graphtaint\n+import scanner\n+\n+class TestSecretGraphs( unittest.TestCase ):\n+\n+    def testHelmGraphV1(self):     \n+        oracle_value = 11\n+        scriptName   = TEST_CONSTANTS._helm_script1\n+        _, tupleList, _, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( tupleList  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHelmGraphV2(self):     \n+        oracle_value = 8\n+        scriptName   = TEST_CONSTANTS._helm_script2\n+        _, tupleList, _ , _= scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( tupleList  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHelmGraphV3(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS._helm_script3\n+        _, tupleList, _ , _= scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( tupleList  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHelmGraphV4(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS._helm_script3\n+        _, _, valid_taint_ls, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( valid_taint_ls  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHelmGraphV5(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS._helm_script4\n+        _, tupList, _, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( tupList  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHelmGraphV6(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS._helm_script4\n+        _, _, valid_taint_ls, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( valid_taint_ls  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHelmGraphV7(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._helm_script4 \n+        within_match, _, _, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual( len(within_match[0]) + len(within_match[1]) + len(within_match[2]) , oracle_value ,   TEST_CONSTANTS._common_error_string + str(oracle_value)  )                           \n+\n+    def testHelmGraphV8(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._helm_script5\n+        _, _, valid_taint_ls, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual(oracle_value, len( valid_taint_ls  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHelmGraphV9(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._helm_script5 \n+        within_match, _, _, _ = scanner.scanSingleManifest(scriptName)\n+        self.assertEqual( len(within_match[0]) + len(within_match[1]) + len(within_match[2]) , oracle_value ,   TEST_CONSTANTS._common_error_string + str(oracle_value)  )                           \n+\n+\n+class TestHTTPGraphs( unittest.TestCase ):\n+\n+    def testHTTPGraphV1(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._http_script1\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+    def testHTTPGraphV2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._http_script2\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV3(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._http_script3\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV4(self):     \n+        oracle_value = 11\n+        scriptName   = TEST_CONSTANTS._http_script4\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV5(self):     \n+        oracle_value = 11\n+        scriptName   = TEST_CONSTANTS._http_script5\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHTTPGraphV6(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._http_script6\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+    def testHTTPGraphV7(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._http_script7\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV8(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._http_script8\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV9(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._http_script9\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testHTTPGraphV10(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._http_script10 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHTTPGraphV11(self):     \n+        oracle_value = 1 \n+        scriptName   = TEST_CONSTANTS._http_script11 # this script is a multi doc \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testHTTPGraphV12(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._http_script12\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+\n+    def testHTTPGraphV13(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._http_script13 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testHTTPGraphV14(self):     \n+        '''\n+        this one tests if empty lists for a HTTP_DICT is returned. If empty list, then HTTP declared but not used \n+        '''\n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._http_script14\n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic[1]  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHTTPGraphV15(self):     \n+        '''\n+        this one tests if a list with one item for a HTTP_DICT is returned. If list with >= 1 item, then HTTP declared and used \n+        '''\n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._http_script1 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic[1]  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHTTPGraphV16(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._http_script15 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic   ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHTTPGraphV17(self):     \n+        '''\n+        this one tests if empty lists for a HTTP_DICT is returned. If empty list, then HTTP declared but not used \n+        '''\n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._http_script16 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic[1]  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHTTPGraphV18(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._http_script16 \n+        res_dic      = scanner.scanForHTTP(scriptName) \n+        self.assertEqual(oracle_value, len( res_dic  ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file"}, {"sha": "61e7f3d6eb3a84a685ddbf036e178ffb6501b1a0", "filename": "TEST_INTEGRATION.py", "status": "added", "additions": 119, "deletions": 0, "changes": 119, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_INTEGRATION.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_INTEGRATION.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_INTEGRATION.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,119 @@\n+'''\n+Akond Rahman \n+June 16, 2021 \n+Placeholder for integration test code \n+'''\n+\n+import unittest \n+import TEST_CONSTANTS \n+import main \n+import scanner \n+import pandas as pd \n+import constants \n+\n+class TestSimpleIntegration( unittest.TestCase ):\n+\n+    def testOutputColums(self):     \n+        oracle_value  = 22\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ) )\n+        _, cols       = df_all.shape \n+        self.assertEqual(oracle_value, cols ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+\n+    def testOutputRows(self):     \n+        oracle_value  = 241 \n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ) )\n+        rows, _       = df_all.shape \n+        self.assertEqual(oracle_value, rows ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+\n+    def testSampleOutput(self):     \n+        oracle_value    = 214\n+        dirName         = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls   = scanner.runScanner( dirName )\n+        df_all          = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        self.assertEqual(oracle_value, sum( df_all[TEST_CONSTANTS._sample_df_field1].tolist() ) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+\n+class TestIntegrationCount( unittest.TestCase ):\n+\n+    def testWithinSecretCount1(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip1] \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testWithinSecretCount2(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip2] \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testWithinSecretCount3(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip3]  \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+\n+    def testWithinSecretCount4(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip4]   \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testWithinSecretCount5(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip5]   \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testWithinSecretCount6(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip6]   \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testWithinSecretCount7(self):     \n+        oracle_value  = 0\n+        dirName       = TEST_CONSTANTS._sample_output_dir \n+        content_as_ls = scanner.runScanner( dirName )\n+        df_all        = pd.DataFrame(  main.getCountFromAnalysis( content_as_ls ),  columns=constants.CSV_HEADER )\n+        script_df     = df_all[df_all[TEST_CONSTANTS.df_yaml_path_field]== TEST_CONSTANTS._integ_fp_scrip7]    \n+        within_sec_cnt= script_df[TEST_CONSTANTS._sample_df_field2].tolist()[0]\n+\n+        self.assertEqual(oracle_value, within_sec_cnt ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file"}, {"sha": "5f311f17839a59c26a87c1e2ef47f615cfe85ab9", "filename": "TEST_PARSING.py", "status": "added", "additions": 157, "deletions": 0, "changes": 157, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_PARSING.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_PARSING.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_PARSING.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,157 @@\n+'''\n+Akond Rahman \n+April 30, 2022 \n+Test Utilities for Parsing \n+'''\n+\n+import unittest \n+import TEST_CONSTANTS \n+import parser\n+\n+class TestParsing( unittest.TestCase ):\n+\n+    def testKeyExtraction(self):     \n+        oracle_value = 4 \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        self.assertEqual(oracle_value, len(yaml_as_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+                    \n+    def testKeyPathLength(self):     \n+        oracle_value = 9 \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis      = parser.keyMiner( yaml_as_dict, TEST_CONSTANTS._value_for_key )\n+        self.assertEqual(oracle_value, len(key_lis) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testKeyPath1(self):     \n+        oracle_value = TEST_CONSTANTS._spec_kw \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis      = parser.keyMiner( yaml_as_dict, TEST_CONSTANTS._value_for_key )\n+        self.assertEqual(oracle_value, key_lis[0] ,  TEST_CONSTANTS._common_error_string + oracle_value  )   \n+\n+    def testKeyPath2(self):     \n+        oracle_value = TEST_CONSTANTS._mount_path_kw \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis      = parser.keyMiner( yaml_as_dict, TEST_CONSTANTS._value_for_key )\n+        self.assertEqual(oracle_value, key_lis[-2] ,  TEST_CONSTANTS._common_error_string + oracle_value  )         \n+\n+    def testKeyCount(self):     \n+        oracle_value = 226 \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis      = [] \n+        parser.getKeyRecursively  ( yaml_as_dict, key_lis )\n+        self.assertEqual(oracle_value, len(key_lis) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )                     \n+\n+    def testValueCount(self):     \n+        oracle_value = 151 \n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis      = list( parser.getValuesRecursively  ( yaml_as_dict ) )\n+        self.assertEqual(oracle_value, len(key_lis) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )    \n+\n+    def testK8SYAMLValidity1(self):     \n+        self.assertTrue( parser.checkIfValidK8SYaml(TEST_CONSTANTS._valid_test_yaml) , TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._true_kw )\n+\n+    def testK8SYAMLValidity2(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml1), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+    \n+    def testK8SYAMLValidity3(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml2), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testK8SYAMLValidity4(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml3), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testK8SYAMLValidity5(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml4), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testK8SYAMLValidity6(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml5 ), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testK8SYAMLValidity7(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml6 ), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+    \n+    def testK8SYAMLValidity8(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml7 ), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testK8SYAMLValidity9(self):     \n+        self.assertFalse( parser.checkIfValidK8SYaml(TEST_CONSTANTS._invalid_test_yaml8 ), TEST_CONSTANTS._common_error_string + TEST_CONSTANTS._false_kw )\n+\n+    def testValueFromKey1(self):     \n+        oracle_value = 53\n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_list    = []\n+        parser.getValsFromKey(yaml_as_dict, TEST_CONSTANTS._name_kw , temp_list  )\n+        self.assertEqual(oracle_value, len(temp_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )    \n+\n+    def testValueFromKey2(self):     \n+        oracle_value = 15\n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_list    = []\n+        parser.getValsFromKey(yaml_as_dict, TEST_CONSTANTS._mount_path_kw , temp_list  )\n+        self.assertEqual(oracle_value, len(temp_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )    \n+\n+    def testValueFromKey3(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_list    = []\n+        parser.getValsFromKey(yaml_as_dict, TEST_CONSTANTS._privilege_kw  , temp_list  )\n+        self.assertEqual(oracle_value, len(temp_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+              \n+    def testValueFromKey4(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_list    = []\n+        parser.getValsFromKey(yaml_as_dict, TEST_CONSTANTS._delay_kw , temp_list  )\n+        self.assertEqual(oracle_value, len(temp_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testValueFromKey5(self):     \n+        oracle_value = 10\n+        scriptName   = TEST_CONSTANTS._test_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_list    = []\n+        parser.getValsFromKey(yaml_as_dict, TEST_CONSTANTS._key_kw , temp_list  )\n+        self.assertEqual(oracle_value, len(temp_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+\n+class TestParseMultidocs( unittest.TestCase ):\n+\n+    def testMultidocV1(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS.multi_doc_script1\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        self.assertEqual(oracle_value, len(dict_as_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testMultidocV2(self):     \n+        oracle_value = 6\n+        scriptName   = TEST_CONSTANTS.multi_doc_script2\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        self.assertEqual(oracle_value, len(dict_as_list) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testMultidoc2SingleDic(self):     \n+        oracle_value = 24\n+        scriptName   = TEST_CONSTANTS.multi_doc_script2\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        single_dict  = parser.getSingleDict4MultiDocs( dict_as_list )\n+        # print(single_dict.keys())\n+        self.assertEqual(oracle_value, len(list(  single_dict.keys()) ),  TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+if __name__ == '__main__':\n+    unittest.main()"}, {"sha": "496d0a7ce503858e7540131f8a235927141c6386", "filename": "TEST_SCANNING.py", "status": "added", "additions": 770, "deletions": 0, "changes": 770, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_SCANNING.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/TEST_SCANNING.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/TEST_SCANNING.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,770 @@\n+import unittest \n+import TEST_CONSTANTS \n+import parser\n+import scanner \n+import constants \n+\n+class TestScanning( unittest.TestCase ):\n+\n+    def testSecret1(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._secret_yaml1\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret2(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._secret_yaml2\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret3(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._secret_yaml3\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret4(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._secret_yaml4\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret5(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._secret_yaml5\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret6(self):     \n+        oracle_value = 0 \n+        scriptName   = TEST_CONSTANTS._secret_yaml6\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )      \n+\n+    def testSecret7(self):     \n+        oracle_value = 5\n+        scriptName   = TEST_CONSTANTS._secret_yaml7\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        holder       = [] \n+        for _, v_ in secret_dict.items():\n+            holder = holder + v_[0] + v_[1]\n+        self.assertEqual(oracle_value, len(holder) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret8(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._secret_yaml8\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        holder       = [] \n+        for _, v_ in secret_dict.items():\n+            holder = holder + v_[0] + v_[1]\n+        self.assertEqual(oracle_value, len(holder) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret9(self):     \n+        oracle_value = 0 \n+        scriptName   = TEST_CONSTANTS._secret_yaml9\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+\n+    def testSecret10(self):     \n+        oracle_value = 0 \n+        scriptName   = TEST_CONSTANTS._secret_yaml10\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+\n+    def testSecret11(self):     \n+        oracle_value = 2 \n+        scriptName   = TEST_CONSTANTS._cert_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+\n+    def testSecret12(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._rsa_key_yaml\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        secret_dict  = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual(oracle_value, len(secret_dict) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+    def testSecret13(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS.special_secret_1\n+        within, _, _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual(oracle_value, len(within[0]) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+\n+    def testSecret14(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS.special_secret_1\n+        within, _, _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual(oracle_value, len(within[1]) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )               \n+\n+    def testSecret15(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.special_secret_1\n+        within, _, _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual(oracle_value, len(within[2]) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )              \n+\n+    def testSecret16(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS.special_secret_1\n+        within, _, _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual(oracle_value, len(within[0])  + len(within[1]) + len(within[2]) ,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )    \n+\n+\n+class TestFalsePositives( unittest.TestCase ):\n+\n+    def testSecret1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script1\n+        within_match_, _, _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( len(within_match_[0]) + len(within_match_[1]) + len(within_match_[2]), 0,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script1\n+        _, templ_match , _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(templ_match) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret3(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._fp_script2\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret4(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script2\n+        _, templ_match , _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(templ_match) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret5(self):     \n+        oracle_value = 3\n+        scriptName   = TEST_CONSTANTS._fp_script3\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret6(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script3\n+        _, _ , taint_map_ls , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(taint_map_ls) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret7(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._fp_script4\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret8(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script4\n+        _, templ_matches , _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(templ_matches) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret9(self):     \n+        oracle_value = 4\n+        scriptName   = TEST_CONSTANTS._fp_script5\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret10(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script5\n+        _, _ , taint_map_ls , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(taint_map_ls) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret11(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._fp_script6\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret12(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script6\n+        _, template_matches , _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(template_matches) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret13(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script7 \n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )          \n+\n+    def testSecret14(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script8 \n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )          \n+\n+    def testSecret15(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._fp_script9\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret16(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script9\n+        _, template_matches , _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(template_matches) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret17(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._fp_script10\n+        dict_as_list = parser.loadMultiYAML( scriptName )\n+        yaml_as_dict = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        initial_secret_dict = scanner.scanForSecrets( yaml_as_dict )\n+        self.assertEqual( oracle_value,  len(initial_secret_dict) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret18(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script10 \n+        _, _ , taint_map_lis , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(taint_map_lis) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret19(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script10 \n+        _, templ_matches , _ , _ = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len(templ_matches) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testSecret20(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._fp_script10 \n+        within_match_ , _ , _, _  = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( len(within_match_[0]) + len(within_match_[1]) + len(within_match_[2]), 0,  TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+\n+class TestOverPrivilegedContainers( unittest.TestCase ):\n+\n+    def testPrivilege1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._privi_scrip1 \n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPrivilege2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._privi_scrip2\n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPrivilege3(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._privi_scrip3\n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPrivilege4(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._privi_scrip4\n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPrivilege5(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._privi_scrip5\n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPrivilegeForMultiDoc(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.multi_doc_script1\n+        _, _, _, privi_dict = scanner.scanSingleManifest( scriptName )\n+        # print( privi_dict )\n+        self.assertEqual( oracle_value,  len( privi_dict ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )          \n+\n+\n+class TestMissingSecuContext( unittest.TestCase ):\n+\n+    def testMissing1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._no_secu_cont_yaml1\n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testMissing2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_secucont_no_yaml \n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testAbsence1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._no_secu_cont_yaml2\n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testAbsence2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.secu_cont_fp_yaml \n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPresent3(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_secucont_no_yaml \n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testPresent4(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._no_secu_cont_yaml1\n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )   \n+\n+    def testMissingSecuForMultiDoc(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.multi_doc_script2\n+        res_dic = scanner.scanForMissingSecurityContext( scriptName )\n+        # print(res_dic)\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )                                \n+\n+\n+class TestDefaultNamespace( unittest.TestCase ):\n+\n+    def testPresent1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml1 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPresent2(self):     \n+        oracle_value = constants.DEPLOYMENT_KW \n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml1 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,   res_dic[1][0] ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPresent3(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml2\n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testPresent4(self):     \n+        oracle_value = constants.POD_KW  \n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml2 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,   res_dic[1][0] ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testMissing1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml3\n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testMissing2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml3\n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic[1] ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+\n+    def testTaintPresence1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml4 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testTaintPresence2(self):     \n+        oracle_value = 2\n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml4 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,  len( res_dic[1][0] ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testTaintPresence3(self):     \n+        oracle_value = TEST_CONSTANTS.test_sink_nspace_yam \n+        scriptName   = TEST_CONSTANTS._dflt_nspace_yaml4 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName )\n+        self.assertEqual( oracle_value,   res_dic[1][0][0] ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+    def testNamespaceAbsence1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_nspace_yaml_1 \n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )          \n+\n+    def testNamespaceAbsence2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_nspace_yaml_2\n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )                  \n+\n+    def testNamespaceAbsence3(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_nspace_yaml_3\n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )    \n+\n+    def testPresent5(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_nspace_yaml\n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )           \n+\n+    def testPresent6(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_nspace_default\n+        res_dic = scanner.scanForDefaultNamespace( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )  \n+\n+        \n+\n+class TestResourceLimits( unittest.TestCase ):\n+\n+    def testAbsent1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.reso_yaml1\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.reso_yaml3 \n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent3(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.reso_yaml4 \n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testPresent1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_allow_privilege\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent4(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml1 \n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent5(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml2\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent6(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml3\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )             \n+    def testAbsent7(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml4\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent8(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml5\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent9(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml6\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )             \n+\n+    def testAbsent10(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml7\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  )             \n+    def testAbsent11(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml8\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent12(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml9\n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent13(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_no_reso_yaml10 \n+        res_dic = scanner.scanForResourceLimits( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+\n+class TestMissingRollingUpdate( unittest.TestCase ):\n+\n+    def testAbsent1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml1 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml2 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent3(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml3 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent4(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml4\n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent5(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml4\n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+\n+    def testAbsent6(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml5 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+\n+    def testAbsent7(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml6 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAbsent8(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml6 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testPresent1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml7  \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testPresent2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml7 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic[1] ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testPresent3(self):     \n+        oracle_value = constants.DEPLOYMENT_KW \n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml7 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  res_dic[1][0]  ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testPresent4(self):     \n+        oracle_value = constants.DEPLOYMENT_KW \n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml8 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  res_dic[1][0]  ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testPresent5(self):     \n+        oracle_value = constants.DEPLOYMENT_KW \n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml9 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  res_dic[1][0]  ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testPresent6(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml9 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic[1] ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testPresent7(self):     \n+        oracle_value = constants.DEPLOYMENT_KW \n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml10 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  res_dic[1][0]  ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testPresent8(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml10 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testPresent9(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml11 \n+        res_dic = scanner.scanForRollingUpdates( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+class TestMissingNetPolicy( unittest.TestCase ):\n+\n+    def testAbsent1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic = scanner.scanForMissingNetworkPolicy( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+class TestHostIssues( unittest.TestCase ):\n+\n+    def testHostPIDPresenceV1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_host_ipc_yaml \n+        res_dic      = scanner.scanForTruePID( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostPIDPresenceV2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_host_net_yaml\n+        res_dic      = scanner.scanForTruePID( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostPIDAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanForTruePID( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testDockerSockPresence(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_docker_sock_yaml\n+        res_dic      = scanner.scanDockerSock( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testDockerSockAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanDockerSock( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostNetworkPresenceV1(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_host_net_yaml\n+        res_dic      = scanner.scanForHostNetwork( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostNetworkAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanForHostNetwork( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testCAPSYSPresence(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_cap_sys_yaml\n+        res_dic      = scanner.scanForCAPSYS( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testCAPSYSAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanForCAPSYS( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+    def testHostAliasPresence(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_cap_sys_yaml\n+        res_dic      = scanner.scanForHostAliases( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostAliasAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanForHostAliases( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testHostNetworkPresenceV2(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_host_net_yaml2 \n+        res_dic      = scanner.scanForHostNetwork( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAllowPrivilegePresence(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_allow_privilege \n+        res_dic      = scanner.scanForAllowPrivileges( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAllowPrivilegeAbsence(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.net_policy_yaml \n+        res_dic      = scanner.scanForAllowPrivileges( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testAnotherDockerSock(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.another_dockersock \n+        res_dic      = scanner.scanDockerSock( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testCAPSYSMODULE(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.cap_module_script\n+        res_dic      = scanner.scanForCAPMODULE( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+\n+class TestSecCompIssues( unittest.TestCase ):\n+\n+    def testUnconfinedSecComp(self):     \n+        oracle_value = 1\n+        scriptName   = TEST_CONSTANTS.tp_seccomp_unconf\n+        res_dic      = scanner.scanForUnconfinedSeccomp( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testNeutralSecComp1(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_seccomp_unconf\n+        res_dic      = scanner.scanForUnconfinedSeccomp( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testNeutralSecComp2(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.fp_rolling_yaml7\n+        res_dic      = scanner.scanForUnconfinedSeccomp( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+    def testNeutralSecComp3(self):     \n+        oracle_value = 0\n+        scriptName   = TEST_CONSTANTS.tp_host_net_yaml\n+        res_dic      = scanner.scanForUnconfinedSeccomp( scriptName ) \n+        self.assertEqual( oracle_value,  len( res_dic ) ,    TEST_CONSTANTS._common_error_string + str(oracle_value)  ) \n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file"}, {"sha": "687dd9df761cdc1835cf9c0d0b4c1110b443623c", "filename": "__pycache__/constants.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fconstants.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fconstants.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fconstants.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "101e5fe7e460c74521ae20e3c7d0739a03fa38e3", "filename": "__pycache__/graphtaint.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fgraphtaint.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fgraphtaint.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fgraphtaint.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "db96e71301eb57ec0a8ce27880447a88bb8c0f1f", "filename": "__pycache__/main.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fmain.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fmain.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fmain.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "7dafa66c41f7a5f6e1c70eeb6ffbab3c58b05c4b", "filename": "__pycache__/parser.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fparser.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fparser.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fparser.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "6fe51c1e04d2166b153fa94db571a61bf9088e1b", "filename": "__pycache__/scanner.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fscanner.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fscanner.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fscanner.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "b37d60c8b6abe30711c89027c8be6db9cf78aea6", "filename": "__pycache__/vault4paper_antidot.cpython-312.pyc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fvault4paper_antidot.cpython-312.pyc", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/__pycache__%2Fvault4paper_antidot.cpython-312.pyc", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/__pycache__%2Fvault4paper_antidot.cpython-312.pyc?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "2f58ed53965818488552eaadad1a87a894701dab", "filename": "bandit_report.csv", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/bandit_report.csv", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/bandit_report.csv", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/bandit_report.csv?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,18 @@\n+filename,test_name,test_id,issue_severity,issue_confidence,issue_cwe,issue_text,line_number,col_offset,end_col_offset,line_range,more_info\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/helm.values.yaml',8,22,55,[8],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/tango.values.yaml',9,22,56,[9],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/charts.values.yaml',10,22,57,[10],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/skampi.values.yaml',11,22,57,[11],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/minecraft.values.yaml',12,22,60,[12],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/kubecf.values.yaml',13,22,57,[13],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/nextcloud.values.yaml',14,22,60,[14],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/keycloak.values.yaml',15,22,59,[15],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/empty.yml',16,22,48,[16],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/kubecf.values.yaml',17,22,57,[17],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./TEST_CONSTANTS.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'TEST_ARTIFACTS/special.secret1.yaml',106,22,59,[106],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./constants.py,hardcoded_password_string,B105,LOW,MEDIUM,https://cwe.mitre.org/data/definitions/259.html,Possible hardcoded password: 'Secret',81,31,39,[81],https://bandit.readthedocs.io/en/1.8.2/plugins/b105_hardcoded_password_string.html\r\n+./parser.py,blacklist,B404,LOW,HIGH,https://cwe.mitre.org/data/definitions/78.html,Consider possible security implications associated with the subprocess module.,15,0,17,[15],https://bandit.readthedocs.io/en/1.8.2/blacklists/blacklist_imports.html#b404-import-subprocess\r\n+./parser.py,start_process_with_partial_path,B607,LOW,HIGH,https://cwe.mitre.org/data/definitions/78.html,Starting a process with a partial executable path,340,25,106,[340],https://bandit.readthedocs.io/en/1.8.2/plugins/b607_start_process_with_partial_path.html\r\n+./parser.py,subprocess_without_shell_equals_true,B603,LOW,HIGH,https://cwe.mitre.org/data/definitions/78.html,subprocess call - check for execution of untrusted input.,340,25,106,[340],https://bandit.readthedocs.io/en/1.8.2/plugins/b603_subprocess_without_shell_equals_true.html\r\n+./parser.py,start_process_with_partial_path,B607,LOW,HIGH,https://cwe.mitre.org/data/definitions/78.html,Starting a process with a partial executable path,355,21,102,[355],https://bandit.readthedocs.io/en/1.8.2/plugins/b607_start_process_with_partial_path.html\r\n+./parser.py,subprocess_without_shell_equals_true,B603,LOW,HIGH,https://cwe.mitre.org/data/definitions/78.html,subprocess call - check for execution of untrusted input.,355,21,102,[355],https://bandit.readthedocs.io/en/1.8.2/plugins/b603_subprocess_without_shell_equals_true.html\r"}, {"sha": "62a4566a2a32bac5f2d53a14bc5fc04c5a3170fc", "filename": "constants.py", "status": "added", "additions": 93, "deletions": 0, "changes": 93, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/constants.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/constants.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/constants.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,93 @@\n+'''\n+Zakariya Veasy\n+April 23, 2025\n+Placeholder for constants for KubeTaint\n+'''\n+\n+FILE_READ_FLAG               = 'r'\n+TEMP_KEY_NAME                = 'TEMP_KEY_DUMMY'\n+API_V_KEYNAME                = 'apiVersion'\n+KIND_KEY_NAME                = 'kind'\n+QUOTE_SYMBOL                 = \"'\"\n+K8S_FORBIDDEN_KW_LIST        = ['CustomResourceDefinition', 'OpenDataHub', 'List', 'ClusterServiceVersion', 'ClusterIssuer']\n+HELM_KW                      = 'helm'\n+CHART_KW                     = 'chart'\n+VALUE_KW                     = 'values'\n+SERVICE_KW                   = 'services'\n+INGRESS_KW                   = 'k8s-ingress'\n+HELM_DEPLOY_KW               = 'deploy' \n+YAML_SKIPPING_TEXT           = 'skipping'\n+YAML_EXTENSION               = 'yaml'\n+YML_EXTENSION                = 'yml'\n+TEMPLATES_DIR_KW             = '/templates/'\n+DOT_SYMBOL                   = '.'\n+HELM_VALUE_KW                = 'Values' \n+VALU_FROM_KW                 = 'valueFrom'\n+PRIVI_KW                     = 'privileged'\n+SECU_CONT_KW                 = 'securityContext'\n+CONTAINER_KW                 = 'containers'\n+SPEC_KW                      = 'spec'\n+DEAMON_KW                    = 'DaemonSet' \n+HTTP_KW                      = 'http://'\n+CONFIGMAP_KW                 = 'ConfigMap'\n+SLASH_SYMBOL                 = '/'\n+SH_EXTENSION                 = 'sh'\n+WWW_KW                       = 'www'\n+ORG_KW                       = 'org'\n+CONFIG_KW                    = 'configuration'\n+DEPLOYMENT_KW                = 'Deployment' \n+POD_KW                       = 'Pod'\n+DEFAULT_KW                   = 'default' \n+K8S_SERVICE_KW               = 'Service'\n+K8S_APP_KW                   = 'app'\n+LIMITS_KW                    = 'limits'\n+CPU_KW                       = 'cpu'\n+MEMORY_KW                    = 'memory' \n+STRATEGY_KW                  = 'strategy'\n+ROLLING_UPDATE_KW            = 'rollingUpdate'\n+VAL_ROLLING_UPDATE_KW        = 'RollingUpdate'\n+NET_POLICY_KW                = 'NetworkPolicy'\n+POD_SELECTOR_KW              = 'podSelector'\n+MATCH_LABEL_KW               = 'matchLabels'\n+ANLYZING_KW                  = 'Analyzing... '\n+SIMPLE_DASH_CHAR             = '-----'\n+WEIRD_PATHS                  = ['github/workflows/', '.github/', '.travis.yml']\n+COUNT_PRINT_KW               = ' COUNT: ' \n+HOST_PID_KW                  = 'hostPID'\n+HOST_IPC_KW                  = 'hostIPC'\n+HOST_NET_KW                  = 'hostNetwork'\n+TRUE_LOWER_KW                = 'true' \n+VOLUMES_KW                   = 'volumes'\n+HOST_PATH_KW                 = 'hostPath'\n+NAME_FIELD_KW                = 'name'\n+PATH_KW                      = 'path'\n+DOCKERSOCK_KW_LIST           = [ VOLUMES_KW, HOST_PATH_KW, NAME_FIELD_KW, PATH_KW  ] ## based on this: https://github.com/controlplaneio/kubesec/blob/master/pkg/ruler/ruleset.go, should be under `volumes` and not `volumeMount`\n+DOCKERSOCK_PATH_KW           = '/var/run/docker.sock'\n+DOCKERSOCK_STRING            = 'dockersock'\n+CAPABILITIES_STRING          = 'capabilities'\n+ADD_KW                       = 'add'\n+CAPSYS_KW_LIST               = [CONTAINER_KW, SECU_CONT_KW, CAPABILITIES_STRING] \n+CAPSYS_ADMIN_STRING          = 'CAP_SYS_ADMIN'\n+CAPSYS_MODULE_STRING         = 'CAP_SYS_MODULE'\n+HOST_ALIAS_KW                = 'hostAliases'\n+ALLOW_PRIVILEGE_KW           = 'allowPrivilegeEscalation'\n+ALLOW_PRIVI_KW_LIST          = [ALLOW_PRIVILEGE_KW, SECU_CONT_KW, CONTAINER_KW, SPEC_KW]\n+SECCOMP_PROFILE_KW           = 'seccompProfile'\n+SECCOMP_KW_LIST              = [ SPEC_KW, SECU_CONT_KW, SECCOMP_PROFILE_KW ]\n+UNCONFIED_KW                 = 'Unconfined'\n+TYPE_KW                      = 'type'\n+YAML_DOC_KW                  = 'YAML.DOC.'\n+NAMESPACE_KW                 = 'namespace'\n+SECRET_KW                    = 'Secret'\n+\n+SECRET_USER_LIST             = ['user']\n+SECRET_PASSWORD_LIST         = ['pwd', 'password', 'passwd', 'admin_pass'] \n+FORBIDDEN_USER_NAMES         = ['domain', 'group', 'mode', 'schema', 'email', '_tenant', '_tree_dn', '_attribute', '_emulation', '_allow_', '_emulation', '%(', '_age'] \n+FORBIDDEN_PASS_NAMES         = ['_auth', '_file', '_path', '_age', '_content', '_hash'] \n+INVALID_SECRET_CONFIG_VALUES = [ ':undef', '[]', '/',  'hiera', 'unset', 'undefined', '%(' ]  \n+LEGIT_KEY_NAMES              = ['crt', 'key']\n+VALID_KEY_STRING             = ['-----BEGIN CERTIFICATE-----', '-----BEGIN RSA PRIVATE KEY-----']\n+\n+\n+CSV_HEADER                   = ['DIR', 'YAML_FULL_PATH', 'WITHIN_MANIFEST_SECRET', 'VALID_TAINT_SECRET', 'SEC_CONT_OVER_PRIVIL', 'INSECURE_HTTP', 'NO_SECU_CONTEXT', 'NO_DEFAULT_NSPACE', 'NO_RESO', 'NO_ROLLING_UPDATE', 'NO_NETWORK_POLICY', 'TRUE_HOST_PID', 'TRUE_HOST_IPC', 'DOCKERSOCK_PATH', 'TRUE_HOST_NET', 'CAP_SYS_ADMIN', 'HOST_ALIAS', 'ALLOW_PRIVI', 'SECCOMP_UNCONFINED', 'CAP_SYS_MODULE', 'K8S_STATUS', 'HELM_STATUS']\n+CSV_ENCODING                 = 'latin-1'"}, {"sha": "bbcf36ddbe2273af11f0c46d7cd3e75d6e98ff48", "filename": "environment.yml", "status": "added", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/environment.yml", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/environment.yml", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/environment.yml?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,10 @@\n+name: KUBESEC\r\n+channels:\r\n+  - defaults\r\n+dependencies:\r\n+  - python=3.11\r\n+  - pandas=2.0.2\r\n+  - pyyaml=6.0\r\n+  - pytest=7.3.1\r\n+  - requests=2.29.0\r\n+  - typer=0.9.0\n\\ No newline at end of file"}, {"sha": "df2c69b620e4f178d8ec9f97caf4931e28fa987a", "filename": "fuzz.py", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/fuzz.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/fuzz.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/fuzz.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,52 @@\n+from hypothesis import given, strategies as st\n+import parser\n+import scanner\n+import graphtaint\n+import main\n+import logging\n+\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(\"fuzz_logger\")\n+\n+@given(st.text())\n+def test_loadMultiYAML(input_str):\n+    try:\n+        with open(\"temp.yaml\", \"w\") as f:\n+            f.write(input_str)\n+        result = parser.loadMultiYAML(\"temp.yaml\")\n+        logger.info(f\"loadMultiYAML returned {len(result)} dicts.\")\n+    except Exception as e:\n+        logger.error(f\"Error in loadMultiYAML: {e}\")\n+\n+@given(st.dictionaries(st.text(), st.text(), max_size=5), st.text())\n+def test_getValsFromKey(dictionary, target):\n+    try:\n+        list_holder = []\n+        parser.getValsFromKey(dictionary, target, list_holder)\n+        logger.info(f\"Found {len(list_holder)} values for key {target}.\")\n+    except Exception as e:\n+        logger.error(f\"Error in getValsFromKey: {e}\")\n+\n+@given(st.dictionaries(st.text(), st.text(), max_size=5))\n+def test_scanForSecrets(dictionary):\n+    try:\n+        result = scanner.scanForSecrets(dictionary)\n+        logger.info(f\"Secrets found in {len(result)} keys.\")\n+    except Exception as e:\n+        logger.error(f\"Error in scanForSecrets: {e}\")\n+\n+@given(st.lists(st.tuples(st.text(), st.text(), st.lists(st.text(), min_size=0), st.lists(st.text(), min_size=0), st.lists(st.text(), min_size=0), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.dictionaries(st.text(), st.text(), max_size=3), st.booleans(), st.booleans())))\n+def test_getCountFromAnalysis(input_data):\n+    try:\n+        result = main.getCountFromAnalysis(input_data)\n+        logger.info(f\"Analysis returned {len(result)} entries.\")\n+    except Exception as e:\n+        logger.error(f\"Error in getCountFromAnalysis: {e}\")\n+\n+@given(st.text(), st.dictionaries(st.text(), st.text(), max_size=5))\n+def test_mineSecretGraph(path, dictionary):\n+    try:\n+        result = graphtaint.mineSecretGraph(path, dictionary, {\"test\": [(\"secret\",)]})\n+        logger.info(f\"mineSecretGraph result: {result}\")\n+    except Exception as e:\n+        logger.error(f\"Error in mineSecretGraph: {e}\")"}, {"sha": "204add3aefc8a20072b73c2448fdb4b4c96ba4cd", "filename": "graphtaint.py", "status": "added", "additions": 216, "deletions": 0, "changes": 216, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/graphtaint.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/graphtaint.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/graphtaint.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,216 @@\n+'''\n+Zakariya Veasy\n+April 23, 2025 \n+Construct taint graphs based on weakness types \n+'''\n+import constants\n+import parser \n+import os \n+from itertools import combinations\n+import logging\n+\n+\n+logger = logging.getLogger(__name__)\n+logging.basicConfig(level=logging.INFO)\n+\n+\n+def getYAMLFiles(path_to_dir):\n+    valid_  = [] \n+    for root_, dirs, files_ in os.walk( path_to_dir ):\n+       for file_ in files_:\n+           full_p_file = os.path.join(root_, file_)\n+           if(os.path.exists(full_p_file)):\n+             if (full_p_file.endswith( constants.YAML_EXTENSION  ) or full_p_file.endswith( constants.YML_EXTENSION  )  ):\n+               valid_.append(full_p_file)\n+    return valid_ \n+\n+def constructHelmString(hiera_tuple): \n+    str2ret  = constants.YAML_SKIPPING_TEXT \n+    upper_key, key, _ = hiera_tuple \n+    if ( upper_key != key  ):\n+        str2ret =   constants.DOT_SYMBOL +  constants.HELM_VALUE_KW + constants.DOT_SYMBOL + upper_key + constants.DOT_SYMBOL + key \n+    return str2ret \n+\n+def getHelmTemplateContent( templ_dir ):\n+    template_content_dict = {}\n+    template_yaml_files =  getYAMLFiles( templ_dir )\n+    for template_yaml_file in template_yaml_files:\n+        value_as_str      = parser.readYAMLAsStr( template_yaml_file )\n+        template_content_dict[template_yaml_file] = value_as_str\n+    return template_content_dict \n+\n+\n+def getMatchingTemplates(path2script, hierarchy_ls):\n+    templ_list = [] \n+    template_content_dict, helm_string_list = {}, []\n+    templateDirOfHelmValues = os.path.dirname( path2script )  + constants.TEMPLATES_DIR_KW \n+    if (os.path.exists( templateDirOfHelmValues )  ):\n+        template_content_dict = getHelmTemplateContent( templateDirOfHelmValues )\n+    for hiera_ in hierarchy_ls:\n+        helm_string_list.append(   constructHelmString( hiera_  ) )\n+    for template_file, template_string in template_content_dict.items():\n+        for helm_string in helm_string_list:\n+            if helm_string != constants.YAML_SKIPPING_TEXT : \n+                if helm_string in template_string: \n+                    match_count = template_string.count( helm_string  )\n+                    for _ in range(match_count):\n+                        templ_list.append( (template_file, helm_string ) )\n+    return templ_list\n+\n+def getValidTaints(  lis_template_matches ): \n+    '''\n+    provides a mapping between the key where the secret occurred and the \n+    files that re affected by teh key \n+    '''\n+    taint_lis  = []\n+    for match in lis_template_matches:\n+        script_name, helm_string = match \n+        inceptor = helm_string.split( constants.DOT_SYMBOL )[-1]\n+        taint_lis.append( (inceptor, script_name) )\n+    return taint_lis \n+\n+\n+\n+def mineSecretGraph( path2script, yaml_dict , secret_dict ):\n+    logger.info(f\"Building taint graph for {path2script}\")\n+    '''\n+    This method looks at YAML files in Helm templates. \n+    Works only for secrets. \n+    Need to provide script path, script dict, dictionary of secrets that appear for the script  \n+    '''\n+\n+    within_match_head = None \n+    hierarchy_list = []\n+    for k_, v_ in secret_dict.items():\n+        for tup_item in v_:\n+            for value in tup_item:\n+                hierarchy_keys = parser.keyMiner(yaml_dict, value)\n+                hierarchy_keys = [x_ for x_ in hierarchy_keys if x_ != constants.YAML_SKIPPING_TEXT ] \n+                compo_hiera_keys = [ constants.DOT_SYMBOL.join(str_) for str_ in combinations( hierarchy_keys , 2 )] ## take 2 strings at a time \n+                # print(compo_hiera_keys) \n+                for h_key in hierarchy_keys:\n+                    hierarchy_list.append( (h_key, k_ , v_) )\n+                '''\n+                the purpose of composite hierarchy keys is to get nested values that are referenced \n+                taking 2 strings at a time \n+                '''\n+                for compo_h_key in compo_hiera_keys:\n+                    hierarchy_list.append( ( compo_h_key, k_, v_  ) )\n+    \n+    templ_match_list = []\n+    if( parser.checkIfValidHelm(path2script) ):                    \n+        templ_match_list = getMatchingTemplates( path2script, hierarchy_list  )\n+    else:\n+        if( len(hierarchy_list) > 0 ):\n+            '''\n+            check if valueFrom exists \n+            '''\n+            if constants.VALU_FROM_KW not in hierarchy_list: \n+                    within_match_head = hierarchy_list[0]\n+    valid_taints = getValidTaints( templ_match_list ) \n+    # print( within_match_head ) \n+    return within_match_head, templ_match_list, valid_taints \n+\n+\n+def getSHFiles(path_to_dir):\n+    valid_  = [] \n+    for root_, _, files_ in os.walk( path_to_dir ):\n+       for file_ in files_:\n+           full_p_file = os.path.join(root_, file_)\n+           if(os.path.exists(full_p_file)):\n+             if (full_p_file.endswith( constants.SH_EXTENSION  )  ):\n+               valid_.append(full_p_file)\n+    return valid_ \n+\n+\n+def readBashAsStr( path_sh_script ):\n+    _as_str = constants.YAML_SKIPPING_TEXT\n+    with open( path_sh_script , constants.FILE_READ_FLAG) as file_:\n+        _as_str = file_.read()\n+    return _as_str\n+\n+def getTaintsFromConfigMaps( script_path ):\n+    list2Return = [] \n+    config_map_dir  = os.path.dirname( script_path )  + constants.SLASH_SYMBOL    \n+    script_name     = script_path.replace( config_map_dir, constants.YAML_SKIPPING_TEXT )\n+    sh_files = getSHFiles( config_map_dir )\n+    for sh_file in sh_files:\n+        sh_content = readBashAsStr( sh_file )\n+        if script_name in sh_content:\n+            sh_match_cnt  = sh_content.count( script_name )\n+            for l_ in range( sh_match_cnt ):\n+                list2Return.append(  sh_file  )\n+    return list2Return\n+    \n+\n+\n+def mineViolationGraph(path2script, yaml_dict, taint_value, k_ ):\n+    '''\n+    This method looks at YAML files in Helm templates. \n+    Works for all types. \n+    Need to provide script path, script dict, value identified as smell, key for which value occurs \n+    '''\n+    hierarchy_list = [] \n+    hierarchy_keys = parser.keyMiner(yaml_dict, taint_value)\n+    hierarchy_keys = [x_ for x_ in hierarchy_keys if x_ != constants.YAML_SKIPPING_TEXT ] \n+    compo_hiera_keys = [ constants.DOT_SYMBOL.join(str_) for str_ in combinations( hierarchy_keys , 2 )] ## take 2 strings at a time \n+    # print(compo_hiera_keys) \n+    for h_key in hierarchy_keys:\n+        hierarchy_list.append( (h_key, k_ , taint_value) )\n+    '''\n+    the purpose of composite hierarchy keys is to get nested values that are referenced \n+    taking 2 strings at a time \n+    '''\n+    for compo_h_key in compo_hiera_keys:\n+        hierarchy_list.append( ( compo_h_key, k_, taint_value  ) )\n+    \n+    templ_match_list = []\n+    templ_match_list = getMatchingTemplates( path2script, hierarchy_list  )    \n+\n+    return templ_match_list\n+\n+def mineServiceGraph( script_path, dict_yaml, src_val ): \n+    '''\n+    This method looks at YAML files that have kind:Service , and checks if used in another YAML with kind:Deployment \n+    Works for all types. \n+    Need to provide script path, script dict, value identified as smell \n+    '''\n+    ret_lis = [] \n+    svc_dir     = os.path.dirname( script_path )  + constants.SLASH_SYMBOL    \n+    yaml_files  = getYAMLFiles( svc_dir )    \n+    for yaml_f in yaml_files:\n+        if( parser.checkIfValidK8SYaml( yaml_f ) ):\n+            dict_as_list   = parser.loadMultiYAML( yaml_f )\n+            sink_yaml_dict = parser.getSingleDict4MultiDocs( dict_as_list )                    \n+            sink_val_li_   = list(  parser.getValuesRecursively(sink_yaml_dict) )\n+            if( src_val in sink_val_li_ ) and ( constants.DEPLOYMENT_KW in sink_val_li_ ): \n+                    sink_keys = parser.keyMiner(sink_yaml_dict, src_val)\n+                    if constants.K8S_APP_KW in sink_keys: \n+                        ret_lis.append( (yaml_f, sink_keys  ) )\n+    return ret_lis \n+\n+\n+def mineNetPolGraph( script_, dict_y, src_val, src_keys ):\n+    '''\n+    Thsi method looks at YAML files that have kind: NetworkPoicy , and checks if used in another YAML\n+    with kind: Deployment or kind: Pod \n+    Works for all types \n+    Need to provide script path, script dict, idnetified values, and all keys of source \n+    '''\n+    lis2ret     = [] \n+    net_pol_dir = os.path.dirname( script_ )  + constants.SLASH_SYMBOL    \n+    yaml_files  = getYAMLFiles( net_pol_dir )    \n+    for yaml_f in yaml_files:\n+        if( parser.checkIfValidK8SYaml( yaml_f ) ):\n+            dict_as_list   = parser.loadMultiYAML( yaml_f )\n+            sink_yaml_dict = parser.getSingleDict4MultiDocs( dict_as_list )                    \n+            sink_val_li_   = list(  parser.getValuesRecursively(sink_yaml_dict) )\n+            if( src_val in sink_val_li_ ) and ( (constants.DEPLOYMENT_KW in sink_val_li_) or (constants.POD_KW in sink_val_li_) ):  \n+                sink_keys = parser.keyMiner(sink_yaml_dict, src_val)                \n+                for sink_k in sink_keys:\n+                    if ( sink_k in src_keys ):\n+                        lis2ret.append( ( src_val, sink_k ) ) \n+    return lis2ret \n+\n+\n+# if __name__=='__main__':"}, {"sha": "96be955295dfee8dfb5c77910ed8c78a9f47d5aa", "filename": "main.py", "status": "added", "additions": 96, "deletions": 0, "changes": 96, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/main.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/main.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/main.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,96 @@\n+'''\n+Zakariya Veasy\n+April 23, 2025\n+Source Code to Run Tool on All Kubernetes Manifests  \n+'''\n+import scanner \n+import pandas as pd \n+import constants\n+import logging\n+import typer\n+from pathlib import Path\n+\n+\n+logger = logging.getLogger(__name__)\n+logging.basicConfig(level=logging.INFO)\n+\n+\n+def getCountFromAnalysis(ls_):\n+    logger.info(f\"Analyzing {len(list_)} manifest entries\")\n+    list2ret           = []\n+    for tup_ in ls_:\n+        within_sec_cnt = 0 \n+        dir_name       = tup_[0]\n+        script_name    = tup_[1]        \n+        within_secret  = tup_[2]  # a list of dicts: [unameDict, passwordDict, tokenDict]\n+        within_sec_cnt = len(within_secret[0]) + len( within_secret[1]  ) + len( within_secret[2] )\n+        '''\n+        ### format: ('data', 'password', ([], ['MTIzNAo='], [])) => (<rootKey>, <key>, <data_list>) ... need the list of the last tuple\n+        if isinstance( within_secret, tuple ):\n+            within_sec_cnt = len( within_secret[-1][1] )\n+            # print( script_name,  within_secret, within_sec_cnt, type(within_secret) ) \n+        '''\n+        templa_secret  = tup_[3]       ### format: a list , we will not use this in dumping       \n+        taint_secret   = tup_[4]       ###   format: a list \n+        privilege_dic  = tup_[5]\n+        http_dict      = tup_[6]        \n+        secuContextDic = tup_[7]\n+        nSpaceDict     = tup_[8]                \n+        absentResoDict = tup_[9]                 \n+        rollUpdateDic  = tup_[10]\n+        netPolicyDict  = tup_[11]                \n+        pidfDict       = tup_[12]                \n+        ipcDict        = tup_[13]                 \n+        dockersockDic  = tup_[14]\n+        hostNetDict    = tup_[15]                        \n+        cap_sys_dic    = tup_[16]\n+        host_alias_dic = tup_[17]\n+        allow_priv_dic = tup_[18]\n+        unconfined_dic = tup_[19]\n+        cap_module_dic = tup_[20]\n+        k8s_flag       = tup_[21]\n+        helm_flag      = tup_[22]\n+\n+        list2ret.append(  ( dir_name, script_name, within_sec_cnt, len(taint_secret), len(privilege_dic), len(http_dict), len(secuContextDic), len(nSpaceDict), len(absentResoDict), len(rollUpdateDic), len(netPolicyDict), len(pidfDict), len(ipcDict), len(dockersockDic), len(hostNetDict), len(cap_sys_dic), len(host_alias_dic), len(allow_priv_dic), len(unconfined_dic), len(cap_module_dic) , k8s_flag, helm_flag  )  )\n+    return list2ret\n+\n+\n+def main(directory: Path = typer.Argument(..., exists=True, help=\"Absolute path to the folder than contains Kubernetes manifests\"),\n+         ):\n+    \"\"\"\n+    Run KubeSec in a Kubernetes directory and get results in a CSV file.\n+\n+    \"\"\"\n+    content_as_ls, sarif_json   = scanner.runScanner( directory )\n+    \n+    with open(\"SLIKUBE.sarif\", \"w\") as f:\n+      f.write(sarif_json)\n+\n+    df_all          = pd.DataFrame( getCountFromAnalysis( content_as_ls ) )\n+    outfile = Path(directory, \"slikube_results.csv\")\n+\n+    df_all.to_csv( outfile, header= constants.CSV_HEADER , index=False, encoding= constants.CSV_ENCODING )\n+\n+\n+if __name__ == '__main__':\n+\n+    '''\n+    DO NOT DELETE ALL IN K8S_REPOS AS TAINT TRACKING RELIES ON BASH SCRIPTS, ONE OF THE STRENGTHS OF THE TOOL \n+    '''\n+    # ORG_DIR         = '/Users/arahman/K8S_REPOS/GITHUB_REPOS/'\n+    # OUTPUT_FILE_CSV = '/Users/arahman/Documents/OneDriveWingUp/OneDrive-TennesseeTechUniversity/Research/Kubernetes/StaticTaint/data/V16_GITHUB_OUTPUT.csv'\n+\n+    # ORG_DIR         = '/Users/arahman/K8S_REPOS/GITLAB_REPOS/'\n+    # OUTPUT_FILE_CSV = '/Users/arahman/Documents/OneDriveWingUp/OneDrive-TennesseeTechUniversity/Research/Kubernetes/StaticTaint/data/V16_GITLAB_OUTPUT.csv'\n+\n+\n+    # ORG_DIR         = '/Users/arahman/K8S_REPOS/BRINTO_REPOS/'\n+    # OUTPUT_FILE_CSV = '/Users/arahman/Documents/OneDriveWingUp/OneDrive-TennesseeTechUniversity/Research/Kubernetes/StaticTaint/data/V16_BRINTO_OUTPUT.csv'\n+\n+    # take sarif_json from scanner\n+    main()\n+\n+\n+\n+\n+# test hook"}, {"sha": "c4f3a18d5ae5c2caf36906a85f7a715af51f6d67", "filename": "parser.py", "status": "added", "additions": 426, "deletions": 0, "changes": 426, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/parser.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/parser.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/parser.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,426 @@\n+'''\n+Zakariya Veasy\n+April 23, 2025 \n+Parser to file YAML files\n+'''\n+import sys\n+import ruamel.yaml \n+from ruamel.yaml.scanner import ScannerError\n+import json\n+#import jsonpath_rw as jp\n+# import yaml\n+import constants \n+import pathlib as pl\n+import re\n+import subprocess\n+import os\n+import logging\n+\n+\n+logger = logging.getLogger(__name__)\n+logging.basicConfig(level=logging.INFO)\n+\n+\n+#update basepath\n+base_path = r\" \"\n+\n+key_jsonpath_mapping = {}\n+\n+    \n+def checkIfWeirdYAML(yaml_script):\n+    '''\n+    to filter invalid YAMLs such as ./github/workflows/ \n+    '''\n+    val = False\n+    if ( any(x_ in yaml_script for x_ in constants.WEIRD_PATHS  ) ):\n+        val = True \n+    return val \n+\n+\n+def keyMiner(dic_, value):\n+  '''\n+  If you give a value, then this function gets the corresponding key, and the keys that call the key \n+  i.e. the whole hierarchy\n+  Returns None if no value is found  \n+  '''\n+  if dic_ == value:\n+    return [dic_]\n+  elif isinstance(dic_, dict):\n+    for k, v in dic_.items():\n+      p = keyMiner(v, value)\n+      if p:\n+        return [k] + p\n+  elif isinstance(dic_, list):\n+    lst = dic_\n+    for i in range(len(lst)):\n+      p = keyMiner(lst[i], value)\n+      if p:\n+        return [str(i)] + p\n+\n+\n+\n+def getKeyRecursively(  dict_, list2hold,  depth_ = 0  ) :\n+    '''\n+    gives you ALL keys in a regular/nested dictionary \n+    '''\n+    if  isinstance(dict_, dict) :\n+        # for key_, val_ in sorted(dict_.items(), key=lambda x: x[0]):    \n+        for key_, val_ in sorted(dict_.items(), key = lambda x: x[0] if ( isinstance(x[0], str) ) else str(x[0])  ):    \n+            if isinstance(val_, dict):\n+                list2hold.append( (key_, depth_) )\n+                depth_ += 1 \n+                getKeyRecursively( val_, list2hold,  depth_ ) \n+            elif isinstance(val_, list):\n+                for listItem in val_:\n+                        if( isinstance( listItem, dict ) ):\n+                            list2hold.append( (key_, depth_) )\n+                            depth_ += 1 \n+                            getKeyRecursively( listItem, list2hold,  depth_ )     \n+            else: \n+                list2hold.append( (key_, depth_) ) \n+    #print(list2hold)               \n+\n+def getValuesRecursively(  dict_   ) :\n+    '''\n+    gives you ALL values in a regular/nested dictionary \n+    '''\n+    if  isinstance(dict_, dict) :\n+        for val_ in dict_.values():\n+            yield from getValuesRecursively(val_) \n+            #print(val_)\n+    elif isinstance(dict_, list):\n+        for v_ in dict_:\n+            yield from getValuesRecursively(v_)\n+            #print(v_)\n+    else: \n+        yield dict_ \n+\n+\n+def checkIfValidK8SYaml(path2yaml):\n+    val2ret   = False \n+    dict_as_list = loadMultiYAML( path2yaml )\n+    yaml_dict    = getSingleDict4MultiDocs( dict_as_list )        \n+    k_list       = []\n+    getKeyRecursively( yaml_dict, k_list )\n+    temp_ = []\n+    for k_ in k_list:\n+        temp_.append( k_[0]  )\n+    key_lis      = list( getValuesRecursively  ( yaml_dict ) )\n+    if ( any(x_ in key_lis for x_ in constants.K8S_FORBIDDEN_KW_LIST ) ): \n+        val2ret = False \n+    else: \n+        if ( constants.API_V_KEYNAME in temp_ ) and (constants.KIND_KEY_NAME in temp_):\n+            val2ret = True \n+    return val2ret\n+\n+\n+\n+def getValsFromKey(dict_, target, list_holder  ):\n+    logger.info(f\"Searching for key: {target}\")\n+    '''\n+    If you give a key, then this function gets the corresponding values \n+    Multiple values are returned if there are keys with the same name  \n+    '''    \n+    if ( isinstance( dict_, dict ) ):\n+        for key, value in dict_.items():\n+            # print( key, len(key) , target, len( target ), value  )\n+            if key == target:\n+                list_holder.append( value )\n+            else: \n+                if isinstance(value, dict):\n+                    getValsFromKey(value, target, list_holder)\n+                elif isinstance(value, list):\n+                    for ls in value:\n+                        getValsFromKey(ls, target, list_holder)\n+\n+def checkIfValidHelm(path_script):\n+    val_ret = False \n+    if ( (constants.HELM_KW in path_script) or (constants.CHART_KW in path_script) or (constants.SERVICE_KW in path_script) or (constants.INGRESS_KW in path_script)  or(constants.HELM_DEPLOY_KW in path_script) or (constants.CONFIG_KW in path_script) )  and (constants.VALUE_KW in path_script) :\n+        val_ret = True \n+    return val_ret\n+\n+def readYAMLAsStr( path_script ):\n+    yaml_as_str = constants.YAML_SKIPPING_TEXT\n+    with open( path_script , constants.FILE_READ_FLAG) as file_:\n+        yaml_as_str = file_.read()\n+    return yaml_as_str\n+\n+# This function checks whether our parser throws an exception for reading the YAML file. \n+def checkParseError( path_script ):\n+    flag = True\n+    with open(path_script, constants.FILE_READ_FLAG) as yml:\n+        yaml = ruamel.yaml.YAML()\n+        try:\n+            for dictionary in yaml.load_all(yml):\n+                pass\n+        except ruamel.yaml.parser.ParserError as parse_error:\n+            flag = False\n+            print(constants.YAML_SKIPPING_TEXT)           \n+        except ruamel.yaml.error.YAMLError as exc:\n+            flag = False\n+            print( constants.YAML_SKIPPING_TEXT  )    \n+        except UnicodeDecodeError as err_: \n+            flag = False\n+            print( constants.YAML_SKIPPING_TEXT  )\n+    return flag\n+\n+def loadMultiYAML( script_ ):\n+    logger.info(f\"Attempting to load YAML from: {script_}\")\n+    dicts2ret = []  \n+    with open(script_, constants.FILE_READ_FLAG  ) as yml_content :\n+        yaml = ruamel.yaml.YAML()\n+        yaml.default_flow_style = False      \n+        try:\n+            for d_ in yaml.load_all(yml_content) :                \n+                # print('='*25)\n+                # print(d_)\n+                dicts2ret.append( d_ )\n+        except ruamel.yaml.parser.ParserError as parse_error:\n+            print(constants.YAML_SKIPPING_TEXT)           \n+        except ruamel.yaml.error.YAMLError as exc:\n+            print( constants.YAML_SKIPPING_TEXT  )    \n+        except UnicodeDecodeError as err_: \n+            print( constants.YAML_SKIPPING_TEXT  )\n+        \n+        path = find_json_path_keys(dicts2ret)\n+        #print(dicts2ret)\n+        no_exception = checkParseError(script_)\n+        if no_exception:\n+            # for debugging purposes\n+\n+            path = find_json_path_keys(dicts2ret) #, key_jsonpath_mapping\n+            # print(path)\n+            updated_path = update_json_paths(path)\n+            # print(updated_path)\n+            # print(\"-------------------HERE IS THE MAPPING---------------\")\n+            # for key in key_jsonpath_mapping:\n+            #     print(key, \"-->\", key_jsonpath_mapping[key],  \"-->\", print(type(key_jsonpath_mapping[key])) )\n+            # print(\"----LINE----\")\n+            # line = show_line_for_paths(script_, 'serviceName') #imagePullSecrets\n+            # print(line)\n+            # print(\"----JSON Validated----\")\n+            # print(type(dicts2ret))\n+            # for d in dicts2ret:\n+            #     print(type(d))\n+                \n+        #print(dicts2ret)\n+    return dicts2ret\n+\n+\n+def count_initial_comment_line (filepath):\n+    initial_comment_line = 0\n+    comment_found = False\n+    # calculates initial line before the comments begin in the file such as empty lines, '---'\n+    with open(filepath, constants.FILE_READ_FLAG  ) as yamlfile :       \n+        textfile = yamlfile.read()\n+        for line in textfile.split('\\n'):\n+            if line.startswith('#'):\n+                comment_found = True\n+                #print(line)\n+                initial_comment_line+=1\n+            elif not line:\n+                #print(line)\n+                if(comment_found is False):\n+                    initial_comment_line +=1\n+            elif line.startswith('---'):\n+                if(comment_found is False):\n+                    initial_comment_line +=1\n+            else:\n+                break\n+        if comment_found is False:\n+            initial_comment_line = 0\n+    return initial_comment_line\n+\n+\n+\"\"\"This function takes input as ruamel yaml ordered dictionary (commentedKeyMap), \n+the parent_path (root jsonpath), paths(additional paths) parameters are optional \n+\n+This function returns jsonpath for each key in the yaml file and also populates key_jsonpath_mapping dictionary\"\"\"\n+\n+def find_json_path_keys(json_file, parent_path='', paths=None):\n+    #key_jsonpath_mapping = {}\n+    # print(type(json_file))\n+\n+    \"\"\"The following regular expressions are used to remove elements to construct a VALID json path\"\"\"\n+\n+    # app.kubernetes.io/release --> \"app.kubernetes.io/release\"\n+    regex_key_dot = re.compile(r\"([^\\s\\.]+[.][\\S]+)\")\n+    # app.kubernetes.io/release --> \"app*kubernetes*io*release\"\n+    regex_special_character_removal = re.compile(\"[^A-Za-z0-9]+\")\n+    #[3].metadata.name --> .metadata.name\n+    regex_remove_initial_index = re.compile(\"^/?(\\[)([0-9])+(\\])\")\n+    \n+    if paths is None:\n+        paths = []   \n+    if isinstance(json_file, dict):\n+        for key, value in json_file.items():\n+        \n+            \"\"\"The following condition is used if there is any key: value mapping like jinja format sych as key: {{value}}. \n+               This is a temporary fix to handle the case\"\"\"\n+            if (isinstance(key,ruamel.yaml.comments.CommentedKeyMap) and value is None) or isinstance(key,int):\n+                pass\n+            else:\n+                if regex_key_dot.match(key):              \n+                    str = regex_special_character_removal.sub(\"*\",key)\n+                    path_withindex = f\"{parent_path}.{str}\"\n+                    path = regex_remove_initial_index.sub('',path_withindex)\n+                    #print(\"NOW IN DOT, the path--->\",path)\n+                    #key_jsonpath_mapping[key] =path\n+                    if key_jsonpath_mapping.get(key) is None:\n+                        key_jsonpath_mapping[key] = []\n+                        key_jsonpath_mapping[key].append(path)\n+                        #print(key_jsonpath_mapping)\n+                    else:\n+                        '''Exckude the path if it is already present in the key_jsonpath_mapping dictionary. '''\n+                        if path not in key_jsonpath_mapping[key]:\n+                            key_jsonpath_mapping[key].append(path)\n+                        # print(\"DOT Keys -->\",key_jsonpath_mapping[key])\n+                                    \n+                    paths.append(path)\n+                    find_json_path_keys(value, parent_path=path, paths=paths)\n+                else:\n+                    path_withindex = f\"{parent_path}.{key}\"\n+                    path = regex_remove_initial_index.sub('',path_withindex)\n+                    #print(\"NOW IN REGULAR, the path--->\",path)\n+                    if key_jsonpath_mapping.get(key) is None:\n+                        key_jsonpath_mapping[key] = []\n+                        key_jsonpath_mapping[key].append(path)\n+                        #print(key_jsonpath_mapping)\n+                    else:\n+                        key_jsonpath_mapping[key] = path\n+                        #print(key_jsonpath_mapping) \n+                    #key_jsonpath_mapping[key] =path\n+                    #print(key_jsonpath_mapping)\n+                    paths.append(path)          \n+                    find_json_path_keys(value, parent_path=path, paths=paths)\n+    elif isinstance(json_file, list):\n+        for index, value in enumerate(json_file):\n+            path_withindex = f\"{parent_path}[{index}]\"\n+            path = regex_remove_initial_index.sub('',path_withindex)\n+            paths.append(path)\n+            find_json_path_keys(value, parent_path=path, paths=paths)\n+    return paths \n+    \n+\n+\"\"\"\"The following function is used to update the json path in the key_jsonpath_mapping dictionary.\n+Useful in MultiYaml file format but redundant in single yaml Need to merge with the above function \"\"\"\n+\n+def update_json_paths (paths):\n+    #[3].metadata.name --> .metadata.name\n+    regex_remove_initial_index = re.compile(\"^/?(\\[)([0-9])+(\\])\")\n+    json_path =[]\n+    updated_paths =[]\n+    remove = ''    \n+    for path in paths:            \n+        path_remove_initial_index = regex_remove_initial_index.sub('',path)\n+        if(path_remove_initial_index != remove):\n+            json_path.append(path_remove_initial_index)\n+            updated_paths.append(path_remove_initial_index)        \n+    return json_path\n+    \n+\n+def show_line_for_paths(  filepath, key): #key_jsonpath_mapping is a global dictionary\n+    line_number = count_initial_comment_line(filepath)\n+    \"\"\"\n+    input: provide  JSON_PATH dictionary and the key\n+    output:  line of appearance of the key in the file\n+    \"\"\"\n+    env_PATH = r\"C:\\ProgramData\\Chocolatey\\bin\"\n+    lines = []\n+    adjusted_lines = []\n+    print(\"This is the mapping for the Key\",key,\"--->\",key_jsonpath_mapping[key]) \n+    # for k in key_jsonpath_mapping:\n+    #     print(\"Key--->\",k, \"Value--->\",key_jsonpath_mapping[k])\n+    if key_jsonpath_mapping.get(key) is not None:\n+        if isinstance(key_jsonpath_mapping[key], list):\n+            for i in key_jsonpath_mapping[key]:\n+                #print(i)\n+                yq_parameter = i + \" | key | line \"\n+                #print(yq_parameter)\n+                result = subprocess.check_output([\"yq\", yq_parameter , filepath], universal_newlines=True)\n+                #result = subprocess.run([\"C:/ProgramData/Chocolatey/bin/yq \",yq_parameter, filepath], shell = True, text= True, capture_output= True,cwd= env_PATH ) #env= {'PATH' : env_PATH}\n+                #env= {'PATH': 'C:\\ProgramData\\Chocolatey\\bin'}\n+                #print(result)\n+                output = result.split(\"---\")\n+                for line in output:\n+                    line.replace(\"\\n\",\"\")\n+                    line_convert = int(line)\n+                    print(type(line_convert))\n+                    if(line_convert >0):\n+                        line_number = line_convert + count_initial_comment_line(filepath)\n+                        lines.append(line_number)\n+                #print(lines)\n+        else:\n+            yq_parameter = key_jsonpath_mapping[key]+ \" | key | line \"\n+            result = subprocess.check_output([\"yq\", yq_parameter , filepath], universal_newlines=True)\n+                #result = subprocess.run([\"C:/ProgramData/Chocolatey/bin/yq \",yq_parameter, filepath], shell = True, text= True, capture_output= True,cwd= env_PATH ) #env= {'PATH' : env_PATH}\n+                #env= {'PATH': 'C:\\ProgramData\\Chocolatey\\bin'}\n+                \n+            output = result.split(\"---\")\n+            for line in output:\n+                line.replace(\"\\n\",\"\")\n+                line_convert = int(line)\n+                #print(type(line_convert))\n+                if(line_convert >0):\n+                    line_number = line_convert + count_initial_comment_line(filepath)\n+                    lines.append(line_number)\n+                #print(lines)\n+                   \n+    return lines\n+\n+\n+\n+def getSingleDict4MultiDocs( lis_dic ):\n+    dict2ret = {} \n+    key_lis  = []\n+    counter  = 0 \n+    for dic in lis_dic:\n+        # print(dic)\n+        # print('='*100) dic = dic[0]\n+        # print(\"----DIC---\")\n+        # print(dic)\n+        # print(\"----DIC---\")\n+        if( isinstance(dic, list) ): \n+            '''\n+            to tackle YAMLs that are Ansible YAMLs and not K8S YAMLS\n+            '''\n+            dic = dic[0]\n+            \n+        '''\n+        the algorithm is if there are keys with similar names \n+        then add a suffix to differentiate between keys for \n+        multiple docs in a single YAML \n+        '''\n+        # print(dic) \n+        '''\n+        to handle Nones \n+        '''\n+\n+        \"\"\" the algorithm doesn't check the keys with suffix for misconfigurations\"\"\"\n+\n+        # if (dic is not None) and isinstance(dic, dict):\n+        #     keys4dic = list(dic.keys())\n+        #     for k_ in keys4dic:\n+        #         dict2ret[k_] = dic[k_]\n+\n+        if ( (dic is None) == False  ) and (isinstance(dic, dict ) ):\n+            keys4dic = list(dic.keys()) \n+            for k_ in keys4dic: \n+                if k_ in key_lis:\n+                    dict2ret[k_ + constants.DOT_SYMBOL + constants.YAML_DOC_KW + str(counter)] = dic[k_]\n+                else:\n+                    key_lis.append( k_ )\n+                    dict2ret[k_] = dic[k_] \n+            counter += 1 \n+            #print(dict2ret) \n+    # print(\"-----In Single Dict 4 multiple docs---\")\n+    # yaml = ruamel.yaml.YAML()\n+    # yaml.dump(dict2ret, sys.stdout)\n+    # print(\"-----\")\n+        \n+    return dict2ret\n+\n+\n+if __name__=='__main__':\n+    yaml_path = pl.Path(base_path,'test.yaml')\n+    dic_lis   = loadMultiYAML(yaml_path)"}, {"sha": "b642a5417bac985c99892b3159818c83138e478b", "filename": "requirements.txt", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/requirements.txt", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/requirements.txt", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/requirements.txt?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f"}, {"sha": "d4571c0acaaf8ace3c0c9e767bb7528951b450af", "filename": "scanner.py", "status": "added", "additions": 1018, "deletions": 0, "changes": 1018, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/scanner.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/scanner.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/scanner.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,1018 @@\n+'''\n+Akond Rahman \n+May 03, 2021 \n+Code to detect security anti-patterns \n+'''\n+import parser \n+import constants \n+import graphtaint \n+import os \n+import pandas as pd \n+import numpy as np\n+import json\n+from sarif_om import *\n+from jschema_to_python.to_json import to_json\n+import logging\n+\n+\n+logger = logging.getLogger(__name__)\n+logging.basicConfig(level=logging.INFO)\n+\n+'''Global SarifLog Object definition and Rule definition for SLI-KUBE. Rule IDs are ordered by the sequence as it appears in the TOSEM paper'''\n+\n+sarif_log = SarifLog(version='2.1.0',schema_uri='https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json', runs =[])\n+run = Run(tool=Tool(driver=ToolComponent(name = 'SLI-KUBE', version = '2.0.0',information_uri ='https://github.com/paser-group/KubeSec',organization = 'PASER',rules=[])))\n+sarif_log.runs.append(run)\n+run.results = []\n+slikube_01 =  ReportingDescriptor(id='SLIKUBE_01',name=\" Absent Resource Limit\",short_description=Message(text=\"Specify resource limits for containers within a pod\"),full_description= Message( text= \"Specify resource limits for containers within a pod\"),help= Message(text= \"Specify resource limits for containers within a pod\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_01)\n+slikube_02 =  ReportingDescriptor(id='SLIKUBE_02',name=\" Absent securityContext\",short_description=Message(text=\" Use securityContext while provisioning containers\"),full_description= Message( text= \" Use securityContext while provisioning containers\"),help= Message(text= \" Use securityContext while provisioning containers\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_02)\n+slikube_03 =  ReportingDescriptor(id='SLIKUBE_03',name=\" Activation of hostIPC\",short_description=Message(text=\" Deactivate hostIPC while specifying configurations in Kubernetes manifests\"),full_description= Message( text= \" Deactivate hostIPC while specifying configurations in Kubernetes manifests\"),help= Message(text= \"Deactivate hostIPC while specifying configurations in Kubernetes manifests\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_03)\n+slikube_04 =  ReportingDescriptor(id='SLIKUBE_04',name=\" Activation of hostNetwork\",short_description=Message(text=\" Deactivate hostNetwork while specifying configurations in Kubernetes manifests\"),full_description= Message( text= \" Deactivate hostNetwork while specifying configurations in Kubernetes manifests\"),help= Message(text= \" Deactivate hostNetwork while specifying configurations in Kubernetes manifests\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_04)\n+slikube_05 =  ReportingDescriptor(id='SLIKUBE_05',name=\" Activation of hostPID\",short_description=Message(text=\" Deactivate hostPID while specifying configurations in Kubernetes manifests\"),full_description= Message( text= \" Deactivate hostPID while specifying configurations in Kubernetes manifests\"),help= Message(text=  \"Deactivate hostPID while specifying configurations in Kubernetes manifests\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_05)\n+slikube_06 =  ReportingDescriptor(id='SLIKUBE_06',name=\" Capability Misuse\",short_description=Message(text=\" Avoid misuse with CAP_SYS_ADMIN, and misuse with CAP_SYS_MODULE configurations\"),full_description= Message( text= \"Avoid misuse with CAP_SYS_ADMIN, and misuse with CAP_SYS_MODULE configurations\"),help= Message(text= \"Avoid misuse with CAP_SYS_ADMIN, and misuse with CAP_SYS_MODULE configurations\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_06)\n+slikube_07 =  ReportingDescriptor(id='SLIKUBE_07',name=\" Docker Socket Mounting\",short_description=Message(text=\" Avoid mounting of the Docker socket path by using the /var/run/docker.sock configuration\"),full_description= Message( text= \"Avoid mounting of the Docker socket path by using the /var/run/docker.sock configuration\"),help= Message(text= \"Avoid mounting of the Docker socket path by using the /var/run/docker.sock configuration\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_07)\n+slikube_08 =  ReportingDescriptor(id='SLIKUBE_08',name=\" Escalated Privileges for Child Container Processes\",short_description=Message(text=\" Avoid allocating privileges for child processes within a container that are higher than that of the parent processes with allowPrivilegeEscaltion : true\"),full_description= Message( text= \" Avoid allocating privileges for child processes within a container that are higher than that of the parent processes with allowPrivilegeEscaltion : true\"),help= Message(text= \" Avoid allocating privileges for child processes within a container that are higher than that of the parent processes with allowPrivilegeEscaltion : true\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_08)\n+slikube_09 =  ReportingDescriptor(id='SLIKUBE_09',name=\" Hard-coded Secret\",short_description=Message(text=\" Avoid providing hard-coded secrets. Do not provide (i) hard-coded usernames, (ii) hard-coded passwords, and (iii) hard-coded private tokens in Kubernetes manifests.\"),full_description= Message( text= \" Avoid providing hard-coded secrets. Do not provide (i) hard-coded usernames, (ii) hard-coded passwords, and (iii) hard-coded private tokens in Kubernetes manifests.\"),help= Message(text= \" Avoid providing hard-coded secrets. Do not provide (i) hard-coded usernames, (ii) hard-coded passwords, and (iii) hard-coded private tokens in Kubernetes manifests.\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_09)\n+slikube_10 =  ReportingDescriptor(id='SLIKUBE_10',name=\" Use of HTTP without TLS\",short_description=Message(text=\" Avoid using HTTP without SSL/TLS certificates to setup URLs or transmit traffic inside and outside the Kubernetes clusters\"),full_description= Message( text= \" Avoid using HTTP without SSL/TLS certificates to setup URLs or transmit traffic inside and outside the Kubernetes clusters\"),help= Message(text= \"Avoid using HTTP without SSL/TLS certificates to setup URLs or transmit traffic inside and outside the Kubernetes clusters\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_10)\n+slikube_11 =  ReportingDescriptor(id='SLIKUBE_11',name=\" Privileged securityContext\",short_description=Message(text=\"Avoid using privileged securityContext in Kubernetes manifests\"),full_description= Message( text= \"Avoid using privileged securityContext in Kubernetes manifests\"),help= Message(text= \"Avoid using privileged securityContext in Kubernetes manifests\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_11)\n+slikube_UNLISTED_01 =  ReportingDescriptor(id='SLIKUBE_UNLISTED_01',name=\" Use of Default Namespace\",short_description=Message(text=\" Avoid using the default namespace\"),full_description= Message( text= \"  Avoid using the default namespace\"),help= Message(text= \" Avoid using the default namespace\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_UNLISTED_01)\n+slikube_UNLISTED_02 =  ReportingDescriptor(id='SLIKUBE_UNLISTED_02',name=\" No Use of Rolling Update\",short_description=Message(text=\"  Use rolling update for deployment\"),full_description= Message( text= \" Use rolling update for deployment\"),help= Message(text= \"Use rolling update for deployment\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_UNLISTED_02)\n+slikube_UNLISTED_03 =  ReportingDescriptor(id='SLIKUBE_UNLISTED_03',name=\" No Network Policy\",short_description=Message(text=\" Use network policy in pod specification\"),full_description= Message( text= \" Use network policy in pod specification\"),help= Message(text= \"Use network policy in pod specification\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_UNLISTED_03)\n+slikube_UNLISTED_04 =  ReportingDescriptor(id='SLIKUBE_UNLISTED_04',name=\" Use of Host Aliases\",short_description=Message(text=\"Avoid Using Host Aliases\"),full_description= Message( text= \"Avoid Using Host Aliases\"),help= Message(text= \"Avoid Using Host Aliases\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_UNLISTED_04)\n+slikube_UNLISTED_05 =  ReportingDescriptor(id='SLIKUBE_UNLISTED_05',name=\" Use of unconfined seccomp profile\",short_description=Message(text=\" Use Seccomp security profiles\"),full_description= Message( text= \"  Use Seccomp security profiles\"),help= Message(text= \" Use Seccomp security profiles\"), default_configuration= ReportingConfiguration(level=\"error\"))\n+run.tool.driver.rules.append(slikube_UNLISTED_05)\n+\n+\n+'''Following lists are used to check the statistics of analyzed files such as invalid, weird and valid k8s and helm charts'''\n+\n+invalid_yaml = []\n+weird_yaml = []\n+helm_chart = []\n+k8s_yaml =[]\n+\n+def getYAMLFiles(path_to_dir):\n+    valid_  = [] \n+    for root_, dirs, files_ in os.walk( path_to_dir ):\n+       for file_ in files_:\n+           full_p_file = os.path.join(root_, file_)\n+           if(os.path.exists(full_p_file)):\n+             if (full_p_file.endswith( constants.YML_EXTENSION  )  or full_p_file.endswith( constants.YAML_EXTENSION  )  ):\n+               valid_.append(full_p_file)\n+    return valid_ \n+\n+def isValidUserName(uName): \n+    valid = True\n+    if (isinstance( uName , str)  ): \n+        if( any(z_ in uName for z_ in constants.FORBIDDEN_USER_NAMES )   ): \n+            valid = False   \n+        else: \n+            valid = True    \n+    else: \n+        valid = False   \n+    return valid\n+\n+def isValidPasswordName(pName): \n+    valid = True\n+    if (isinstance( pName , str)  ): \n+        if( any(z_ in pName for z_ in constants.FORBIDDEN_PASS_NAMES) )  : \n+            valid = False  \n+        else: \n+            valid = True    \n+    else: \n+        valid = False               \n+    return valid\n+\n+def isValidKey(keyName): \n+    valid = False \n+    if ( isinstance( keyName, str )  ):\n+        if( any(z_ in keyName for z_ in constants.LEGIT_KEY_NAMES ) ) : \n+            valid = True   \n+        else: \n+            valid = False     \n+    else: \n+        valid = False                      \n+    return valid    \n+\n+def checkIfValidSecret(single_config_val):\n+    flag2Ret = False \n+    # print(type( single_config_val ), single_config_val  )\n+    if ( isinstance( single_config_val, str ) ):\n+        single_config_val = single_config_val.lower()\n+        config_val = single_config_val.strip() \n+        if ( any(x_ in config_val for x_ in constants.INVALID_SECRET_CONFIG_VALUES ) ):\n+            flag2Ret = False \n+        else:\n+            if(  len(config_val) > 2 )  :\n+                flag2Ret = True \n+    else: \n+        flag2Ret = False \n+    return flag2Ret\n+\n+def scanUserName(k_ , val_lis ):\n+    hard_coded_unames = []\n+    if isinstance(k_, str):\n+        k_ = k_.lower()    \n+    # print('INSPECTING:', k_) \n+    if( isValidUserName( k_ )   and any(x_ in k_ for x_ in constants.SECRET_USER_LIST )  ):\n+        # print( val_lis ) \n+        for val_ in val_lis:\n+            if (checkIfValidSecret( val_ ) ): \n+                # print(val_) \n+                hard_coded_unames.append( val_ )\n+    return hard_coded_unames\n+\n+def scanPasswords(k_ , val_lis ):\n+    hard_coded_pwds = []\n+    if isinstance(k_, str):\n+        k_ = k_.lower()    \n+    if( isValidPasswordName( k_ )   and any(x_ in k_ for x_ in constants.SECRET_PASSWORD_LIST )  ):\n+        for val_ in val_lis:\n+            if (checkIfValidSecret( val_ ) ): \n+                hard_coded_pwds.append( val_ )\n+    return hard_coded_pwds\n+\n+\n+def checkIfValidKeyValue(single_config_val):\n+    flag2Ret = False \n+    if ( isinstance( single_config_val, str ) ):\n+        if ( any(x_ in single_config_val for x_ in constants.VALID_KEY_STRING ) ):\n+            flag2Ret = True \n+    return flag2Ret\n+\n+def scanKeys(k_, val_lis):\n+    hard_coded_keys = []\n+    if isinstance(k_, str):\n+        k_ = k_.lower()    \n+    if( isValidKey( k_ )    ):\n+        for val_ in val_lis:\n+            if (checkIfValidKeyValue( val_ ) ): \n+                hard_coded_keys.append( val_ )\n+    return hard_coded_keys    \n+\n+\n+def scanForSecrets(yaml_d):\n+    logger.info(f\"Scanning for secrets in parsed YAML dict\")\n+    key_lis, dic2ret_secret   = [], {} \n+    parser.getKeyRecursively( yaml_d, key_lis )\n+    '''\n+    if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+    as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+    '''    \n+    for key_data  in key_lis:\n+        key_     = key_data[0]\n+        value_list = [] \n+        parser.getValsFromKey( yaml_d, key_ , value_list )\n+        unameList = scanUserName( key_, value_list  )\n+        # print(unameList)\n+        passwList = scanPasswords( key_, value_list  )\n+        keyList   = scanKeys( key_, value_list )\n+        # print(keyList)\n+        if( len(unameList) > 0  )  or ( len(passwList) > 0  ) or ( len(keyList) > 0  ) :\n+            dic2ret_secret[key_] =  ( unameList, passwList, keyList ) \n+    # print(dic2ret_secret)\n+    return dic2ret_secret\n+\n+\n+def scanForOverPrivileges(script_path):\n+    key_count , privi_dict_return = 0, {} \n+    kind_values = [] \n+    checkVal = parser.checkIfValidK8SYaml( script_path )\n+    if(checkVal): \n+        dict_as_list = parser.loadMultiYAML( script_path )\n+        yaml_dict    = parser.getSingleDict4MultiDocs( dict_as_list )\n+        # print(yaml_dict.keys())\n+        key_lis   = []\n+        parser.getKeyRecursively(yaml_dict, key_lis) \n+        # print('KEY LIST ALL-------------------------------------')\n+        # print(key_lis)\n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        just_keys = [x_[0] for x_ in key_lis] \n+        # print('JUST KEYS ALL -----------------------------------------------------')\n+        # print(just_keys)\n+        # just_keys = list( np.unique( just_keys )  )\n+        if ( constants.KIND_KEY_NAME in just_keys ):\n+            parser.getValsFromKey( yaml_dict, constants.KIND_KEY_NAME, kind_values )\n+            \n+        '''\n+        For the time being Kind:DeamonSet is not a legit sink because they do not directly provision deplyoments \n+        '''\n+        # print(just_keys) \n+        if ( constants.PRIVI_KW in just_keys ) and ( constants.DEAMON_KW not in kind_values  ) :\n+            privilege_values = []\n+            parser.getValsFromKey( yaml_dict, constants.PRIVI_KW , privilege_values )\n+            # print(privilege_values) \n+            for value_ in privilege_values:\n+                    if value_ == True: \n+                        key_lis_holder = parser.keyMiner(yaml_dict, value_ ) \n+                        # print( key_lis_holder )\n+                        if(constants.CONTAINER_KW in key_lis_holder) and (constants.SECU_CONT_KW in key_lis_holder) and (constants.PRIVI_KW in key_lis_holder):\n+                            key_count += 1\n+                            privi_dict_return[key_count] = value_, key_lis_holder \n+                            line_number = parser.show_line_for_paths(script_path, constants.PRIVI_KW)\n+                            for line in line_number:\n+                                result= Result(rule_id='SLIKUBE_11',rule_index= 10, level='error',attachments = [] ,message=Message(text=\" Privileged securityContext\"))\n+                                location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=script_path),region = Region(start_line =line)))\n+                                result.locations = [location]\n+                                run.results.append(result)\n+    return privi_dict_return \n+\n+def getItemFromSecret( dict_sec, pos ): \n+    dic2ret = {}\n+    cnt     = 0 \n+    for key_name , key_tup in dict_sec.items():\n+        secret_data_list = key_tup[pos]\n+        for data_ in secret_data_list: \n+            dic2ret[cnt] = (key_name, data_)\n+            cnt          += 1\n+    return dic2ret\n+\n+\n+def scanSingleManifest( path_to_script ):\n+    '''\n+    While it is named as `scanSingleManifest` \n+    it can only do taint tracking for secrets and over privileges \n+    '''\n+    checkVal = parser.checkIfValidK8SYaml( path_to_script )\n+    # print(checkVal) \n+    # initializing \n+    within_secret_ = []\n+    dict_secret    = {} \n+    dict_list      = parser.loadMultiYAML( path_to_script )\n+    yaml_dict      = parser.getSingleDict4MultiDocs( dict_list )\n+    if(checkVal):\n+        '''\n+        additional logic to handle secrets within a valid Kubernetes manifest \n+        '''\n+        val_lis    = list( parser.getValuesRecursively  ( yaml_dict ) )\n+        if( constants.CONFIGMAP_KW in val_lis   ) or (constants.SECRET_KW in val_lis):  \n+            secret_key_list    = parser.keyMiner(yaml_dict, constants.SECRET_KW)  \n+            configmap_key_list = parser.keyMiner(yaml_dict, constants.CONFIGMAP_KW)  \n+            key_lis            = []\n+            if( isinstance(secret_key_list, list) ): \n+                key_lis = key_lis + secret_key_list \n+            if( isinstance(configmap_key_list, list) ): \n+                key_lis = key_lis + configmap_key_list \n+            if (len( key_lis ) > 0 ) :\n+                unique_keys = np.unique( key_lis )\n+                unique_keys = [x_ for x_ in unique_keys if constants.KIND_KEY_NAME in x_]\n+                if (len( unique_keys ) > 0 ):\n+                    dict_secret = scanForSecrets( yaml_dict )\n+                    within_secret_.append( getItemFromSecret( dict_secret, 0 )  ) # 0 for username \n+                    within_secret_.append( getItemFromSecret( dict_secret, 1 )  ) # 1 for password \n+                    within_secret_.append( getItemFromSecret( dict_secret, 2 )  ) # 2 for tokens  \n+                else: \n+                    within_secret_.append({})\n+                    within_secret_.append({})\n+                    within_secret_.append({})                     \n+            else: \n+                within_secret_.append({})\n+                within_secret_.append({})\n+                within_secret_.append({}) \n+        else: \n+            within_secret_.append({})\n+            within_secret_.append({})\n+            within_secret_.append({})            \n+    elif ( parser.checkIfValidHelm( path_to_script )) :\n+        dict_secret = scanForSecrets( yaml_dict )\n+        within_secret_.append({})\n+        within_secret_.append({})\n+        within_secret_.append({}) \n+    else: \n+        within_secret_.append({})\n+        within_secret_.append({})\n+        within_secret_.append({}) \n+    \n+    '''\n+    taint tracking zone for secret dictionary \n+    '''\n+    # print(dict_secret)   \n+    \n+    # for each key in dict_secret we extract the key, return the line number and extracted valid taints for the secret\n+\n+    for key in dict_secret:\n+        line_number = parser.show_line_for_paths(path_to_script, key)\n+        for line in line_number:\n+            print(line)\n+            secret, template_secret, valid_taint = graphtaint.mineSecretGraph(path_to_script, yaml_dict, dict_secret)\n+            '''Included valid taints in attachments []'''\n+            result= Result(rule_id='SLIKUBE_09',rule_index= 8, level='error',attachments = [valid_taint] ,message=Message(text=\" Hard-coded Secret\"))\n+            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_to_script),region = Region(start_line =line)))\n+            result.locations = [location]\n+            run.results.append(result)\n+\n+    _, templ_secret_, valid_taint_secr  = graphtaint.mineSecretGraph(path_to_script, yaml_dict, dict_secret) \n+    # print(within_secret_) \n+    # print(templ_secret_) \n+    # print(valid_taint_secr) \n+    '''\n+    taint tracking for over privileges \n+    '''\n+    #valid_taint_privi  = scanForOverPrivileges( path_to_script )\n+    # print(valid_taint_privi) \n+\n+    return within_secret_, templ_secret_, valid_taint_secr \n+\n+\n+ \n+\n+def scanForMissingSecurityContext(path_scrpt):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_scrpt )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_scrpt )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        key_lis = [] \n+        parser.getKeyRecursively(yaml_di, key_lis)\n+        yaml_values = list( parser.getValuesRecursively(yaml_di) )\n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        real_key_lis = [x_[0] for x_ in key_lis]\n+        # print(real_key_lis) \n+        if (constants.SECU_CONT_KW  not in real_key_lis)  and ( constants.CONTAINER_KW in real_key_lis ): \n+            occurrences = real_key_lis.count( constants.CONTAINER_KW )\n+            for _ in range( occurrences ):\n+                prop_value = constants.YAML_SKIPPING_TEXT\n+                # if ( constants.DEPLOYMENT_KW in yaml_values ) : \n+                #     prop_value = constants.DEPLOYMENT_KW\n+                #     lis.append( prop_value )\n+                if ( constants.POD_KW in yaml_values ) :\n+                    pod_kw_lis = parser.keyMiner(  yaml_di, constants.POD_KW  )\n+                    if ( constants.KIND_KEY_NAME in pod_kw_lis ):\n+                        cnt += 1 \n+                        prop_value = constants.POD_KW \n+                        lis.append( prop_value )\n+                        dic[ cnt ] = lis\n+\n+                        # As absent securityContext, we assume that it should be in pod specification so line number will be the line number of the spec.\n+                        line_number = parser.show_line_for_paths(path_scrpt,constants.SPEC_KW)\n+                        for line in line_number:\n+                            result= Result(rule_id='SLIKUBE_02',rule_index=1, level='error',attachments = [] ,message=Message(text=\" Absent securityContext\"))\n+                            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_scrpt),region = Region(start_line =line)))\n+                            result.locations = [location]\n+                            run.results.append(result)\n+    # print(dic) \n+    return dic \n+\n+\n+def scanForDefaultNamespace(path_scrpt):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_scrpt )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_scrpt )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        nspace_vals  = []\n+        parser.getValsFromKey( yaml_di, constants.NAMESPACE_KW, nspace_vals )\n+        # print(nspace_vals)\n+        '''\n+        we are not going to process list of dicts \n+        '''\n+        nspace_vals        = [x_ for x_ in nspace_vals if isinstance( x_ , str ) ]\n+        unique_nspace_vals =  list( np.unique( nspace_vals  ) )\n+        if (len(unique_nspace_vals) == 1 ) and ( unique_nspace_vals[0] == constants.DEFAULT_KW  ): \n+            key_lis = parser.keyMiner(yaml_di, constants.DEFAULT_KW)\n+            if (isinstance( key_lis, list ) ):\n+                if (len(key_lis) > 0 ) : \n+                    all_values = list( parser.getValuesRecursively(yaml_di)  )\n+                    cnt += 1 \n+                    \n+                    # Lines for SARIF output\n+                    line_number = parser.show_line_for_paths(path_scrpt,constants.NAMESPACE_KW)\n+                    for line in line_number:\n+                        result= Result(rule_id='SLIKUBE_UNLISTED_01',rule_index= 11, level='error',attachments = [] ,message=Message(text=\" Use of Default Namespace\"))\n+                        location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_scrpt),region = Region(start_line =line)))\n+                        result.locations = [location]\n+                        run.results.append(result)\n+                    prop_value = constants.YAML_SKIPPING_TEXT \n+                    if ( constants.DEPLOYMENT_KW in all_values ) : \n+                        prop_value = constants.DEPLOYMENT_KW\n+                        lis.append( prop_value )\n+                    elif ( constants.POD_KW in all_values ) :\n+                        prop_value = constants.POD_KW \n+                        lis.append( prop_value )\n+                    else: \n+                        holder_ = [] \n+                        parser.getValsFromKey(yaml_di, constants.KIND_KEY_NAME, holder_ )\n+                        if ( constants.K8S_SERVICE_KW in holder_ ): \n+                            srv_val_li_ = [] \n+                            parser.getValsFromKey( yaml_di, constants.K8S_APP_KW, srv_val_li_  ) \n+                            for srv_val in srv_val_li_:\n+                                lis = graphtaint.mineServiceGraph( path_scrpt, yaml_di, srv_val )\n+\n+\n+                    dic[ cnt ] = lis\n+    # print(dic) \n+    return dic \n+\n+\n+def scanForResourceLimits(path_scrpt):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_scrpt )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_scrpt )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        '''\n+        get values for a key from the dict \n+        then check if at least one unique entry is kind:Pod \n+        '''\n+        val_lis = [] \n+        parser.getValsFromKey(yaml_di, constants.KIND_KEY_NAME, val_lis) \n+        kind_entries =  list( np.unique( val_lis ) )\n+        if ( constants.POD_KW in kind_entries ):\n+            if ( (constants.CONTAINER_KW in key_list) and (constants.LIMITS_KW not in key_list ) and ( (constants.CPU_KW not in key_list)  or (constants.MEMORY_KW not in key_list) ) ):\n+                cnt += 1\n+                # the line number for SARIF output should be the line number of the container spec \n+                line_number = parser.show_line_for_paths(path_scrpt, constants.SPEC_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_01',rule_index= 0, level='error',attachments = [] ,message=Message(text=\"  Absent Resource Limit\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_scrpt),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+\n+                if( len(temp_ls) > 0 ):\n+                    all_values = list( parser.getValuesRecursively(yaml_di)  )\n+                    # print(all_values)\n+                    prop_value = constants.YAML_SKIPPING_TEXT \n+                    if ( constants.POD_KW in all_values ) :\n+                        prop_value = constants.POD_KW \n+                        lis.append( prop_value )\n+                dic[ cnt ] = lis\n+    # print(dic) \n+    return dic \n+\n+\n+def scanForRollingUpdates(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if ( (constants.STRATEGY_KW not in key_list ) and  (constants.ROLLING_UPDATE_KW not in key_list) and (constants.SPEC_KW in key_list)   ):\n+            if( len(temp_ls) > 0 ):\n+                all_values = list( parser.getValuesRecursively(yaml_di)  )\n+                # print(all_values)\n+                prop_value = constants.YAML_SKIPPING_TEXT \n+                if ( constants.DEPLOYMENT_KW in all_values ) and ( constants.VAL_ROLLING_UPDATE_KW not in all_values ) : \n+                    keyFromVal =  parser.keyMiner(yaml_di, constants.DEPLOYMENT_KW)\n+                    if( constants.KIND_KEY_NAME in keyFromVal ):\n+                        cnt += 1 \n+                        # the line number for SARIF output should be the line number of the container spec for rolling update\n+                        line_number = parser.show_line_for_paths(path_script,constants.SPEC_KW)\n+                        for line in line_number:\n+                            result= Result(rule_id='SLIKUBE_UNLISTED_02',rule_index= 12, level='error',attachments = [] ,message=Message(text=\" No Use of Rolling Update\"))\n+                            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                            result.locations = [location]\n+                            run.results.append(result)\n+                        dic[ cnt ] = [ constants.DEPLOYMENT_KW ]\n+    return dic     \n+\n+\n+def scanForMissingNetworkPolicy(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        all_values = list( parser.getValuesRecursively(yaml_di)  )\n+        #print(all_values)\n+        #print(all_values)\n+        if ( constants.NET_POLICY_KW not in all_values ):\n+            cnt += 1 \n+            temp_ls = [] \n+            parser.getKeyRecursively(yaml_di, temp_ls) \n+            '''\n+            if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+            as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+            '''\n+            #print (temp_ls)\n+            key_list = [ x_[0] for x_ in temp_ls  ]\n+            #print(key_list)\n+            \n+            # specification is present the line number of the SARIF output should be the line number of the container spec \n+            if (constants.SPEC_KW in key_list ):\n+                line_number = parser.show_line_for_paths(path_script,constants.SPEC_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_UNLISTED_03',rule_index= 13, level='error',attachments = [] ,message=Message(text=\" No Use of Network Policy\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+            # if specification is not present the line number of the SARIF output is be the first line number\n+            else:\n+                line = 1\n+                result= Result(rule_id='SLIKUBE_UNLISTED_03',rule_index= 13, level='error',attachments = [] ,message=Message(text=\" No Use of Network Policy\"))\n+                location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                result.locations = [location]\n+                run.results.append(result)\n+            \n+            dic[ cnt ] = [ constants.NET_POLICY_KW ]\n+            if ( (constants.SPEC_KW in key_list ) and  (constants.POD_SELECTOR_KW in key_list) and  (constants.MATCH_LABEL_KW in key_list) ):\n+                for src_val in all_values:\n+                    lis  = graphtaint.mineNetPolGraph(path_script, yaml_di, src_val, key_list )\n+            dic[ cnt ] = lis\n+    # print(dic) \n+    return dic  \n+\n+def scanForTruePID(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if (constants.SPEC_KW in key_list ) and ( constants.HOST_PID_KW in key_list ) :\n+            vals_for_pid = [] \n+            parser.getValsFromKey(yaml_di, constants.HOST_PID_KW, vals_for_pid)\n+            # print(vals_for_pid)\n+            vals_for_pid = [str(z_) for z_ in vals_for_pid if isinstance( z_,  bool) ]\n+            vals_for_pid = [z_.lower() for z_ in vals_for_pid]\n+            if constants.TRUE_LOWER_KW in vals_for_pid: \n+                cnt += 1\n+                # For Sarif output \n+                line_number = parser.show_line_for_paths(path_script,constants.HOST_PID_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_05',rule_index= 4, level='error',attachments = [] ,message=Message(text=\" Activation of hostPID\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+                \n+    return dic  \n+\n+\n+def scanForTrueIPC(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if (constants.SPEC_KW in key_list ) and ( constants.HOST_IPC_KW in key_list ) :\n+            vals_for_ipc = [] \n+            parser.getValsFromKey(yaml_di, constants.HOST_IPC_KW, vals_for_ipc)\n+            vals_for_ipc = [str(z_) for z_ in vals_for_ipc if isinstance( z_,  bool) ]\n+            vals_for_ipc = [z_.lower() for z_ in vals_for_ipc]\n+            if constants.TRUE_LOWER_KW in vals_for_ipc: \n+                cnt += 1\n+                # For Sarif output \n+                line_number = parser.show_line_for_paths(path_script,constants.HOST_IPC_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_03',rule_index= 2, level='error',attachments = [] ,message=Message(text=\" Activation of hostIPC\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+    return dic  \n+\n+def scanDockerSock(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if ( all( z_ in key_list for z_ in constants.DOCKERSOCK_KW_LIST )  ) :\n+            all_values = list( parser.getValuesRecursively(yaml_di)  )\n+            if (constants.DOCKERSOCK_PATH_KW in all_values):\n+                cnt += 1\n+                docker_sock_path = parser.keyMiner(yaml_di, constants.DOCKERSOCK_PATH_KW)\n+                # The last element of the http_in_spec is the value so the second last element is a . so third last element of the docker_sock_path is the key\n+\n+                line_number = parser.show_line_for_paths(path_script,docker_sock_path[-2])\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_07',rule_index= 6, level='error',attachments = [] ,message=Message(text=\" Docker Socket Mounting\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+    return dic  \n+\n+def runScanner(dir2scan):\n+    all_content   = [] \n+    all_yml_files = getYAMLFiles(dir2scan)\n+    val_cnt       = 0 \n+    for yml_ in all_yml_files:\n+        '''\n+        Need to filter out `.github/workflows.yml files` first \n+        '''\n+        if(parser.checkIfWeirdYAML ( yml_  )  == False): \n+            print (\"\\n\\n--------------- FILE --------------\\n\\t-->\",yml_)\n+            if( ( parser.checkIfValidK8SYaml( yml_ ) ) or (  parser.checkIfValidHelm( yml_ ) ) ) and parser.checkParseError( yml_) :\n+                # print (\" \\n\\n--------------- FILE RUNNING NOW---------------\")\n+                # print (yml_)\n+                # print(\"---------------############################### ------\\n\\n\\n\")\n+                helm_flag             = parser.checkIfValidHelm(yml_)\n+                k8s_flag              = parser.checkIfValidK8SYaml(yml_)\n+                if (helm_flag):\n+                    helm_chart.append(yml_)\n+                    print(\"HELM Chart\")\n+\n+                if (k8s_flag):\n+                    k8s_yaml.append(yml_)\n+                    print(\"Kubernetes YAML\")\n+                \n+                val_cnt = val_cnt + 1 \n+                print(constants.ANLYZING_KW + yml_ + constants.COUNT_PRINT_KW + yml_ +str(val_cnt) )\n+               \n+                print(\"get valid taint secrets\")\n+                within_secret_, templ_secret_, valid_taint_secr  = scanSingleManifest( yml_ )\n+\n+\n+                print(\"get privileged security contexts\")\n+                valid_taint_privi  = scanForOverPrivileges( yml_ )\n+               \n+                print(\"get insecure HTTP\")            \n+                http_dict             = scanForHTTP( yml_ )\n+               \n+                print(\"get missing security context\") \n+                absentSecuContextDict = scanForMissingSecurityContext( yml_ )\n+               \n+                print(\"get use of default namespace\") \n+                defaultNameSpaceDict  = scanForDefaultNamespace( yml_ )\n+               \n+                print(\"get missing resource limit\")\n+                absentResourceDict    = scanForResourceLimits( yml_ )\n+\n+                print(\"get absent rolling update count\") \n+                rollingUpdateDict     = scanForRollingUpdates( yml_ )\n+\n+                print(\"get absent network policy count\") \n+                absentNetPolicyDic    = scanForMissingNetworkPolicy( yml_ )\n+\n+                print(\" get hostPIDs where True is assigned \")\n+                pid_dic               = scanForTruePID( yml_ )\n+\n+                print(\"get hostIPCs where True is assigned\") \n+                ipc_dic               = scanForTrueIPC( yml_ )\n+                \n+                print(\"scan for docker sock paths: /var.run/docker.sock\") \n+                dockersock_dic        = scanDockerSock( yml_ )\n+\n+                print(\"scan for hostNetwork where True is assigned \")\n+                host_net_dic          = scanForHostNetwork( yml_ )\n+                \n+                print(\"scan for CAP SYS\") \n+                cap_sys_dic           = scanForCAPSYS( yml_ )\n+                \n+                print(\"scan for Host Aliases\") \n+                host_alias_dic        = scanForHostAliases( yml_ )\n+                \n+                print(\"scan for allowPrivilegeEscalation\") \n+                allow_privi_dic       = scanForAllowPrivileges( yml_ )\n+                \n+                print(\"scan for unconfied seccomp \")\n+                unconfied_seccomp_dict= scanForUnconfinedSeccomp( yml_ )\n+                \n+                print(\" scan for cap sys module \")\n+                cap_module_dic        = scanForCAPMODULE( yml_ )\n+                # need the flags to differentiate legitimate HELM and K8S flags \n+                \n+                print (\" \\n\\n---------------END FILE RUNNING--------------\")\n+                print(constants.SIMPLE_DASH_CHAR )\n+                \n+                #print(yml_)\n+                \n+                                      \n+                # sarif_json = to_json(sarif_log)\n+                # print(sarif_json)\n+                # #Write the JSON string to a file\n+                # sarif_file = yml_.split('\\\\')[-1].split('.')[0]+'.sarif'\n+                # with open(sarif_file, \"w\") as f:\n+                #     f.write(sarif_json)\n+\n+                all_content.append( ( dir2scan, yml_, within_secret_, templ_secret_, valid_taint_secr, valid_taint_privi, http_dict, absentSecuContextDict, defaultNameSpaceDict, absentResourceDict, rollingUpdateDict, absentNetPolicyDic, pid_dic, ipc_dic, dockersock_dic, host_net_dic, cap_sys_dic, host_alias_dic, allow_privi_dic, unconfied_seccomp_dict, cap_module_dic, k8s_flag, helm_flag ) )\n+            else:\n+                print(\"Invalid YAML --> \",yml_)\n+                invalid_yaml.append(yml_)\n+        else:\n+            print(\" Weird YAML --> \",yml_)\n+            weird_yaml.append(yml_)\n+\n+        sarif_json = to_json(sarif_log)\n+        #print(sarif_json)       \n+\n+\n+    return all_content, sarif_json\n+\n+\n+def scanForHostNetwork(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if (constants.SPEC_KW in key_list ) and ( constants.HOST_NET_KW in key_list ) :\n+            vals_for_net = [] \n+            parser.getValsFromKey(yaml_di, constants.HOST_NET_KW, vals_for_net)\n+            # print(vals_for_net)\n+            vals_for_net = [str(z_) for z_ in vals_for_net if isinstance( z_,  bool) ]\n+            vals_for_net = [z_.lower() for z_ in vals_for_net]\n+            if constants.TRUE_LOWER_KW in vals_for_net: \n+                cnt += 1 \n+                line_number = parser.show_line_for_paths(path_script,constants.HOST_NET_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_04',rule_index= 3, level='error',attachments = [] ,message=Message(text=\" Activation of hostNetwork\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+    return dic  \n+\n+\n+def scanForCAPSYS(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if ( all( z_ in key_list for z_ in constants.CAPSYS_KW_LIST )  ) :\n+            relevant_values = parser.getValuesRecursively(yaml_di)\n+            if (constants.CAPSYS_ADMIN_STRING in relevant_values) :\n+                cnt += 1\n+                capsys_key = parser.keyMiner(yaml_di, constants.CAPSYS_ADMIN_STRING)\n+                #print(capsys_key)\n+                # ['spec.YAML.DOC.2', 'template', 'spec', 'containers', '0', 'securityContext', 'capabilities', 'add', '1', 'CAP_SYS_ADMIN'] --> [-3]\n+                # Line number of the line where the key was found for CAPSYS_ADMIN  to provide  SARIF output\n+                line_number = parser.show_line_for_paths(path_script,capsys_key[-3])\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_06',rule_index= 5, level='error',attachments = [] ,message=Message(text=\" Capability Misuse\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+    return dic  \n+\n+def scanForCAPMODULE(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if ( all( z_ in key_list for z_ in constants.CAPSYS_KW_LIST )  ) :\n+            relevant_values = parser.getValuesRecursively(yaml_di)\n+            if (constants.CAPSYS_MODULE_STRING in relevant_values) :\n+                cnt += 1\n+                capsys_key = parser.keyMiner(yaml_di, constants.CAPSYS_MODULE_STRING)\n+                #print(capsys_key)\n+                #['spec.YAML.DOC.2', 'template', 'spec', 'containers', '0', 'securityContext', 'capabilities', 'add', '2', 'CAP_SYS_MODULE']\n+                # Line number of the line where the key was found for CAPSYS_MODULE to provide  SARIF output\n+                line_number = parser.show_line_for_paths(path_script,capsys_key[-3])\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_06',rule_index= 5, level='error',attachments = [] ,message=Message(text=\" Capability Misuse\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                dic[ cnt ] = []\n+    return dic      \n+\n+def scanForHostAliases(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        if ( constants.HOST_ALIAS_KW in key_list ) :\n+                cnt += 1 \n+                # Line number of the line where the key was found for CAPSYS_MODULE to provide  SARIF output\n+                line_number = parser.show_line_for_paths(path_script,constants.HOST_ALIAS_KW)\n+                for line in line_number:\n+                    result= Result(rule_id='SLIKUBE_UNLISTED_04',rule_index= 14, level='error',attachments = [] ,message=Message(text=\" Use of Host Aliases\"))\n+                    location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                    result.locations = [location]\n+                    run.results.append(result)\n+                relevant_values = [] \n+                parser.getValsFromKey(yaml_di, constants.HOST_ALIAS_KW, relevant_values)\n+                dic[ cnt ] = relevant_values\n+    return dic  \n+\n+\n+def scanForAllowPrivileges(path_script ):\n+    dic, lis   = {}, []\n+    #print(path_script)\n+    # with open(path_script, \"r\") as f:\n+    #     for line in f.readlines():\n+    #         print(line)\n+    if ( parser.checkIfValidK8SYaml( path_script )  ):\n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        #print(key_list)\n+        if ( all( var in key_list for var in constants.ALLOW_PRIVI_KW_LIST ) ) :\n+                cnt += 1 \n+                relevant_values = [] \n+                parser.getValsFromKey(yaml_di, constants.ALLOW_PRIVILEGE_KW, relevant_values)\n+                relevant_values = [str(x_) for x_ in relevant_values if isinstance(x_, bool)]\n+                relevant_values = [x_.lower() for x_ in relevant_values]\n+                if constants.TRUE_LOWER_KW in relevant_values:\n+                    dic[cnt] = []\n+                    line_number = parser.show_line_for_paths(path_script, constants.ALLOW_PRIVILEGE_KW)\n+                    for line in line_number:\n+                        #print(line7\n+                        result= Result(rule_id='SLIKUBE_08',rule_index= 7, message=Message(text=\"Escalated Privileges for Child Container Processes\"),level='error')\n+                        location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))                       \n+                        result.locations = [location]\n+                        #print(result)\n+                        #result.locations.append(location)\n+                        #run.results = [result]   \n+                        #print(result) \n+                        run.results.append(result)\n+    return dic  \n+\n+def scanForHTTP( path2script ):\n+    sh_files_configmaps = {} \n+    http_count = 0 \n+    if parser.checkIfValidK8SYaml( path2script ) or parser.checkIfValidHelm( path2script ) or True:\n+        dict_as_list = parser.loadMultiYAML( path2script )\n+        yaml_d       = parser.getSingleDict4MultiDocs( dict_as_list )\n+        all_vals     = list (parser.getValuesRecursively( yaml_d )  )\n+        all_vals     = [x_ for x_ in all_vals if isinstance(x_, str) ] \n+        #print(all_vals)\n+        for val_ in all_vals:\n+            # if (constants.HTTP_KW in val_ ) and ( (constants.WWW_KW in val_) and (constants.ORG_KW in val_) ):\n+            if (constants.HTTP_KW in val_ ) :\n+                key_lis   = []\n+                parser.getKeyRecursively(yaml_d, key_lis) \n+                '''\n+                if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+                as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+                '''\n+                just_keys = [x_[0] for x_ in key_lis] \n+                http_in_spec = parser.keyMiner(yaml_d,val_)\n+                # The last element of the http_in_spec is the value so the second last element of the http_in_spec is the key\n+                http_key = http_in_spec[-2]\n+                print(\" THIS IS HTTP KEY ----> \", http_key)\n+\n+                if ( constants.SPEC_KW in just_keys ):\n+                    '''\n+                    this branch is for HTTP values coming from Deplyoment manifests  \n+                    '''                    \n+                    http_count += 1 \n+                    sh_files_configmaps[http_count] =  val_ \n+\n+                    line_number = parser.show_line_for_paths(path2script, http_key)\n+                    for line in line_number:\n+                            # print(line)\n+                            result= Result(rule_id='SLIKUBE_10',rule_index= 9,level='error', attachments= [], message=Message(text=\"Use of HTTP without TLS\"),)\n+                            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path2script),region = Region(start_line =line)))\n+                            result.locations = [location] \n+                            run.results.append(result)\n+                            \n+                elif( parser.checkIfValidHelm( path2script ) ):\n+                    '''\n+                    this branch is for HTTP values coming from Values.yaml in HELM charts  \n+                    '''\n+                    http_count += 1 \n+                    matching_keys = parser.keyMiner(yaml_d, val_)\n+                    key_ = matching_keys[-1]  \n+                    infected_list = graphtaint.mineViolationGraph(path2script, yaml_d, val_, key_) \n+                    sh_files_configmaps[http_count] = infected_list\n+                    line_number = parser.show_line_for_paths(path2script, http_key)\n+                    for line in line_number:\n+                            #print(line)\n+                            result= Result(rule_id='SLIKUBE_10',rule_index= 9,level='error', attachments= [], message=Message(text=\"Use of HTTP without TLS\"),)\n+                            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path2script),region = Region(start_line =line)))\n+                            result.locations = [location] \n+                            run.results.append(result)\n+                    \n+                else: \n+                    '''\n+                    this branch is for HTTP values coming from ConfigMaps \n+                    '''                    \n+                    val_holder = [] \n+                    parser.getValsFromKey(yaml_d, constants.KIND_KEY_NAME, val_holder)\n+                    if ( constants.CONFIGMAP_KW in val_holder ):\n+                        http_count += 1 \n+                        infected_list = graphtaint.getTaintsFromConfigMaps( path2script  ) \n+                        sh_files_configmaps[http_count] = infected_list\n+                        line_number = parser.show_line_for_paths(path2script, http_key)\n+                        for line in line_number:\n+                            #print(line)\n+                            result= Result(rule_id='SLIKUBE_10',rule_index= 9,level='error', attachments= [], message=Message(text=\"Use of HTTP without TLS\"),)\n+                            location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path2script),region = Region(start_line =line)))\n+                            result.locations = [location] \n+                            run.results.append(result)\n+                         \n+    \n+    # print(sh_files_configmaps) \n+    return sh_files_configmaps\n+\n+def scanForUnconfinedSeccomp(path_script ):\n+    dic, lis   = {}, []\n+    if ( parser.checkIfValidK8SYaml( path_script )  ): \n+        cnt = 0 \n+        dict_as_list = parser.loadMultiYAML( path_script )\n+        yaml_di      = parser.getSingleDict4MultiDocs( dict_as_list )        \n+        temp_ls = [] \n+        parser.getKeyRecursively(yaml_di, temp_ls) \n+        '''\n+        if you are using `parser.getKeyRecursively` to get all keys , you need to do some trnasformation to get the key names \n+        as the output is a list of tuples so, `[(k1, v1), (k2, v2), (k3, v3)]`\n+        '''\n+        key_list = [ x_[0] for x_ in temp_ls  ]\n+        # print(key_list)\n+        if ( all( var in key_list for var in constants.SECCOMP_KW_LIST ) ) :\n+                cnt += 1 \n+                relevant_values = [] \n+                parser.getValsFromKey(yaml_di, constants.TYPE_KW, relevant_values)\n+                # print( relevant_values )\n+                if constants.UNCONFIED_KW in relevant_values:\n+                    line_number = parser.show_line_for_paths(path_script,constants.UNCONFIED_KW)\n+                    for line in line_number:\n+                        result= Result(rule_id='SLIKUBE_UNLISTED_05',rule_index= 4, level='error',attachments = [] ,message=Message(text=\" Use of unconfined seccomp profile\"))\n+                        location = Location(physical_location=PhysicalLocation(artifact_location=ArtifactLocation(uri=path_script),region = Region(start_line =line)))\n+                        result.locations = [location]\n+                        run.results.append(result)\n+                    dic[cnt] = [] \n+    return dic  \n+\n+if __name__ == '__main__':\n+    #provide directory to scan\n+    dir2scan = r'C:\\Users\\..'\n+    a,b = runScanner(dir2scan)\n+    with open(\"test-scanner.sarif\", \"w\") as f:\n+        f.write(b)"}, {"sha": "6dd7e41d3e4abf436825cca3d7100d812703e225", "filename": "vault4paper.py", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/vault4paper.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/vault4paper.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/vault4paper.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,53 @@\n+import os\n+import shutil\n+import json\n+import re\n+\n+LOG_FILE = \".vault4paper-log.json\"\n+BACKUP_DIR = \".vault_backups\"\n+\n+def backup_file(file_path):\n+    if not os.path.exists(BACKUP_DIR):\n+        os.makedirs(BACKUP_DIR)\n+    backup_path = os.path.join(BACKUP_DIR, os.path.basename(file_path))\n+    shutil.copy2(file_path, backup_path)\n+    log_backup(file_path, backup_path)\n+\n+def log_backup(original, backup):\n+    if os.path.exists(LOG_FILE):\n+        with open(LOG_FILE, \"r\") as f:\n+            log_data = json.load(f)\n+    else:\n+        log_data = {}\n+\n+    log_data[original] = backup\n+    with open(LOG_FILE, \"w\") as f:\n+        json.dump(log_data, f, indent=2)\n+\n+def replace_secrets_in_ansible():\n+    print(\"\ud83d\udd0d Scanning Ansible/ for secrets...\")\n+    for root, _, files in os.walk(\"Ansible\"):\n+        for file in files:\n+            if file.endswith(\".yml\") or file.endswith(\".yaml\"):\n+                file_path = os.path.join(root, file)\n+                with open(file_path, \"r\") as f:\n+                    lines = f.readlines()\n+\n+                modified = False\n+                new_lines = []\n+                for line in lines:\n+                    if \"password:\" in line:\n+                        backup_file(file_path)\n+                        secret_value = line.split(\"password:\")[1].strip()\n+                        print(f\"\u2705 Replacing secret: {secret_value}\")\n+                        new_lines.append(\"  password: {{ vault_secret }}\\n\")\n+                        modified = True\n+                    else:\n+                        new_lines.append(line)\n+\n+                if modified:\n+                    with open(file_path, \"w\") as f:\n+                        f.writelines(new_lines)\n+                        print(f\"\ud83d\udd10 Updated: {file_path}\")\n+\n+replace_secrets_in_ansible()"}, {"sha": "54725db8f18f85939deab00a50876876fdf3c98d", "filename": "vault4paper_antidot.py", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/vault4paper_antidot.py", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/d209c222585e5e4ebc0080a9b00c301c3fe90b6f/vault4paper_antidot.py", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/vault4paper_antidot.py?ref=d209c222585e5e4ebc0080a9b00c301c3fe90b6f", "patch": "@@ -0,0 +1,44 @@\n+# vault4paper_antidot.py\n+import os\n+import shutil\n+import json\n+\n+BACKUP_DIR = \".vault_backups\"\n+LOG_FILE = \".vault4paper-log.json\"\n+\n+\n+def load_log():\n+    if os.path.exists(LOG_FILE):\n+        with open(LOG_FILE, \"r\") as f:\n+            return json.load(f)\n+    return {}\n+\n+\n+def restore_files(log_data):\n+    if not os.path.exists(BACKUP_DIR):\n+        print(\"Backup directory not found. Nothing to restore.\")\n+        return\n+\n+    for original_path, backup_path in log_data.items():\n+        if os.path.exists(backup_path):\n+            shutil.copy2(backup_path, original_path)\n+            print(f\"Restored: {original_path}\")\n+        else:\n+            print(f\"Missing backup for: {original_path}\")\n+\n+\n+def main():\n+    print(\"Restoring all files modified by vault4paper.py...\")\n+    log_data = load_log()\n+\n+    if not log_data:\n+        print(\"No changes found to revert.\")\n+        return\n+\n+    restore_files(log_data)\n+    print(\"\u2705 All reversible changes from vault4paper have been restored.\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n+"}]}, "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f_null": {"sha": "58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "node_id": "C_kwDOOevfU9oAKDU4ZmZlMzdhZTQxNzI5MDYxNjlmZmFhM2ZlMTFlNmE1MmYxY2RkOGY", "commit": {"author": {"name": "Zakariya Veasy", "email": "73971304+zveasy@users.noreply.github.com", "date": "2025-04-24T02:44:41Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2025-04-24T02:44:41Z"}, "message": "Initial commit", "tree": {"sha": "60d21b0f926bfb3f641438bb8e75300cb92542e3", "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/git/trees/60d21b0f926bfb3f641438bb8e75300cb92542e3"}, "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/git/commits/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null, "verified_at": null}}, "url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "html_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/commit/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "comments_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/commits/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f/comments", "author": {"login": "zveasy", "id": 73971304, "node_id": "MDQ6VXNlcjczOTcxMzA0", "avatar_url": "https://avatars.githubusercontent.com/u/73971304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zveasy", "html_url": "https://github.com/zveasy", "followers_url": "https://api.github.com/users/zveasy/followers", "following_url": "https://api.github.com/users/zveasy/following{/other_user}", "gists_url": "https://api.github.com/users/zveasy/gists{/gist_id}", "starred_url": "https://api.github.com/users/zveasy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zveasy/subscriptions", "organizations_url": "https://api.github.com/users/zveasy/orgs", "repos_url": "https://api.github.com/users/zveasy/repos", "events_url": "https://api.github.com/users/zveasy/events{/privacy}", "received_events_url": "https://api.github.com/users/zveasy/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "parents": [], "stats": {"total": 2, "additions": 2, "deletions": 0}, "files": [{"sha": "1e39f51757f192c5606db76290db5df1284396ed", "filename": "README.md", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/blob/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f/README.md", "raw_url": "https://github.com/zveasy/Veasy-SQA2025-AUBURN/raw/58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f/README.md", "contents_url": "https://api.github.com/repos/zveasy/Veasy-SQA2025-AUBURN/contents/README.md?ref=58ffe37ae4172906169ffaa3fe11e6a52f1cdd8f", "patch": "@@ -0,0 +1,2 @@\n+# Veasy-SQA2025-AUBURN\n+Project"}]}}